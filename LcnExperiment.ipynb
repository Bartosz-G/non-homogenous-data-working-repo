{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22311,
     "status": "ok",
     "timestamp": 1693527126975,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "hG3fkgsQBBCL",
    "outputId": "902e6808-21ae-4d46-fb4a-8ceb6bf74e5e",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drive version:\n",
    "# !pip uninstall TabularExperimentTrackerClient --y\n",
    "# !pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
    "# !pip uninstall NeuralNetworksTrainingPackage --y\n",
    "# !pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
      "  Cloning https://github.com/DanielWarfield1/TabularExperimentTrackerClient to /tmp/pip-req-build-vd8ci520\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/DanielWarfield1/TabularExperimentTrackerClient /tmp/pip-req-build-vd8ci520\n",
      "  Resolved https://github.com/DanielWarfield1/TabularExperimentTrackerClient to commit 780933411aa8c4e394478a26dec4a39447f8f012\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting openml (from TabularExperimentTrackerClient==0.0.1)\n",
      "  Using cached openml-0.14.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from TabularExperimentTrackerClient==0.0.1) (2.28.2)\n",
      "Collecting liac-arff>=2.4.0 (from openml->TabularExperimentTrackerClient==0.0.1)\n",
      "  Using cached liac_arff-2.5.0-py3-none-any.whl\n",
      "Collecting xmltodict (from openml->TabularExperimentTrackerClient==0.0.1)\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.0.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.23.5)\n",
      "Collecting minio (from openml->TabularExperimentTrackerClient==0.0.1)\n",
      "  Using cached minio-7.1.16-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (12.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (2023.5.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->openml->TabularExperimentTrackerClient==0.0.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->openml->TabularExperimentTrackerClient==0.0.1) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil->openml->TabularExperimentTrackerClient==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (3.1.0)\n",
      "Building wheels for collected packages: TabularExperimentTrackerClient\n",
      "  Building wheel for TabularExperimentTrackerClient (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for TabularExperimentTrackerClient: filename=TabularExperimentTrackerClient-0.0.1-py3-none-any.whl size=6377 sha256=2d9c51a318b19994d9cc29f3095ef8aaed632b1d38d64b60143c92bcc633db94\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ye1u5chl/wheels/f9/2e/f3/69345202c956e07475c8f2f55074ad4d97b3e6a976b70fafab\n",
      "Successfully built TabularExperimentTrackerClient\n",
      "Installing collected packages: xmltodict, minio, liac-arff, openml, TabularExperimentTrackerClient\n",
      "Successfully installed TabularExperimentTrackerClient-0.0.1 liac-arff-2.5.0 minio-7.1.16 openml-0.14.1 xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage\n",
      "  Cloning https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to /tmp/pip-req-build-k70ufxcw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Bartosz-G/NeuralNetworksTrainingPackage /tmp/pip-req-build-k70ufxcw\n",
      "  Resolved https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to commit 45dfa03c2d4e1b8338fa1086a74cdc5423d5639e\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.23.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.2.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (2.0.0)\n",
      "Collecting torcheval (from NeuralNetworksTrainingPackage==1.0.0)\n",
      "  Using cached torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->NeuralNetworksTrainingPackage==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->NeuralNetworksTrainingPackage==1.0.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->NeuralNetworksTrainingPackage==1.0.0) (1.3.0)\n",
      "Building wheels for collected packages: NeuralNetworksTrainingPackage\n",
      "  Building wheel for NeuralNetworksTrainingPackage (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NeuralNetworksTrainingPackage: filename=NeuralNetworksTrainingPackage-1.0.0-py3-none-any.whl size=8425 sha256=2bb3b598f88e1dea09d865b77fa77764b035291ca8190c9013a962a8437bbf5d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xtsgx_p5/wheels/82/01/e8/da81de869be2f6b8c193a6fc5c42ee3ff0b6b97a7b70cd565a\n",
      "Successfully built NeuralNetworksTrainingPackage\n",
      "Installing collected packages: torcheval, NeuralNetworksTrainingPackage\n",
      "Successfully installed NeuralNetworksTrainingPackage-1.0.0 torcheval-0.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
    "%pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5858,
     "status": "ok",
     "timestamp": 1693527132821,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "jguLhhgFBSkE",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1693527132821,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "A9UNy92SBdFB",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing NeuralNetworksPackage\n",
    "#from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import *\n",
    "# Global namespace:\n",
    "# Hyperparams(**run_info.get('hyp'))\n",
    "# CustomDataset(X, Y, relative_indices, tensor_type=torch.float)\n",
    "# CustomDatasetWrapper(train_dataset, relative_indices)\n",
    "# kfold_dataloader_iterator(dataset, n_splits=10, random_state=42, batch_size=16, shuffle_kfold=True, shuffle_dataloader=True)\n",
    "# get_train_test(X, y, categorical_indicator, attribute_names, train_split, seed)\n",
    "# get_train_val_test\n",
    "#from NeuralNetworksTrainingPackage.metrics.basic_metrics import *\n",
    "# test(args, model, device, test_loader, test_set_name)\n",
    "# train(args, model, device, train_loader, optimizer, epoch, anneal, alpha=1)\n",
    "# calc_metrics(y, yhat, is_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43635,
     "status": "ok",
     "timestamp": 1693527176450,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "xsHc3zlSCO7J",
    "outputId": "121cd4b8-d138-4d2f-a0dc-d0de85c48f65",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from TabularExperimentTrackerClient.ExperimentClient import ExperimentClient\n",
    "\n",
    "path =  '../creds/'\n",
    "creds_orch_file = \"creds-orch.txt\"\n",
    "creds_openml_file = \"creds-openml.txt\"\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(path, creds_orch_file), 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    orchname = lines[0].strip()\n",
    "    orchsecret = lines[1].strip()\n",
    "\n",
    "with open (os.path.join(path, creds_openml_file), \"r\") as myfile:\n",
    "    openMLAPIKey = myfile.read()\n",
    "\n",
    "ex = ExperimentClient(verbose = True)\n",
    "\n",
    "\n",
    "ex.define_orch_cred(orchname, orchsecret)\n",
    "ex.define_opml_cred(openMLAPIKey)\n",
    "\n",
    "# Colab version\n",
    "# ex.define_opml_cred_drive('/My Drive/research/non-homogenous-data/creds/creds-openml.txt')\n",
    "# ex.define_orch_cred_drive('bart', '/My Drive//research/non-homogenous-data/creds/creds-colab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.event_handler import dataPreProcessingEventEmitter\n",
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import filterCardinality, quantileTransform, trunctuateData\n",
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import oneHotEncodePredictors, oneHotEncodeTargets, toDataFrame\n",
    "\n",
    "n_sample = 20000\n",
    "quantile_transform_distribution='uniform'\n",
    "\n",
    "\n",
    "data_pre_processing = dataPreProcessingEventEmitter()\n",
    "\n",
    "filter_cardinality = filterCardinality()\n",
    "data_pre_processing.add_pre_processing('regression', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing('classification', filter_cardinality)\n",
    "\n",
    "quantile_transform = quantileTransform(output_distribution = quantile_transform_distribution)\n",
    "data_pre_processing.add_pre_processing('regression', quantile_transform)\n",
    "data_pre_processing.add_pre_processing('classification', quantile_transform)\n",
    "\n",
    "trunctuate_data = trunctuateData(n = n_sample)\n",
    "data_pre_processing.add_pre_processing('regression', trunctuate_data)\n",
    "data_pre_processing.add_pre_processing('classification', trunctuate_data)\n",
    "\n",
    "one_hot_encode_predictors = oneHotEncodePredictors()\n",
    "data_pre_processing.add_pre_processing('regression', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing('classification', one_hot_encode_predictors)\n",
    "\n",
    "one_hot_encode_targets = oneHotEncodeTargets()\n",
    "data_pre_processing.add_pre_processing('classification', one_hot_encode_targets)\n",
    "\n",
    "to_data_frame = toDataFrame()\n",
    "data_pre_processing.add_pre_processing('regression', to_data_frame)\n",
    "data_pre_processing.add_pre_processing('classification', to_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBYDfcuJE-bP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Defining the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1693527176452,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "lyMEiU5ODM2e",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = 'experiment_LCN_1'\n",
    "\n",
    "\n",
    "\n",
    "# LCN and LLN Parameters\n",
    "depth = {'distribution': 'int_uniform', 'min':1, 'max':11}\n",
    "seed = {'distribution': 'constant', 'value': 42}\n",
    "drop_type = {'distribution': 'categorical', 'values':['node_dropconnect', 'none']}\n",
    "p = {'distribution': 'float_uniform', 'min':0.25, 'max':0.75}\n",
    "back_n = {'distribution': 'categorical', 'values':[0, 0, 0, 1]}\n",
    "hidden_dim = {'distribution': 'constant', 'value': 1} # Assertion error coming from Net if not 1\n",
    "anneal = {'distribution': 'categorical', 'values':['interpolation', 'none', 'approx']}\n",
    "batch_size = {'distribution': 'categorical', 'values':[16,32,64,64,64,128,256]}\n",
    "epochs = {'distribution': 'constant', 'value': 30}\n",
    "lr = {'distribution': 'log_uniform', 'min':0.05, 'max':0.2} # yields mean = 0.1082, median 0.1\n",
    "momentum = {'distribution': 'constant', 'value': 0.9}\n",
    "no_cuda = {'distribution': 'constant', 'value': False}\n",
    "lr_step_size = {'distribution': 'categorical', 'values':[10, 10, 15, 20]}\n",
    "gamma = {'distribution': 'constant', 'value': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1693527176453,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "3atPjhTUEn05",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# Regression: Locally Constant Networks\n",
    "#============================================================\n",
    "LCN_reg_SGD_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'drop_type': drop_type,\n",
    "    'p': p,\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': back_n,\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'anneal': anneal,\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma,\n",
    "    'task': {'distribution': 'constant', 'value': 'regression'}\n",
    "    }\n",
    "\n",
    "LCN_reg_AMSGrad_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'drop_type': drop_type,\n",
    "    'p': p,\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': back_n,\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'anneal': anneal,\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma,\n",
    "    'task': {'distribution': 'constant', 'value': 'regression'}\n",
    "    }\n",
    "\n",
    "#============================================================\n",
    "# Regression: Locally Linear Networks\n",
    "#============================================================\n",
    "\n",
    "LLN_reg_SGD_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'drop_type': drop_type,\n",
    "    'p': p,\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': back_n,\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'anneal': anneal,\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma,\n",
    "    'task': {'distribution': 'constant', 'value': 'regression'}\n",
    "    }\n",
    "\n",
    "\n",
    "LLN_reg_AMSGrad_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'drop_type': drop_type,\n",
    "    'p': p,\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': back_n,\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'anneal': anneal,\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma,\n",
    "    'task': {'distribution': 'constant', 'value': 'regression'}\n",
    "    }\n",
    "\n",
    "#============================================================\n",
    "# Classification: Locally Constant Networks\n",
    "#============================================================\n",
    "\n",
    "LCN_cls_SGD_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'drop_type': drop_type,\n",
    "    'p': p,\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': back_n,\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'anneal': anneal,\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma,\n",
    "    'task': {'distribution': 'constant', 'value': 'classification'}\n",
    "    }\n",
    "\n",
    "LCN_cls_AMSGrad_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'drop_type': drop_type,\n",
    "    'p': p,\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': back_n,\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'anneal': anneal,\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma,\n",
    "    'task': {'distribution': 'constant', 'value': 'classification'}\n",
    "    }\n",
    "\n",
    "#============================================================\n",
    "# Classification: Locally Linear Networks\n",
    "#============================================================\n",
    "\n",
    "LLN_cls_SGD_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'drop_type': drop_type,\n",
    "    'p': p,\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': back_n,\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'anneal': anneal,\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma,\n",
    "    'task': {'distribution': 'constant', 'value': 'classification'}\n",
    "    }\n",
    "\n",
    "\n",
    "LLN_cls_AMSGrad_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'drop_type': drop_type,\n",
    "    'p': p,\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': back_n,\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'anneal': anneal,\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma,\n",
    "    'task': {'distribution': 'constant', 'value': 'classification'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1693527176453,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "5UI-B-EbEJUh",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_groups = {\n",
    "    'LCN_reg_SGD':{'model':'LCN_reg_SGD', 'hype':LCN_reg_SGD_space},\n",
    "    'LCN_reg_AMSGrad':{'model':'LCN_reg_AMSGrad', 'hype':LCN_reg_AMSGrad_space},\n",
    "    'LLN_reg_SGD':{'model':'LLN_reg_SGD', 'hype':LLN_reg_SGD_space},\n",
    "    'LLN_reg_AMSGrad':{'model':'LLN_reg_AMSGrad', 'hype':LLN_reg_AMSGrad_space},\n",
    "    'LCN_cls_SGD':{'model':'LCN_cls_SGD', 'hype':LCN_cls_SGD_space},\n",
    "    'LCN_cls_AMSGrad':{'model':'LCN_cls_AMSGrad', 'hype':LCN_cls_AMSGrad_space},\n",
    "    'LLN_cls_SGD':{'model':'LLN_cls_SGD', 'hype':LLN_cls_SGD_space},\n",
    "    'LLN_cls_AMSGrad':{'model':'LLN_cls_AMSGrad', 'hype':LLN_cls_AMSGrad_space},\n",
    "}\n",
    "\n",
    "ex.def_model_groups(model_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 1771,
     "status": "ok",
     "timestamp": 1693527178212,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "BFVftUr4EMSr",
    "outputId": "096cceb4-bd60-4c4c-f052-fc8967518e9f",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automatically defined data groups: dict_keys(['opml_reg_purnum_group', 'opml_class_purnum_group', 'opml_reg_numcat_group', 'opml_class_numcat_group'])\n",
      "existing experiment found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'existing experiment found'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.def_data_groups_opml()\n",
    "print(f'automatically defined data groups: {ex.data_groups.keys()}')\n",
    "\n",
    "classification_models = [k for k in model_groups.keys() if '_cls' in k]\n",
    "regression_models = [k for k in model_groups.keys() if '_reg' in k]\n",
    "\n",
    "\n",
    "applications = {'opml_reg_purnum_group': regression_models,\n",
    "                'opml_reg_numcat_group': regression_models,\n",
    "                'opml_class_purnum_group': classification_models,\n",
    "                'opml_class_numcat_group': classification_models}\n",
    "\n",
    "ex.def_applications(applications)\n",
    "ex.reg_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1693527178723,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "-I95Kh4qMo1u",
    "outputId": "db2495fa-b945-4480-e310-32f6e368c0c3",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total required runs: 14160\n"
     ]
    }
   ],
   "source": [
    "exp_info = ex.experiment_info()\n",
    "successful_runs = exp_info['successful_runs']\n",
    "required_runs = exp_info['required_runs']\n",
    "print('total required runs: {}'.format(required_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLkZmpB9yVys",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Main training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJKPv2m6nFzd",
    "outputId": "36bedcf2-a3b9-4b13-81f1-43c5b157c89f",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import get_train_val_test, CustomDataset\n",
    "from NeuralNetworksTrainingPackage.metrics.basic_metrics import calc_metrics\n",
    "from models.LcnNetwork import *\n",
    "from training.LcnTrain import *\n",
    "\n",
    "\n",
    "for i in range(14160):\n",
    "    print(f'==== Begin run:{i} ====')\n",
    "    print('---- Initialising parameters for the run ----')\n",
    "    run_info = ex.begin_run_sticky()\n",
    "    args = Hyperparams(**run_info.get('hyp')) # hyperparameters for LCN need to be in form of an object (you can ignore this)\n",
    "    print(run_info)\n",
    "\n",
    "\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    data_pre_processing.set_seed_for_all(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('---- Loading datasets ----')\n",
    "    X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "    train_data, val_data, test_data = get_train_val_test(X, y, categorical_indicator, attribute_names,\n",
    "                                                         data_pre_processing,\n",
    "                                                         task = args.task,\n",
    "                                                         model = run_info.get('model'),\n",
    "                                                         split = [0.5, 0.25, 0.25],\n",
    "                                                         args = args) # Returns CustomDataset obj instances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, # CustomDataset obj can be directly passed to dataloader\n",
    "                                                   batch_size=args.batch_size,\n",
    "                                                   shuffle= True)\n",
    "\n",
    "\n",
    "    train_eval_dataloader = torch.utils.data.DataLoader(train_data, # CustomDataset obj can be directly passed to dataloader\n",
    "                                                        batch_size=len(train_data),\n",
    "                                                        shuffle= True) # Required for test_metrics()\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, # CustomDataset obj can be directly passed to dataloader\n",
    "                                                 batch_size=len(val_data),\n",
    "                                                 shuffle= True)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, # CustomDataset obj can be directly passed to dataloader\n",
    "                                                  batch_size=len(test_data),\n",
    "                                                  shuffle= True)\n",
    "\n",
    "\n",
    "    model = Net(input_dim= args.input_dim, \n",
    "                output_dim= args.output_dim, \n",
    "                hidden_dim= args.hidden_dim, \n",
    "                num_layer= args.depth, \n",
    "                num_back_layer= args.back_n, \n",
    "                dense= True,\n",
    "                drop_type= args.drop_type,\n",
    "                net_type= args.net_type,\n",
    "                approx= args.anneal).to(device)\n",
    "\n",
    "\n",
    "    if args.optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n",
    "    elif args.optimizer == 'AMSGrad':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.gamma)\n",
    "\n",
    "\n",
    "    metrics = {}\n",
    "    start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n",
    "    if args.anneal == 'approx':\n",
    "        args.net_type = 'approx_' + args.net_type\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch, args.epochs + start_epoch):\n",
    "        print(f\"----{epoch}th training epoch ----\")\n",
    "        epoch_metrics = {}\n",
    "\n",
    "        scheduler.step(epoch)\n",
    "\n",
    "        alpha = get_alpha(epoch, args.epochs)\n",
    "        train_approximate_loss = train(args, model, device, train_dataloader, optimizer, epoch, args.anneal, alpha)\n",
    "\n",
    "        train_loss = get_loss(args, model, device, train_dataloader, 'train')\n",
    "        val_loss = get_loss(args, model, device, val_dataloader, 'valid')\n",
    "        test_loss = get_loss(args, model, device, test_dataloader, 'test')\n",
    "\n",
    "        if epoch == args.epochs:\n",
    "            continue\n",
    "\n",
    "        epoch_metrics['train_loss'] = train_loss\n",
    "        epoch_metrics['val_loss'] = val_loss\n",
    "        epoch_metrics['test_loss'] = test_loss\n",
    "        epoch_metrics['epoch'] = epoch\n",
    "        \n",
    "        ex.update_run(epoch_metrics)\n",
    "\n",
    "        if torch.isnan(torch.tensor(train_loss)).item():\n",
    "            print('---Stopping training due to loss being nan!---')\n",
    "            break\n",
    "\n",
    "\n",
    "    else:\n",
    "        metrics['epoch_time'] = time.time()-start_time\n",
    "\n",
    "\n",
    "        train_metrics = get_metrics(args, model, device, train_eval_dataloader, calc_metrics, 'train') #Requires batch_size to be entire dataset\n",
    "        val_metrics = get_metrics(args, model, device, val_dataloader, calc_metrics, 'valid')\n",
    "        test_metrics = get_metrics(args, model, device, test_dataloader, calc_metrics, 'test')\n",
    "\n",
    "        metrics['train_loss'] = train_loss\n",
    "        metrics['val_loss'] = val_loss\n",
    "        metrics['test_loss'] = test_loss\n",
    "        metrics['train_metrics'] = train_metrics\n",
    "        metrics['validate_metrics'] = val_metrics\n",
    "        metrics['test_metrics'] = test_metrics\n",
    "        \n",
    "        ex.update_run(metrics)\n",
    "        ex.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsz24ztP9CQ8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Checking whether the code works as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading task 335-361103\n",
      "task different than previous task, downloading...\n"
     ]
    }
   ],
   "source": [
    "X, y, categorical_indicator, attribute_names = ex.opml_load_task('335-361103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "# X_c, y_c, categorical_indicator_c, attribute_names_c = deepcopy(X), deepcopy(y), deepcopy(categorical_indicator), deepcopy(attribute_names)\n",
    "X, y, categorical_indicator, attribute_names = deepcopy(X_c), deepcopy(y_c), deepcopy(categorical_indicator_c), deepcopy(attribute_names_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "some_val = train_data.X.iloc[[5], :]\n",
    "print(some_val)\n",
    "torch.tensor(some_val.values.squeeze(axis=0), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y, categorical_indicator, attribute_names = data_pre_processing.apply('regression', X, y, categorical_indicator, attribute_names)\n",
    "X, y, categorical_indicator, attribute_names = data_pre_processing.apply('LCN_reg_AMSGrad', X, y, categorical_indicator, attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing New Metrics Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '64faff7eb9df67c3dfb9bda7', 'metrics_per_epoch': [], 'experiment_id': '64f0f2556e02727fe9a6ff59', 'experiment_name': 'experiment_LCN_1', 'mtpair_index': 162, 'mtpair_model': 'LLN_cls_SGD', 'mtpair_task': '337-361063', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 7, 'seed': 42, 'drop_type': 'node_dropconnect', 'p': 0.34397587452213385, 'ensemble_n': 1, 'shrinkage': 1, 'back_n': 0, 'net_type': 'locally_linear', 'hidden_dim': 1, 'anneal': 'approx', 'optimizer': 'SGD', 'batch_size': 64, 'epochs': 30, 'lr': 0.05644720473737607, 'momentum': 0.9, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1, 'task': 'classification'}, 'model': 'LLN_cls_SGD', 'task': '337-361063'}\n",
      "4faff7eb9df67c3dfb9bda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_id': '64faff7eb9df67c3dfb9bda7',\n",
       " 'metrics_per_epoch': [],\n",
       " 'experiment_id': '64f0f2556e02727fe9a6ff59',\n",
       " 'experiment_name': 'experiment_LCN_1',\n",
       " 'mtpair_index': 162,\n",
       " 'mtpair_model': 'LLN_cls_SGD',\n",
       " 'mtpair_task': '337-361063',\n",
       " 'is_completed': False,\n",
       " 'user_id': '64d3a7457658d6ec6db139d0',\n",
       " 'user_name': 'bart',\n",
       " 'hyp': {'depth': 7,\n",
       "  'seed': 42,\n",
       "  'drop_type': 'node_dropconnect',\n",
       "  'p': 0.34397587452213385,\n",
       "  'ensemble_n': 1,\n",
       "  'shrinkage': 1,\n",
       "  'back_n': 0,\n",
       "  'net_type': 'locally_linear',\n",
       "  'hidden_dim': 1,\n",
       "  'anneal': 'approx',\n",
       "  'optimizer': 'SGD',\n",
       "  'batch_size': 64,\n",
       "  'epochs': 30,\n",
       "  'lr': 0.05644720473737607,\n",
       "  'momentum': 0.9,\n",
       "  'no_cuda': False,\n",
       "  'lr_step_size': 10,\n",
       "  'gamma': 0.1,\n",
       "  'task': 'classification'},\n",
       " 'model': 'LLN_cls_SGD',\n",
       " 'task': '337-361063'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_info = ex.begin_run()\n",
    "run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Loading datasets ----\n",
      "downloading task 337-361063\n",
      "task different than previous task, downloading...\n",
      "----1th training epoch ----\n",
      "----2th training epoch ----\n",
      "----3th training epoch ----\n",
      "----4th training epoch ----\n",
      "----5th training epoch ----\n",
      "----6th training epoch ----\n",
      "----7th training epoch ----\n",
      "----8th training epoch ----\n",
      "----9th training epoch ----\n",
      "----10th training epoch ----\n",
      "----11th training epoch ----\n",
      "----12th training epoch ----\n",
      "----13th training epoch ----\n",
      "----14th training epoch ----\n",
      "----15th training epoch ----\n",
      "----16th training epoch ----\n",
      "----17th training epoch ----\n",
      "----18th training epoch ----\n",
      "----19th training epoch ----\n",
      "----20th training epoch ----\n",
      "----21th training epoch ----\n",
      "----22th training epoch ----\n",
      "----23th training epoch ----\n",
      "----24th training epoch ----\n",
      "----25th training epoch ----\n",
      "----26th training epoch ----\n",
      "----27th training epoch ----\n",
      "----28th training epoch ----\n",
      "----29th training epoch ----\n",
      "----30th training epoch ----\n"
     ]
    }
   ],
   "source": [
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import get_train_val_test, CustomDataset\n",
    "from NeuralNetworksTrainingPackage.metrics.basic_metrics import calc_metrics\n",
    "from models.LcnNetwork import *\n",
    "from training.LcnTrain import *\n",
    "\n",
    "args = Hyperparams(**run_info.get('hyp'))\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "data_pre_processing.set_seed_for_all(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "print('---- Loading datasets ----')\n",
    "X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "train_data, val_data, test_data = get_train_val_test(X, y, categorical_indicator, attribute_names,\n",
    "                                                        data_pre_processing,\n",
    "                                                        task = args.task,\n",
    "                                                        model = run_info.get('model'),\n",
    "                                                        split = [0.5, 0.25, 0.25],\n",
    "                                                        args = args) # Returns CustomDataset obj instances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, # CustomDataset obj can be directly passed to dataloader\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                shuffle= True)\n",
    "\n",
    "\n",
    "train_eval_dataloader = torch.utils.data.DataLoader(train_data, # CustomDataset obj can be directly passed to dataloader\n",
    "                                                    batch_size=len(train_data),\n",
    "                                                    shuffle= True) # Required for test_metrics()\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, # CustomDataset obj can be directly passed to dataloader\n",
    "                                                batch_size=len(val_data),\n",
    "                                                shuffle= True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, # CustomDataset obj can be directly passed to dataloader\n",
    "                                                batch_size=len(test_data),\n",
    "                                                shuffle= True)\n",
    "\n",
    "\n",
    "model = Net(input_dim= args.input_dim, \n",
    "            output_dim= args.output_dim, \n",
    "            hidden_dim= args.hidden_dim, \n",
    "            num_layer= args.depth, \n",
    "            num_back_layer= args.back_n, \n",
    "            dense= True,\n",
    "            drop_type= args.drop_type,\n",
    "            net_type= args.net_type,\n",
    "            approx= args.anneal).to(device)\n",
    "\n",
    "\n",
    "if args.optimizer == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n",
    "elif args.optimizer == 'AMSGrad':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.gamma)\n",
    "\n",
    "\n",
    "metrics = {}\n",
    "start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n",
    "if args.anneal == 'approx':\n",
    "    args.net_type = 'approx_' + args.net_type\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(start_epoch, args.epochs + start_epoch):\n",
    "    print(f\"----{epoch}th training epoch ----\")\n",
    "    epoch_metrics = {}\n",
    "\n",
    "    scheduler.step(epoch)\n",
    "\n",
    "    alpha = get_alpha(epoch, args.epochs)\n",
    "    train_approximate_loss = train(args, model, device, train_dataloader, optimizer, epoch, args.anneal, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torcheval in /opt/conda/lib/python3.10/site-packages (0.0.7)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torcheval) (4.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For evaluating time\n",
    "import time\n",
    "\n",
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"The function took {end_time - start_time} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 0.9189419746398926 seconds to execute.\n",
      "The function took 1.0482335090637207 seconds to execute.\n",
      "The function took 0.9133145809173584 seconds to execute.\n",
      "The function took 0.9159131050109863 seconds to execute.\n",
      "The function took 0.9191830158233643 seconds to execute.\n",
      "The function took 0.9187009334564209 seconds to execute.\n",
      "The function took 0.9281127452850342 seconds to execute.\n",
      "The function took 0.9227919578552246 seconds to execute.\n",
      "The function took 1.0493524074554443 seconds to execute.\n",
      "The function took 0.9110701084136963 seconds to execute.\n",
      "The function took 0.9193122386932373 seconds to execute.\n",
      "The function took 0.9211428165435791 seconds to execute.\n",
      "The function took 0.9248418807983398 seconds to execute.\n",
      "The function took 0.9229333400726318 seconds to execute.\n",
      "The function took 0.9151473045349121 seconds to execute.\n",
      "The function took 0.9212911128997803 seconds to execute.\n",
      "The function took 0.9221484661102295 seconds to execute.\n",
      "The function took 1.0537025928497314 seconds to execute.\n",
      "The function took 0.930368185043335 seconds to execute.\n",
      "The function took 0.9189765453338623 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8692, dtype=torch.float64),\n",
       " tensor([0.9390, 0.9390], dtype=torch.float64),\n",
       " tensor([[1451.,  237.],\n",
       "         [ 204., 1480.]], device='cuda:0'))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = LcnMetricsClassification()\n",
    "for _ in range(20):\n",
    "    score = metrics.get_metrics(model, val_dataloader, device)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 0.9419999122619629 seconds to execute.\n",
      "The function took 0.9360606670379639 seconds to execute.\n",
      "The function took 0.9318320751190186 seconds to execute.\n",
      "The function took 0.926185131072998 seconds to execute.\n",
      "The function took 0.9490854740142822 seconds to execute.\n",
      "The function took 1.0689499378204346 seconds to execute.\n",
      "The function took 0.9393575191497803 seconds to execute.\n",
      "The function took 0.9274029731750488 seconds to execute.\n",
      "The function took 0.9264748096466064 seconds to execute.\n",
      "The function took 0.9338750839233398 seconds to execute.\n",
      "The function took 0.9263033866882324 seconds to execute.\n",
      "The function took 0.9285609722137451 seconds to execute.\n",
      "The function took 1.0509958267211914 seconds to execute.\n",
      "The function took 0.9219207763671875 seconds to execute.\n",
      "The function took 0.9202761650085449 seconds to execute.\n",
      "The function took 0.9375119209289551 seconds to execute.\n",
      "The function took 0.9305665493011475 seconds to execute.\n",
      "The function took 0.9239964485168457 seconds to execute.\n",
      "The function took 0.9175198078155518 seconds to execute.\n",
      "The function took 0.9234974384307861 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8692, dtype=torch.float64),\n",
       " tensor([0.9390, 0.9390], dtype=torch.float64),\n",
       " tensor([[1451.,  237.],\n",
       "         [ 204., 1480.]], device='cuda:0'))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = LcnMetricsClassification()\n",
    "for _ in range(20):\n",
    "    score = metrics.get_metrics(model, val_dataloader, device)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 1.0151679515838623 seconds to execute.\n",
      "The function took 0.8488566875457764 seconds to execute.\n",
      "The function took 0.8456614017486572 seconds to execute.\n",
      "The function took 0.8461451530456543 seconds to execute.\n",
      "The function took 0.8485949039459229 seconds to execute.\n",
      "The function took 0.848900556564331 seconds to execute.\n",
      "The function took 0.8555574417114258 seconds to execute.\n",
      "The function took 0.8454968929290771 seconds to execute.\n",
      "The function took 0.8552320003509521 seconds to execute.\n",
      "The function took 0.8611528873443604 seconds to execute.\n",
      "The function took 0.8606765270233154 seconds to execute.\n",
      "The function took 0.9763870239257812 seconds to execute.\n",
      "The function took 0.858640193939209 seconds to execute.\n",
      "The function took 0.8529043197631836 seconds to execute.\n",
      "The function took 0.851269006729126 seconds to execute.\n",
      "The function took 0.8486208915710449 seconds to execute.\n",
      "The function took 0.8478198051452637 seconds to execute.\n",
      "The function took 0.8446660041809082 seconds to execute.\n",
      "The function took 0.8521640300750732 seconds to execute.\n",
      "The function took 0.8511779308319092 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.31648332227025877}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = LcnMetricsClassification()\n",
    "for _ in range(20):\n",
    "    score = metrics.get_loss(model, val_dataloader, device)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torcheval.metrics import BinaryAUROC\n",
    "from torcheval.metrics import BinaryAccuracy\n",
    "from torcheval.metrics.toolkit import sync_and_compute\n",
    "\n",
    "\n",
    "class LcnMetricsClassification():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @timer_decorator\n",
    "    def get_loss(self, model, test_loader, device, test_set_name = None):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            dataset_len = 0\n",
    "\n",
    "\n",
    "            for data, target in test_loader:\n",
    "                dataset_len += len(target)\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                target = target.type(torch.cuda.LongTensor)\n",
    "\n",
    "                ###############\n",
    "                data.requires_grad = True\n",
    "                if model.net_type == 'locally_constant':\n",
    "                    output, relu_masks = model(data, p=0, training=False)\n",
    "                elif model.net_type == 'locally_linear':\n",
    "                    output, relu_masks = model.normal_forward(data, p=0, training=False)\n",
    "                ###############\n",
    "\n",
    "                target_one_dim = torch.argmax(target, dim=1)\n",
    "                test_loss += F.cross_entropy(output, target_one_dim, reduction='sum').item()\n",
    "\n",
    "            test_loss /= dataset_len\n",
    "            \n",
    "            if test_set_name:\n",
    "                assert isinstance(test_set_name, str), \"test_set_name must be a string, such as train, val, test\"\n",
    "                test_set_name = f'{test_set_name}_loss'\n",
    "                final_metrics = {test_set_name: test_loss}\n",
    "            else:\n",
    "                final_metrics = {'loss': test_loss}\n",
    "\n",
    "            return final_metrics\n",
    "\n",
    "    @timer_decorator\n",
    "    def get_metrics(self, model, test_loader, device):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            dataset_len = 0\n",
    "            \n",
    "            roc_auc_accumulator = BinaryAUROC(num_tasks=2)\n",
    "            acc_accumulator = BinaryAUROC()\n",
    "            confusion_accumulator = ConfusionMatrix()\n",
    "            \n",
    "\n",
    "            for data, target in test_loader:\n",
    "                dataset_len += len(target)\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                target = target.type(torch.cuda.LongTensor)\n",
    "\n",
    "                ###############\n",
    "                data.requires_grad = True\n",
    "                if model.net_type == 'locally_constant':\n",
    "                    output, relu_masks = model(data, p=0, training=False)\n",
    "                elif model.net_type == 'locally_linear':\n",
    "                    output, relu_masks = model.normal_forward(data, p=0, training=False)\n",
    "                ###############\n",
    "\n",
    "                \n",
    "                target_one_dim = torch.argmax(target, dim=1)\n",
    "                test_loss += F.cross_entropy(output, target_one_dim, reduction='sum').item()\n",
    "                output = torch.softmax(output, dim=-1)\n",
    "                \n",
    "                \n",
    "                preds = torch.argmax(output, dim=1).cpu()\n",
    "                target_one_dim = target_one_dim.cpu()\n",
    "                output = output.cpu()\n",
    "                \n",
    "                \n",
    "                \n",
    "                roc_auc_accumulator.update(output.t(), target.t())\n",
    "                acc_accumulator.update(preds, target_one_dim)\n",
    "                confusion_accumulator.update(preds, target_one_dim, num_classes = target.shape[1])\n",
    "                \n",
    "                \n",
    "            \n",
    "            roc_auc = sync_and_compute(roc_auc_accumulator)\n",
    "            accuracy = sync_and_compute(acc_accumulator)\n",
    "            confusion_matrix = confusion_accumulator.compute()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            return accuracy, roc_auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    def __init__(self, num_classes = None):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        if num_classes:\n",
    "            self.matrix = torch.zeros((num_classes, num_classes), device='cuda')\n",
    "        else:\n",
    "            self.matrix = num_classes\n",
    "\n",
    "    def update(self, outputs, targets, num_classes = 2):\n",
    "        if self.matrix is None:\n",
    "            self.num_classes = num_classes\n",
    "            self.matrix = torch.zeros((num_classes, num_classes), device='cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for t, p in zip(targets.view(-1), outputs.view(-1)):\n",
    "                self.matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    def compute(self):\n",
    "        return self.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function took 1.7190251350402832 seconds to execute.\n",
      "The function took 1.8374691009521484 seconds to execute.\n",
      "The function took 1.7214810848236084 seconds to execute.\n",
      "The function took 1.7017178535461426 seconds to execute.\n",
      "The function took 1.692218542098999 seconds to execute.\n",
      "The function took 1.811413049697876 seconds to execute.\n",
      "The function took 1.6977145671844482 seconds to execute.\n",
      "The function took 1.693662166595459 seconds to execute.\n",
      "The function took 1.719682216644287 seconds to execute.\n",
      "The function took 1.704272985458374 seconds to execute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'accuracy_score': 0.8692170818505338,\n",
       "  'roc_auc_score': 0.9390042608999112,\n",
       "  'confusion_matrix': [[1451, 237], [204, 1480]]},\n",
       " 0.31648335847141906)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    train_metrics = get_metrics(args, model, device, val_dataloader, calc_metrics, 'train')\n",
    "    \n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@timer_decorator\n",
    "def get_metrics(args, model, device, test_loader, metrics_func, test_set_name):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        # ==============================================================\n",
    "        # ===TODO: Add batched dataloader handling\n",
    "        # ==============================================================\n",
    "\n",
    "        data, target = next(iter(test_loader))\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        if args.task == 'classification':\n",
    "            target = target.type(torch.cuda.LongTensor)\n",
    "\n",
    "        ###############\n",
    "        data.requires_grad = True\n",
    "        if model.net_type == 'locally_constant':\n",
    "            output, relu_masks = model(data, p=0, training=False)\n",
    "        elif model.net_type == 'locally_linear':\n",
    "            output, relu_masks = model.normal_forward(data, p=0, training=False)\n",
    "        ###############\n",
    "\n",
    "        if args.task == 'classification':\n",
    "            output = torch.softmax(output, dim=-1)\n",
    "            metrics = metrics_func(target, output, True)\n",
    "        elif args.task == 'regression':\n",
    "            metrics = metrics_func(target, output, False)\n",
    "            \n",
    "        loss = get_loss(args, model, device, test_loader, 'val')\n",
    "            \n",
    "\n",
    "        return metrics, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loss(args, model, device, test_loader, test_set_name):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        score = []\n",
    "        label = []\n",
    "        dataset_len = 0\n",
    "\n",
    "        pattern_to_pred = dict()\n",
    "        tree_x = []\n",
    "        tree_pattern = []\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            dataset_len += len(target)\n",
    "            label += list(target)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if args.task == 'classification':\n",
    "                target = target.type(torch.cuda.LongTensor)\n",
    "\n",
    "            ###############\n",
    "            data.requires_grad = True\n",
    "            if model.net_type == 'locally_constant':\n",
    "                output, relu_masks = model(data, p=0, training=False)\n",
    "            elif model.net_type == 'locally_linear':\n",
    "                output, relu_masks = model.normal_forward(data, p=0, training=False)\n",
    "            ###############\n",
    "\n",
    "            if args.task == 'classification':\n",
    "                # Modified: Bart\n",
    "                target_one_dim = torch.argmax(target, dim=1)\n",
    "                test_loss += F.cross_entropy(output, target_one_dim, reduction='sum').item()\n",
    "                # Removed: Bart\n",
    "                # output = torch.softmax(output, dim=-1)\n",
    "                # ...\n",
    "                # output = output[:, 1]\n",
    "            elif args.task == 'regression':\n",
    "                output = output.squeeze(-1)\n",
    "                test_loss += ((output - target) ** 2).mean().item() * len(target)\n",
    "\n",
    "        test_loss /= dataset_len\n",
    "\n",
    "        # Removed: Bart\n",
    "        # if args.task == 'classification':\n",
    "        # ...\n",
    "\n",
    "        return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "authorship_tag": "ABX9TyO6d7scFvmxtcd+gePzcxnN",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
