{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rAPhXEZ2SNKL","executionInfo":{"status":"ok","timestamp":1693236038217,"user_tz":-60,"elapsed":8820,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"965b54c5-92ad-4a44-b325-cb94c460bdbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: TabularExperimentTrackerClient 0.0.1\n","Uninstalling TabularExperimentTrackerClient-0.0.1:\n","  Successfully uninstalled TabularExperimentTrackerClient-0.0.1\n","Collecting git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n","  Cloning https://github.com/DanielWarfield1/TabularExperimentTrackerClient to /tmp/pip-req-build-y34nb4nq\n","  Running command git clone --filter=blob:none --quiet https://github.com/DanielWarfield1/TabularExperimentTrackerClient /tmp/pip-req-build-y34nb4nq\n","  Resolved https://github.com/DanielWarfield1/TabularExperimentTrackerClient to commit df52eac0ce37df983d93a1b76cb9f4380a27b40d\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: openml in /usr/local/lib/python3.10/dist-packages (from TabularExperimentTrackerClient==0.0.1) (0.14.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from TabularExperimentTrackerClient==0.0.1) (2.31.0)\n","Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.5.0)\n","Requirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (0.13.0)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.2.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.8.2)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.5.3)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.10.1)\n","Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.23.5)\n","Requirement already satisfied: minio in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (7.1.16)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (9.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->TabularExperimentTrackerClient==0.0.1) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->TabularExperimentTrackerClient==0.0.1) (2023.7.22)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->openml->TabularExperimentTrackerClient==0.0.1) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->openml->TabularExperimentTrackerClient==0.0.1) (1.16.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (3.2.0)\n","Building wheels for collected packages: TabularExperimentTrackerClient\n","  Building wheel for TabularExperimentTrackerClient (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for TabularExperimentTrackerClient: filename=TabularExperimentTrackerClient-0.0.1-py3-none-any.whl size=6378 sha256=00a32bb822040a3d7e80dc9ca70e128c8749da4ce06ef41d7e60b2d6a311ed28\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0kvulqby/wheels/f9/2e/f3/69345202c956e07475c8f2f55074ad4d97b3e6a976b70fafab\n","Successfully built TabularExperimentTrackerClient\n","Installing collected packages: TabularExperimentTrackerClient\n","Successfully installed TabularExperimentTrackerClient-0.0.1\n"]}],"source":["!pip uninstall TabularExperimentTrackerClient --y\n","!pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient"]},{"cell_type":"code","source":["!pip install numpy\n","!pip install pandas\n","!pip install scikit-learn\n","!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLuLJlf9StGX","executionInfo":{"status":"ok","timestamp":1693236064841,"user_tz":-60,"elapsed":26631,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"e1d0e845-4df6-4f07-b3f3-7f217d7d5504"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip uninstall NeuralNetworksTrainingPackage --y\n","!pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USjjPIj6rB88","executionInfo":{"status":"ok","timestamp":1693236075983,"user_tz":-60,"elapsed":11151,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"70b83e21-82cf-4a72-d401-514bcc63891c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: NeuralNetworksTrainingPackage 1.0.0\n","Uninstalling NeuralNetworksTrainingPackage-1.0.0:\n","  Successfully uninstalled NeuralNetworksTrainingPackage-1.0.0\n","Collecting git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage\n","  Cloning https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to /tmp/pip-req-build-_e0fsly6\n","  Running command git clone --filter=blob:none --quiet https://github.com/Bartosz-G/NeuralNetworksTrainingPackage /tmp/pip-req-build-_e0fsly6\n","  Resolved https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to commit add4df696e7f8e5949bdaa09bc67eda32de91139\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.5.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.2.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from NeuralNetworksTrainingPackage==1.0.0) (2.0.1+cu118)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2023.3)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->NeuralNetworksTrainingPackage==1.0.0) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->NeuralNetworksTrainingPackage==1.0.0) (16.0.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->NeuralNetworksTrainingPackage==1.0.0) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->NeuralNetworksTrainingPackage==1.0.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->NeuralNetworksTrainingPackage==1.0.0) (1.3.0)\n","Building wheels for collected packages: NeuralNetworksTrainingPackage\n","  Building wheel for NeuralNetworksTrainingPackage (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for NeuralNetworksTrainingPackage: filename=NeuralNetworksTrainingPackage-1.0.0-py3-none-any.whl size=4459 sha256=7adc052b06c7b4efd9a237f6f1c129eddf6b34049acca24e36261d52ae7c58dd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-83i5n1gu/wheels/82/01/e8/da81de869be2f6b8c193a6fc5c42ee3ff0b6b97a7b70cd565a\n","Successfully built NeuralNetworksTrainingPackage\n","Installing collected packages: NeuralNetworksTrainingPackage\n","Successfully installed NeuralNetworksTrainingPackage-1.0.0\n"]}]},{"cell_type":"code","source":["from NNTraining import *\n","# Global namespace:\n","# Hyperparams(**run_info.get('hyp'))\n","# CustomDataset(X, Y, relative_indices, tensor_type=torch.float)\n","# CustomDatasetWrapper(train_dataset, relative_indices)\n","# kfold_dataloader_iterator(dataset, n_splits=10, random_state=42, batch_size=16, shuffle_kfold=True, shuffle_dataloader=True)\n","# get_train_test(X, y, categorical_indicator, attribute_names, train_split, seed)"],"metadata":{"id":"2i6wKLMbSm5M","executionInfo":{"status":"ok","timestamp":1693236078625,"user_tz":-60,"elapsed":2648,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import sklearn\n","import torch"],"metadata":{"id":"HBbb4I8IzD2Z","executionInfo":{"status":"ok","timestamp":1693236078626,"user_tz":-60,"elapsed":11,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from TabularExperimentTrackerClient.ExperimentClient import ExperimentClient\n","\n","# ==== Setup ====\n","#creating experiment client utilities\n","ex = ExperimentClient(verbose = True)\n","if True:\n","\n","    # BART\n","\n","    #getting openml credentials from drive\n","    ex.define_opml_cred_drive('/My Drive/research/non-homogenous-data/creds/creds-openml.txt')\n","    #getting orchestration credentials from drive\n","    ex.define_orch_cred_drive('bart', '/My Drive//research/non-homogenous-data/creds/creds-colab.txt')\n","\n","else:\n","\n","    # DANIEL\n","\n","    #getting openml credentials from drive\n","    ex.define_opml_cred_drive('/My Drive/Colab Notebooks/Non-Homogeneous Data/openMLAPIKey.txt')\n","    #getting orchestration credentials from drive\n","    ex.define_orch_cred_drive('test1', '/My Drive/Colab Notebooks/Non-Homogeneous Data/tabExpTrackAPIKey.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hloNgGeKSypk","executionInfo":{"status":"ok","timestamp":1693236084099,"user_tz":-60,"elapsed":5482,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"bf54ee2a-7e07-4f8c-96a8-f021805898a2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Parameters varying\n","depth = {'distribution': 'int_uniform', 'min':1, 'max':11}\n","seed = {'distribution': 'constant', 'value': 42}\n","drop_type = {'distribution': 'categorical', 'values':['node_dropconnect', 'none']}\n","p = {'distribution': 'float_uniform', 'min':0, 'max':0.999} # Must be in the range [0, 1)\n","back_n = {'distribution': 'categorical', 'values':[0, 0, 0, 1]}\n","hidden_dim = {'distribution': 'constant', 'value': 1} # Assertion error coming from Net if not 1\n","anneal = {'distribution': 'categorical', 'values':['interpolation', 'none', 'approx']}\n","batch_size = {'distribution': 'categorical', 'values':[16,32,64,64,64,128,256]}\n","epochs = {'distribution': 'categorical', 'values':[30, 60, 90]}\n","lr = {'distribution': 'log_uniform', 'min':0.05, 'max':0.2} # yields mean = 0.1082, median 0.1\n","momentum = {'distribution': 'constant', 'value': 0.9}\n","no_cuda = {'distribution': 'constant', 'value': False}\n","lr_step_size = {'distribution': 'categorical', 'values':[10, 10, 15, 20]}\n","gamma = {'distribution': 'constant', 'value': 0.1}\n","\n","\n","\n","\n","# Regression Spaces\n","LCN_reg_SGD_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'regression'}\n","    }\n","\n","LCN_reg_AMSGrad_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'regression'}\n","    }\n","\n","LLN_reg_SGD_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'regression'}\n","    }\n","\n","\n","LLN_reg_AMSGrad_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'regression'}\n","    }\n","\n","# Classification spaces\n","\n","LCN_cls_SGD_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'classification'}\n","    }\n","\n","LCN_cls_AMSGrad_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'classification'}\n","    }\n","\n","LLN_cls_SGD_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'classification'}\n","    }\n","\n","\n","LLN_cls_AMSGrad_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'classification'}\n","    }\n"],"metadata":{"id":"Nwt_9-JMTTft","executionInfo":{"status":"ok","timestamp":1693236084100,"user_tz":-60,"elapsed":8,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Testing Hyperparameter Spaces"],"metadata":{"id":"MP1WbmGEjSKT"}},{"cell_type":"code","source":["ex.monte_carlo_sample_space({\n","    # 'depth': depth,\n","    # 'seed': seed,\n","    'drop_type': drop_type,\n","    # 'p': p,\n","    # 'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    # 'shrinkage': {'distribution': 'constant', 'value': 1},\n","    # 'back_n': back_n,\n","    # 'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    # 'hidden_dim': hidden_dim,\n","    # 'anneal': anneal,\n","    # 'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    # 'batch_size': batch_size,\n","    # 'epochs': epochs,\n","    # 'lr': lr,\n","    # 'momentum': momentum,\n","    # 'no_cuda': no_cuda,\n","    # 'lr_step_size': lr_step_size,\n","    # 'gamma': gamma,\n","    # 'task': {'distribution': 'constant', 'value': 'classification'}\n","    }, 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3BMwdFWjQi8","executionInfo":{"status":"ok","timestamp":1693236084907,"user_tz":-60,"elapsed":813,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"6bd61f1e-e699-45c7-eb33-706229c1019f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["sampled 1 points in the space:\n","{'drop_type': {'distribution': 'categorical', 'values': ['node_dropconnect', 'none']}}\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'drop_type': 'none'}]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["model_groups = {\n","    'LCN_reg_SGD':{'model':'LCN_reg_SGD', 'hype':LCN_reg_SGD_space},\n","    'LCN_reg_AMSGrad':{'model':'LCN_reg_AMSGrad', 'hype':LCN_reg_AMSGrad_space},\n","    'LLN_reg_SGD':{'model':'LLN_reg_SGD', 'hype':LLN_reg_SGD_space},\n","    'LLN_reg_AMSGrad':{'model':'LLN_reg_AMSGrad', 'hype':LLN_reg_AMSGrad_space},\n","    'LCN_cls_SGD':{'model':'LCN_cls_SGD', 'hype':LCN_cls_SGD_space},\n","    'LCN_cls_AMSGrad':{'model':'LCN_cls_AMSGrad', 'hype':LCN_cls_AMSGrad_space},\n","    'LLN_cls_SGD':{'model':'LLN_cls_SGD', 'hype':LLN_cls_SGD_space},\n","    'LLN_cls_AMSGrad':{'model':'LLN_cls_AMSGrad', 'hype':LLN_cls_AMSGrad_space},\n","}\n","\n","ex.def_model_groups(model_groups)"],"metadata":{"id":"cyzQSiOtzLBa","executionInfo":{"status":"ok","timestamp":1693236084907,"user_tz":-60,"elapsed":13,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["ex.def_data_groups_opml()\n","print('automatically defined data groups:')\n","print(ex.data_groups.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_hWhsfs9Lem","executionInfo":{"status":"ok","timestamp":1693236084908,"user_tz":-60,"elapsed":12,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"6d8627ed-d135-4f57-fc73-9ccaa7bd34d6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["automatically defined data groups:\n","dict_keys(['opml_reg_purnum_group', 'opml_class_purnum_group', 'opml_reg_numcat_group', 'opml_class_numcat_group'])\n"]}]},{"cell_type":"code","source":["classification_models = [k for k in model_groups.keys() if '_cls' in k]\n","regression_models = [k for k in model_groups.keys() if '_reg' in k]\n","\n","\n","applications = {'opml_reg_purnum_group': regression_models,\n","                'opml_reg_numcat_group': regression_models,\n","                'opml_class_purnum_group': classification_models,\n","                'opml_class_numcat_group': classification_models}\n","\n","ex.def_applications(applications)\n","ex.reg_experiment('Testing_LCN_5')\n","# ex.reg_experiment('20230822')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"9_3lujFq9gxJ","executionInfo":{"status":"ok","timestamp":1693236085904,"user_tz":-60,"elapsed":1001,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"300404d5-2342-4b47-abbb-aca801181b7a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["existing experiment found\n"]},{"output_type":"execute_result","data":{"text/plain":["'existing experiment found'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["run_info = ex.begin_run_sticky()\n","run_info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-j28taEW_XxU","executionInfo":{"status":"ok","timestamp":1693236087653,"user_tz":-60,"elapsed":1754,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"2270a445-0765-4ced-ec16-079f8e65ca10"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["{'_id': '64ecbb76efdc7a33ff89ab6e', 'metrics_per_epoch': [], 'experiment_id': '64eca8f8b6a3ffa056edfaa5', 'experiment_name': 'Testing_LCN_5', 'mtpair_index': 220, 'mtpair_model': 'LCN_cls_SGD', 'mtpair_task': '334-361282', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 1, 'seed': 42, 'drop_type': 'none', 'p': 0.1902997095468926, 'ensemble_n': 1, 'shrinkage': 1, 'back_n': 0, 'net_type': 'locally_constant', 'hidden_dim': 1, 'anneal': 'approx', 'optimizer': 'SGD', 'batch_size': 32, 'epochs': 60, 'lr': 0.06694289668098778, 'momentum': 0.9, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1, 'task': 'classification'}, 'model': 'LCN_cls_SGD', 'task': '334-361282'}\n","4ecbb76efdc7a33ff89ab6\n"]},{"output_type":"execute_result","data":{"text/plain":["{'_id': '64ecbb76efdc7a33ff89ab6e',\n"," 'metrics_per_epoch': [],\n"," 'experiment_id': '64eca8f8b6a3ffa056edfaa5',\n"," 'experiment_name': 'Testing_LCN_5',\n"," 'mtpair_index': 220,\n"," 'mtpair_model': 'LCN_cls_SGD',\n"," 'mtpair_task': '334-361282',\n"," 'is_completed': False,\n"," 'user_id': '64d3a7457658d6ec6db139d0',\n"," 'user_name': 'bart',\n"," 'hyp': {'depth': 1,\n","  'seed': 42,\n","  'drop_type': 'none',\n","  'p': 0.1902997095468926,\n","  'ensemble_n': 1,\n","  'shrinkage': 1,\n","  'back_n': 0,\n","  'net_type': 'locally_constant',\n","  'hidden_dim': 1,\n","  'anneal': 'approx',\n","  'optimizer': 'SGD',\n","  'batch_size': 32,\n","  'epochs': 60,\n","  'lr': 0.06694289668098778,\n","  'momentum': 0.9,\n","  'no_cuda': False,\n","  'lr_step_size': 10,\n","  'gamma': 0.1,\n","  'task': 'classification'},\n"," 'model': 'LCN_cls_SGD',\n"," 'task': '334-361282'}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["run_info.get('hyp')"],"metadata":{"id":"eHPE_iIMAr2_","executionInfo":{"status":"ok","timestamp":1693236087654,"user_tz":-60,"elapsed":19,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b2029b35-7d56-44b3-a1ef-06a40042f53f"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'depth': 1,\n"," 'seed': 42,\n"," 'drop_type': 'none',\n"," 'p': 0.1902997095468926,\n"," 'ensemble_n': 1,\n"," 'shrinkage': 1,\n"," 'back_n': 0,\n"," 'net_type': 'locally_constant',\n"," 'hidden_dim': 1,\n"," 'anneal': 'approx',\n"," 'optimizer': 'SGD',\n"," 'batch_size': 32,\n"," 'epochs': 60,\n"," 'lr': 0.06694289668098778,\n"," 'momentum': 0.9,\n"," 'no_cuda': False,\n"," 'lr_step_size': 10,\n"," 'gamma': 0.1,\n"," 'task': 'classification'}"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["ex.opml_load_task(run_info['mtpair_task'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jEa1jmiXaS6","executionInfo":{"status":"ok","timestamp":1693236109487,"user_tz":-60,"elapsed":21843,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"9be2a031-1bde-4f53-879a-a8fa33eb9084"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading task 334-361282\n","task different than previous task, downloading...\n"]},{"output_type":"execute_result","data":{"text/plain":["(         V1     V2    V3  V4      V5     V6    V7  V8     V9  V10  ...    V43  \\\n"," 0       1.0    2.0   5.0   4  1092.0    4.0   3.0   9   36.0  1.0  ...   50.0   \n"," 1      14.0    0.0   1.0   3   247.0    3.0  14.0  14   13.0  3.0  ...    1.0   \n"," 2       1.0    3.0  12.0   8  1080.0   38.0   7.0  38  136.0  1.0  ...    2.0   \n"," 3       1.0    5.0   9.0  16     2.0    5.0   4.0  18   88.0  1.0  ...   38.0   \n"," 4       1.0    0.0   2.0   9    54.0   13.0   1.0   4    9.0  1.0  ...   21.0   \n"," ...     ...    ...   ...  ..     ...    ...   ...  ..    ...  ...  ...    ...   \n"," 58247   1.0   45.0  30.0  20    28.0   20.0   2.0  23  101.0  1.0  ...   28.0   \n"," 58248   3.0    1.0   4.0   4  1002.0  132.0  40.0  39  329.0  1.0  ...   64.0   \n"," 58249   2.0    1.0   3.0  29    21.0   36.0   3.0  31  119.0  1.0  ...  597.0   \n"," 58250   3.0  180.0  17.0   5  1042.0  100.0   3.0  48   36.0  1.0  ...    5.0   \n"," 58251   0.0    2.0   1.0   2  4031.0   39.0   4.0   1   20.0  0.0  ...    5.0   \n"," \n","        V45 V47  V50 V51 V52   V59 V63  V72 V75  \n"," 0        1   0  6.0   2   0   0.0   6    3   2  \n"," 1        1  10  2.0   0   1  14.0   7   30   2  \n"," 2        1   8  2.0   1   1   0.0   8    1   1  \n"," 3        6   5  4.0  12   0   9.0   6    2   0  \n"," 4        1   5  0.0   0   1   5.0   7    1   2  \n"," ...    ...  ..  ...  ..  ..   ...  ..  ...  ..  \n"," 58247    8   8  6.0   1   0   0.0   1    7   0  \n"," 58248    1   8  2.0   1   1   4.0   6    1  19  \n"," 58249    0  10  2.0   1   0   0.0   6    8   2  \n"," 58250    6   8  0.0   1   0   1.0   6   19   1  \n"," 58251    1   8  5.0  10   0   0.0   7    4   2  \n"," \n"," [58252 rows x 31 columns],\n"," 0        0\n"," 1        0\n"," 2        0\n"," 3        0\n"," 4        0\n","         ..\n"," 58247    1\n"," 58248    1\n"," 58249    1\n"," 58250    1\n"," 58251    1\n"," Name: class, Length: 58252, dtype: category\n"," Categories (2, object): ['0' < '1'],\n"," [False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  True,\n","  True,\n","  True,\n","  True,\n","  True,\n","  True,\n","  False,\n","  True,\n","  False,\n","  False,\n","  True,\n","  True,\n","  False,\n","  False,\n","  False,\n","  False,\n","  True,\n","  False,\n","  False],\n"," ['V1',\n","  'V2',\n","  'V3',\n","  'V4',\n","  'V5',\n","  'V6',\n","  'V7',\n","  'V8',\n","  'V9',\n","  'V10',\n","  'V11',\n","  'V13',\n","  'V19',\n","  'V22',\n","  'V30',\n","  'V33',\n","  'V35',\n","  'V36',\n","  'V40',\n","  'V41',\n","  'V42',\n","  'V43',\n","  'V45',\n","  'V47',\n","  'V50',\n","  'V51',\n","  'V52',\n","  'V59',\n","  'V63',\n","  'V72',\n","  'V75'])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from NNTraining import *\n","args = Hyperparams(**run_info.get('hyp'))"],"metadata":{"id":"t7QejAbP-tGO","executionInfo":{"status":"ok","timestamp":1693236109487,"user_tz":-60,"elapsed":6,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# 1. LCN Model"],"metadata":{"id":"c6fWISMKznlN"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","def my_softplus(x, tau=1., threshold=20.):\n","    truncate_mask = (x > threshold).type(torch.cuda.FloatTensor)\n","    return truncate_mask * x + (1. - truncate_mask) * (tau * torch.log(1 + torch.exp((1. - truncate_mask) * x / tau)))\n","\n","def my_softplus_derivative(x, tau=1., threshold=20.):\n","    truncate_mask = (x > threshold).type(torch.cuda.FloatTensor)\n","    return truncate_mask + (1. - truncate_mask) / (1 + torch.exp(- (1. - truncate_mask) * x / tau) )\n","\n","class Net(nn.Module):\n","    def __init__(self,\n","            input_dim,\n","            output_dim,\n","            hidden_dim,\n","            num_layer,\n","            num_back_layer,\n","            dense = False,\n","            drop_type = 'none',\n","            net_type = 'locally_constant',\n","            approx = 'none'):\n","        super(Net, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layer = num_layer\n","        self.num_back_layer = num_back_layer\n","        self.dense = dense\n","        self.drop_type = drop_type\n","        self.net_type = net_type\n","        self.n_neuron = self.hidden_dim * self.num_layer\n","        self.approx = approx\n","\n","        self.layer = nn.ModuleList()\n","\n","        self.weights = dict()\n","        self.biases = dict()\n","        self.output_constants = dict()\n","        self.output_weights = dict()\n","        self.output_biases = dict()\n","\n","        accu_dim = input_dim\n","        self.weight_masks = []\n","        for i in range(self.num_layer):\n","            self.layer.append(nn.Linear(accu_dim, hidden_dim))\n","            ite_mask = np.zeros((1, hidden_dim, accu_dim))\n","            pick = np.random.choice(accu_dim, int(np.sqrt(accu_dim)), replace=False)\n","            ite_mask[0, 0, pick] = 1\n","            assert(hidden_dim == 1)\n","            ite_mask = torch.tensor(ite_mask.astype(np.float32)).cuda()\n","            self.weight_masks.append(ite_mask)\n","\n","            if self.dense:\n","                accu_dim += hidden_dim\n","            else:\n","                accu_dim = hidden_dim\n","\n","        if self.net_type == 'locally_constant':\n","            self.backward_layer = nn.ModuleList()\n","            backward_dim = 256\n","            cur_dim = self.num_layer * self.hidden_dim * (1 + self.input_dim)\n","            for i in range(self.num_back_layer):\n","                self.backward_layer.append(nn.Linear(cur_dim, backward_dim))\n","                cur_dim = backward_dim\n","            self.backward_layer.append(nn.Linear(cur_dim, output_dim))\n","\n","        elif self.net_type == 'locally_linear':\n","            self.output_fc = nn.Linear(accu_dim, self.output_dim)\n","        else:\n","            print('net_type', self.net_type, 'is not supported')\n","            exit(0)\n","\n","    def normal_forward(self, init_layer, p=0, training=True):\n","        assert(self.net_type == 'locally_linear')\n","        relu_masks = []\n","        if len(init_layer.shape) == 4:\n","            bz = init_layer.shape[0]\n","            init_layer = init_layer.view(bz, -1)\n","        cur_embed = init_layer\n","        batch_size = init_layer.shape[0]\n","\n","        for i in range(self.num_layer):\n","            if training == False:\n","                next_embed = self.layer[i](cur_embed)\n","            else:\n","                w = self.layer[i].weight\n","                w = w.view(1, w.shape[0], w.shape[1]).expand(batch_size, -1, -1)\n","\n","                b = self.layer[i].bias\n","                b = b.view(1, b.shape[0]).expand(batch_size, -1)\n","\n","                # Error appears\n","                print('==== Place 1 ======')\n","                print(f'cur_embed:{cur_embed.shape}')\n","                print(f'cur_embed.unsqueeze(-1):{cur_embed.unsqueeze(-1).shape}')\n","                print(f'b.shape:{b.shape}')\n","                print(f'b.unsqueeze(-1).shape: {b.unsqueeze(-1).shape}')\n","\n","                next_embed = torch.bmm(F.dropout(w, p=p, training=training), cur_embed.unsqueeze(-1)) + b.unsqueeze(-1)\n","                next_embed = next_embed.squeeze(-1)\n","\n","            relu_masks.append( (next_embed > 0) )\n","            next_embed = F.relu(next_embed)\n","            if self.dense:\n","                cur_embed = torch.cat((cur_embed, next_embed), 1)\n","            else:\n","                cur_embed = next_embed\n","        return self.output_fc(cur_embed), relu_masks\n","\n","    def forward(self, init_layer, p=0, training=True, alpha=None, anneal='none'):\n","        assert(self.net_type == 'locally_constant')\n","        relu_masks = []\n","        if len(init_layer.shape) == 4:\n","            bz = init_layer.shape[0]\n","            init_layer = init_layer.view(bz, -1)\n","        # cur_embed is used for forward computation.\n","        cur_embed = init_layer\n","        # x is used to compute Jacobian using dynamic programming.\n","        x = init_layer.unsqueeze(-1)\n","\n","        batch_size = x.shape[0]\n","        patterns = []\n","        for i in range(self.num_layer):\n","            if self.drop_type != 'node_dropconnect':\n","                next_embed = self.layer[i](cur_embed)\n","            w = self.layer[i].weight\n","            w = w.view(1, w.shape[0], w.shape[1]).expand(batch_size, -1, -1)\n","\n","            if self.drop_type == 'node_dropconnect':\n","                b = self.layer[i].bias\n","                b = b.view(1, b.shape[0]).expand(batch_size, -1)\n","\n","                # Error appears\n","                print('==== Place 2 ======')\n","                print(f'cur_embed:{cur_embed.shape}')\n","                print(f'cur_embed.unsqueeze(-1):{cur_embed.unsqueeze(-1).shape}')\n","                print(f'b.shape:{b.shape}')\n","                print(f'b.unsqueeze(-1).shape: {b.unsqueeze(-1).shape}')\n","                next_embed = torch.bmm(F.dropout(w, p=p, training=training), cur_embed.unsqueeze(-1)) + b.unsqueeze(-1)\n","                next_embed = next_embed.squeeze(-1)\n","            else:\n","                pass\n","\n","            relu_masks.append( (next_embed > 0) )\n","            if self.approx == 'approx':\n","                neur_deriv = my_softplus_derivative(next_embed)\n","                next_embed = my_softplus(next_embed)\n","            elif anneal == 'interpolation':\n","                neur_deriv = alpha * (next_embed > 0).type(torch.cuda.FloatTensor) + (1 - alpha) * my_softplus_derivative(next_embed)\n","                next_embed = alpha * F.relu(next_embed) + (1 - alpha) * my_softplus(next_embed)\n","            elif anneal == 'none':\n","                neur_deriv = (next_embed > 0).type(torch.cuda.FloatTensor)\n","                next_embed = F.relu(next_embed)\n","\n","            patterns.append(neur_deriv)\n","            print('==== Place 3 ======')\n","            print(f'neur_deriv.unsqueeze(-1).shape: {neur_deriv.unsqueeze(-1).shape}')\n","            neur_deriv = neur_deriv.unsqueeze(-1)\n","\n","            if i == 0:\n","\n","                jacobians = neur_deriv * w\n","                # Error appears\n","                print('==== Place 4 ======')\n","                print(f'jacobians.shape: {jacobians.shape}')\n","                print(f'jacobians = {jacobians}')\n","                print(f'x.shape: {x.shape}')\n","                print(f'w.shape: {w.shape}')\n","                offsets = next_embed - torch.bmm(jacobians, x).squeeze(-1)\n","            else:\n","                if self.dense:\n","                    ite_jacobians = w[:, :, :self.input_dim] + torch.bmm(w[:, :, self.input_dim:], jacobians)\n","                else:\n","                    ite_jacobians = torch.bmm(w, jacobians[:, -self.hidden_dim:, :])\n","\n","                ite_jacobians = neur_deriv * ite_jacobians\n","                ite_offsets = next_embed - torch.bmm(ite_jacobians, x).squeeze(-1)\n","\n","                jacobians = torch.cat([jacobians, ite_jacobians], dim=1)\n","                offsets = torch.cat([offsets, ite_offsets], dim=1)\n","\n","            if self.dense:\n","                cur_embed = torch.cat((cur_embed, next_embed), 1)\n","            else:\n","                cur_embed = next_embed\n","\n","        leaf_input = torch.cat([jacobians, offsets.unsqueeze(-1)], dim=2).view(batch_size, -1)\n","        for i in range(self.num_back_layer):\n","            leaf_input = F.relu(self.backward_layer[i](leaf_input))\n","        leaf_output = self.backward_layer[-1](leaf_input)\n","\n","        return leaf_output, relu_masks"],"metadata":{"id":"-5AmSj4lzmpp","executionInfo":{"status":"ok","timestamp":1693236109487,"user_tz":-60,"elapsed":4,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# 2. LCN utils"],"metadata":{"id":"eLYG6KXkYs1m"}},{"cell_type":"code","source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def print_args(args):\n","    print(\"\\nParameters:\")\n","    for attr, value in sorted(args.__dict__.items()):\n","        print(\"\\t{}={}\".format(attr.upper(), value))"],"metadata":{"id":"S4MIlQ2tX9Xg","executionInfo":{"status":"ok","timestamp":1693236109487,"user_tz":-60,"elapsed":4,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# 3. LCN Test"],"metadata":{"id":"VlSqjhUHYzn6"}},{"cell_type":"code","source":["def train(args, model, device, train_loader, optimizer, epoch, anneal, alpha=1):\n","    model.train()\n","    dataset_len = 0\n","    avg_loss = AverageMeter()\n","\n","    for (data, target) in train_loader:\n","        dataset_len += len(target)\n","        data, target = data.to(device), target.to(device)\n","        if args.task == 'classification':\n","            target = target.type(torch.cuda.LongTensor)\n","\n","        optimizer.zero_grad()\n","        ###############\n","        data.requires_grad = True\n","        if model.net_type == 'locally_constant':\n","            if args.p != -1:\n","                assert(args.p >= 0. and args.p < 1)\n","                output, regularization = model(data, alpha=alpha, anneal=anneal, p=args.p, training=True)\n","            else:\n","                output, regularization = model(data, alpha=alpha, anneal=anneal, p=1-alpha, training=True)\n","\n","        elif model.net_type == 'locally_linear':\n","            output, regularization = model.normal_forward(data)\n","        ###############\n","\n","        optimizer.zero_grad()\n","        if args.task == 'classification':\n","            loss = F.cross_entropy(output, target)\n","        elif args.task == 'regression':\n","            output = output.squeeze(-1)\n","            loss = ((output - target) ** 2).mean()\n","\n","        loss.backward()\n","        optimizer.step()\n","        avg_loss.update(loss.item())\n","\n","    return avg_loss.avg\n","\n","def test(args, model, device, test_loader, test_set_name):\n","    with torch.no_grad():\n","        model.eval()\n","        test_loss = 0\n","        correct = 0\n","\n","        score = []\n","        label = []\n","        dataset_len = 0\n","\n","        pattern_to_pred = dict()\n","        tree_x = []\n","        tree_pattern = []\n","\n","        for data, target in test_loader:\n","            dataset_len += len(target)\n","            label += list(target)\n","            data, target = data.to(device), target.to(device)\n","            if args.task == 'classification':\n","                target = target.type(torch.cuda.LongTensor)\n","\n","            ###############\n","            data.requires_grad = True\n","            if model.net_type == 'locally_constant':\n","                output, relu_masks = model(data, p=0, training=False)\n","            elif model.net_type == 'locally_linear':\n","                output, relu_masks = model.normal_forward(data, p=0, training=False)\n","            ###############\n","\n","            if args.task == 'classification':\n","                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n","                output = torch.softmax(output, dim=-1)\n","                score += list(output[:, 1].cpu().data.numpy())\n","                pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n","                correct += pred.eq(target.view_as(pred)).sum().item()\n","                output = output[:, 1]\n","            elif args.task == 'regression':\n","                output = output.squeeze(-1)\n","                test_loss += ((output - target) ** 2).mean().item() * len(target)\n","\n","        test_loss /= dataset_len\n","        if args.task == 'classification':\n","            if args.output_dim == 2:\n","                AUC = roc_auc_score(label, score)\n","                test_score = AUC\n","            else:\n","                AUC = -1\n","                test_score = correct / dataset_len\n","\n","        elif args.task == 'regression':\n","            RMSE = np.sqrt(test_loss)\n","            test_score = -RMSE\n","\n","        return test_loss, test_score\n"],"metadata":{"id":"mL-qoJWeZF_f","executionInfo":{"status":"ok","timestamp":1693236109487,"user_tz":-60,"elapsed":3,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# 4. LCN Main"],"metadata":{"id":"aBgpmCRiZsGz"}},{"cell_type":"code","source":["def get_alpha(epoch, total_epoch):\n","    return float(epoch) / float(total_epoch)"],"metadata":{"id":"Ck1e9hNGZtj7","executionInfo":{"status":"ok","timestamp":1693236109487,"user_tz":-60,"elapsed":3,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# 5. LCN custom training loop"],"metadata":{"id":"XezqrpZxaKdn"}},{"cell_type":"code","source":["from NNTraining import *\n","\n","print('==== Begin run:====')\n","print('---- Initialising parameters for the run ----')\n","run_info = ex.begin_run_sticky()\n","\n","use_cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","np.random.seed(args.seed)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","args = Hyperparams(**run_info.get('hyp')) # hyperparameters for LCN need to be in form of an object (you can ignore this)\n","\n","print('---- Loading datasets ----')\n","X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n","train_data, test_data, input_dim, output_dim = get_train_test(X, y, categorical_indicator, attribute_names, 0.75, args.seed) # Returns CustomDataset obj instances\n","\n","\n","DataLoadersIter = kfold_dataloader_iterator(train_data, # train_data needs to be an instance of CustomDataset\n","                                            n_splits= 5,\n","                                            random_state= args.seed,\n","                                            batch_size= args.batch_size,\n","                                            shuffle_kfold= True,\n","                                            shuffle_dataloader= True)\n","\n","test_dataloader = torch.utils.data.DataLoader(test_data, # CustomDataset obj can be directly passed to dataloader\n","                                              batch_size=len(test_data),\n","                                              shuffle= True)\n","\n","for kfold, (train_dataloader, val_dataloader) in enumerate(DataLoadersIter):\n","  print(f\"==== {kfold}th kfold sweep starting ====\")\n","\n","  model = Net(input_dim= input_dim, output_dim= output_dim,\n","              hidden_dim=args.hidden_dim,\n","              num_layer=args.depth,\n","              num_back_layer=args.back_n,\n","              dense=True,\n","              drop_type=args.drop_type,\n","              net_type=args.net_type,\n","              approx=args.anneal).to(device)\n","\n","\n","  if args.optimizer == 'SGD':\n","    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n","  elif args.optimizer == 'AMSGrad':\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)\n","  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.gamma)\n","\n","\n","\n","  start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n","  if args.anneal == 'approx':\n","    args.net_type = 'approx_' + args.net_type\n","\n","\n","  for epoch in range(start_epoch, args.epochs + start_epoch):\n","      print(f\"----{epoch}th training epoch ----\")\n","      scheduler.step(epoch)\n","\n","      alpha = get_alpha(epoch, args.epochs)\n","      train_approximate_loss = train(args, model, device, train_dataloader, optimizer, epoch, args.anneal, alpha)\n","\n","      print(f'train_approximate_loss = {train_approximate_loss}')\n","\n","\n","      train_loss, train_score = test(args, model, device, train_dataloader, 'train')\n","      val_loss, val_score = test(args, model, device, val_dataloader, 'valid')\n","      test_loss, test_score = test(args, model, device, test_dataloader, 'test')\n","\n","      print(f'train_loss = {train_loss}, train_score = {train_score}')\n","      print(f'valid_loss = {val_loss}, valid_score = {val_score}')\n","      print(f'test_loss = {test_loss}, test_score = {test_score}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"altcS6fzaJ7T","executionInfo":{"status":"error","timestamp":1693236114099,"user_tz":-60,"elapsed":4615,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"ba83a2ca-f352-44f7-f383-03b944633004"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["==== Begin run:====\n","---- Initialising parameters for the run ----\n","{'_id': '64ecbb8d3d80ff57f4f241d2', 'metrics_per_epoch': [], 'experiment_id': '64eca8f8b6a3ffa056edfaa5', 'experiment_name': 'Testing_LCN_5', 'mtpair_index': 222, 'mtpair_model': 'LLN_cls_SGD', 'mtpair_task': '334-361282', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 7, 'seed': 42, 'drop_type': 'none', 'p': 0.6141091016042993, 'ensemble_n': 1, 'shrinkage': 1, 'back_n': 0, 'net_type': 'locally_linear', 'hidden_dim': 1, 'anneal': 'interpolation', 'optimizer': 'SGD', 'batch_size': 64, 'epochs': 60, 'lr': 0.19841675498605538, 'momentum': 0.9, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1, 'task': 'classification'}, 'model': 'LLN_cls_SGD', 'task': '334-361282'}\n","4ecbb8d3d80ff57f4f241d\n","---- Loading datasets ----\n","downloading task 334-361282\n","using values from previous task load, skipped download\n","==== 0th kfold sweep starting ====\n","----1th training epoch ----\n","==== Place 1 ======\n","cur_embed:torch.Size([64, 107])\n","cur_embed.unsqueeze(-1):torch.Size([64, 107, 1])\n","b.shape:torch.Size([64, 1])\n","b.unsqueeze(-1).shape: torch.Size([64, 1, 1])\n","==== Place 1 ======\n","cur_embed:torch.Size([64, 108])\n","cur_embed.unsqueeze(-1):torch.Size([64, 108, 1])\n","b.shape:torch.Size([64, 1])\n","b.unsqueeze(-1).shape: torch.Size([64, 1, 1])\n","==== Place 1 ======\n","cur_embed:torch.Size([64, 109])\n","cur_embed.unsqueeze(-1):torch.Size([64, 109, 1])\n","b.shape:torch.Size([64, 1])\n","b.unsqueeze(-1).shape: torch.Size([64, 1, 1])\n","==== Place 1 ======\n","cur_embed:torch.Size([64, 110])\n","cur_embed.unsqueeze(-1):torch.Size([64, 110, 1])\n","b.shape:torch.Size([64, 1])\n","b.unsqueeze(-1).shape: torch.Size([64, 1, 1])\n","==== Place 1 ======\n","cur_embed:torch.Size([64, 111])\n","cur_embed.unsqueeze(-1):torch.Size([64, 111, 1])\n","b.shape:torch.Size([64, 1])\n","b.unsqueeze(-1).shape: torch.Size([64, 1, 1])\n","==== Place 1 ======\n","cur_embed:torch.Size([64, 112])\n","cur_embed.unsqueeze(-1):torch.Size([64, 112, 1])\n","b.shape:torch.Size([64, 1])\n","b.unsqueeze(-1).shape: torch.Size([64, 1, 1])\n","==== Place 1 ======\n","cur_embed:torch.Size([64, 113])\n","cur_embed.unsqueeze(-1):torch.Size([64, 113, 1])\n","b.shape:torch.Size([64, 1])\n","b.unsqueeze(-1).shape: torch.Size([64, 1, 1])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-cb0d382d7875>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0mtrain_approximate_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manneal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train_approximate_loss = {train_approximate_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-6892e06775ec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch, anneal, alpha)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'classification'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'regression'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"]}]},{"cell_type":"code","source":["# from NNTraining import *\n","\n","# print('==== Begin run:====')\n","# print('---- Initialising parameters for the run ----')\n","# run_info = ex.begin_run_sticky()\n","\n","# use_cuda = not args.no_cuda and torch.cuda.is_available()\n","# torch.manual_seed(args.seed)\n","# np.random.seed(args.seed)\n","# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","# args = Hyperparams(**run_info.get('hyp')) # hyperparameters for LCN need to be in form of an object (you can ignore this)\n","\n","# print('---- Loading datasets ----')\n","# X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n","# train_data, test_data, input_dim, output_dim = get_train_test(X, y, categorical_indicator, attribute_names, 0.75, args.seed) # Returns CustomDataset obj instances\n","\n","\n","# DataLoadersIter = kfold_dataloader_iterator(train_data, # train_data needs to be an instance of CustomDataset\n","#                                             n_splits= 5,\n","#                                             random_state= args.seed,\n","#                                             batch_size= args.batch_size,\n","#                                             shuffle_kfold= True,\n","#                                             shuffle_dataloader= True)\n","\n","# test_dataloader = torch.utils.data.DataLoader(test_data, # CustomDataset obj can be directly passed to dataloader\n","#                                               batch_size=len(test_data),\n","#                                               shuffle= True)\n","\n","# for kfold, (train_dataloader, val_dataloader) in enumerate(DataLoadersIter):\n","#   print(f\"==== {kfold}th kfold sweep starting ====\")\n","#   print(next(iter(train_dataloader)))"],"metadata":{"id":"vNq8JY_DwjkY","executionInfo":{"status":"aborted","timestamp":1693236114101,"user_tz":-60,"elapsed":18,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reproducing errors:"],"metadata":{"id":"XIb9GKq0Y2Fv"}},{"cell_type":"code","source":["pred, out = next(iter(train_dataloader))"],"metadata":{"id":"QH8Vub1PBNc3","executionInfo":{"status":"aborted","timestamp":1693236114101,"user_tz":-60,"elapsed":17,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type_df = X.applymap(lambda x: str(type(x))).apply(lambda x: x.unique())\n","type_df"],"metadata":{"id":"dMiV9UVudjVj","executionInfo":{"status":"aborted","timestamp":1693236114102,"user_tz":-60,"elapsed":15,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type_df = y.to_frame().applymap(lambda x: str(type(x))).apply(lambda x: x.unique())\n","type_df"],"metadata":{"id":"n0HtX7JsdyA0","executionInfo":{"status":"aborted","timestamp":1693236114102,"user_tz":-60,"elapsed":15,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type_df = train_data.X.applymap(lambda x: str(type(x))).apply(lambda x: x.unique())\n","type_df"],"metadata":{"id":"tRlCRo3VeVKh","executionInfo":{"status":"aborted","timestamp":1693236114102,"user_tz":-60,"elapsed":14,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type_df = train_data.Y.applymap(lambda x: str(type(x))).apply(lambda x: x.unique())\n","type_df"],"metadata":{"id":"hWUM71O3evER","executionInfo":{"status":"aborted","timestamp":1693236114103,"user_tz":-60,"elapsed":15,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data.Y.get_dummies()"],"metadata":{"id":"QMou_d9Ue-kv","executionInfo":{"status":"aborted","timestamp":1693236114103,"user_tz":-60,"elapsed":15,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.get_dummies(train_data.Y)"],"metadata":{"id":"-3I3SzeQfC-G","executionInfo":{"status":"aborted","timestamp":1693236114103,"user_tz":-60,"elapsed":14,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_info['hyp']"],"metadata":{"id":"xuzRlyidfwkF","executionInfo":{"status":"aborted","timestamp":1693236114103,"user_tz":-60,"elapsed":14,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args.task"],"metadata":{"id":"qrFXOwXuhpYN","executionInfo":{"status":"aborted","timestamp":1693236114103,"user_tz":-60,"elapsed":14,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args.batch_size"],"metadata":{"id":"lG-K3TvjiXSU","executionInfo":{"status":"aborted","timestamp":1693236114104,"user_tz":-60,"elapsed":15,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim"],"metadata":{"id":"NcV6R_F3lfBx","executionInfo":{"status":"aborted","timestamp":1693236114104,"user_tz":-60,"elapsed":15,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[5][0].shape"],"metadata":{"id":"GqwzFhbYlp7Z","executionInfo":{"status":"aborted","timestamp":1693236114104,"user_tz":-60,"elapsed":14,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9MEhSCelrxCe","executionInfo":{"status":"aborted","timestamp":1693236114104,"user_tz":-60,"elapsed":14,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":null,"outputs":[]}]}