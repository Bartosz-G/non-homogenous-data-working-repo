{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rAPhXEZ2SNKL","executionInfo":{"status":"ok","timestamp":1692960052283,"user_tz":-60,"elapsed":18407,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"f87d174a-64a5-449f-a63a-41ba915e7784"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping TabularExperimentTrackerClient as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n","  Cloning https://github.com/DanielWarfield1/TabularExperimentTrackerClient to /tmp/pip-req-build-g6g4mbl7\n","  Running command git clone --filter=blob:none --quiet https://github.com/DanielWarfield1/TabularExperimentTrackerClient /tmp/pip-req-build-g6g4mbl7\n","  Resolved https://github.com/DanielWarfield1/TabularExperimentTrackerClient to commit df52eac0ce37df983d93a1b76cb9f4380a27b40d\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting openml (from TabularExperimentTrackerClient==0.0.1)\n","  Downloading openml-0.14.1.tar.gz (131 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from TabularExperimentTrackerClient==0.0.1) (2.31.0)\n","Collecting liac-arff>=2.4.0 (from openml->TabularExperimentTrackerClient==0.0.1)\n","  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting xmltodict (from openml->TabularExperimentTrackerClient==0.0.1)\n","  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.2.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.8.2)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.5.3)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.10.1)\n","Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.23.5)\n","Collecting minio (from openml->TabularExperimentTrackerClient==0.0.1)\n","  Downloading minio-7.1.16-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openml->TabularExperimentTrackerClient==0.0.1) (9.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->TabularExperimentTrackerClient==0.0.1) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->TabularExperimentTrackerClient==0.0.1) (2023.7.22)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->openml->TabularExperimentTrackerClient==0.0.1) (2023.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->openml->TabularExperimentTrackerClient==0.0.1) (1.16.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (3.2.0)\n","Building wheels for collected packages: TabularExperimentTrackerClient, openml, liac-arff\n","  Building wheel for TabularExperimentTrackerClient (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for TabularExperimentTrackerClient: filename=TabularExperimentTrackerClient-0.0.1-py3-none-any.whl size=6378 sha256=e1847a06a4b2da70ba7be136731c81e5274a685c0bef1a3a0f5ef6d92664465b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-qldcqqjv/wheels/f9/2e/f3/69345202c956e07475c8f2f55074ad4d97b3e6a976b70fafab\n","  Building wheel for openml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openml: filename=openml-0.14.1-py3-none-any.whl size=146924 sha256=6b6c3b90970d33af1178245834cfa102cdce2b8a99e57feb3620bfc30881128d\n","  Stored in directory: /root/.cache/pip/wheels/75/bc/fd/739778254a2881ef96b139d0aaf60c6d4f9130bb1459b48f10\n","  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11717 sha256=a48c7559b637ec6df573ac4e90cd4081f6c852b0d4d3c6460a94b91e0dc1d785\n","  Stored in directory: /root/.cache/pip/wheels/5d/2a/9c/3895d9617f8f49a0883ba686326d598e78a1c2f54fe3cae86d\n","Successfully built TabularExperimentTrackerClient openml liac-arff\n","Installing collected packages: xmltodict, minio, liac-arff, openml, TabularExperimentTrackerClient\n","Successfully installed TabularExperimentTrackerClient-0.0.1 liac-arff-2.5.0 minio-7.1.16 openml-0.14.1 xmltodict-0.13.0\n"]}],"source":["!pip uninstall TabularExperimentTrackerClient --y\n","!pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient"]},{"cell_type":"code","source":["!pip install numpy\n","!pip install pandas\n","!pip install scikit-learn\n","!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mLuLJlf9StGX","executionInfo":{"status":"ok","timestamp":1692960083485,"user_tz":-60,"elapsed":31217,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"cacccf83-26c7-4f9d-c99e-2f3487695452"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip uninstall NeuralNetworksTrainingPackage --y\n","!pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USjjPIj6rB88","executionInfo":{"status":"ok","timestamp":1692961560271,"user_tz":-60,"elapsed":10426,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"a7f3b716-a37d-4165-c519-88919afcedd1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: NeuralNetworksTrainingPackage 1.0.0\n","Uninstalling NeuralNetworksTrainingPackage-1.0.0:\n","  Successfully uninstalled NeuralNetworksTrainingPackage-1.0.0\n","Collecting git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage\n","  Cloning https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to /tmp/pip-req-build-il_vvzhh\n","  Running command git clone --filter=blob:none --quiet https://github.com/Bartosz-G/NeuralNetworksTrainingPackage /tmp/pip-req-build-il_vvzhh\n","  Resolved https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to commit 91ee0099acb1cb598ae252bdc2e19adcae14014a\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.5.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.2.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from NeuralNetworksTrainingPackage==1.0.0) (2.0.1+cu118)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2023.3)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->NeuralNetworksTrainingPackage==1.0.0) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->NeuralNetworksTrainingPackage==1.0.0) (16.0.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->NeuralNetworksTrainingPackage==1.0.0) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->NeuralNetworksTrainingPackage==1.0.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->NeuralNetworksTrainingPackage==1.0.0) (1.3.0)\n","Building wheels for collected packages: NeuralNetworksTrainingPackage\n","  Building wheel for NeuralNetworksTrainingPackage (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for NeuralNetworksTrainingPackage: filename=NeuralNetworksTrainingPackage-1.0.0-py3-none-any.whl size=4442 sha256=a4a4278a24b486b1a5091e07db78a490906aa1d96b56440fa958d722c0dfadf3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-w30s34hd/wheels/82/01/e8/da81de869be2f6b8c193a6fc5c42ee3ff0b6b97a7b70cd565a\n","Successfully built NeuralNetworksTrainingPackage\n","Installing collected packages: NeuralNetworksTrainingPackage\n","Successfully installed NeuralNetworksTrainingPackage-1.0.0\n"]}]},{"cell_type":"code","source":["from NNTraining import *\n","# Global namespace:\n","# Hyperparams(**run_info.get('hyp'))\n","# CustomDataset(X, Y, relative_indices, tensor_type=torch.float)\n","# CustomDatasetWrapper(train_dataset, relative_indices)\n","# kfold_dataloader_iterator(dataset, n_splits=10, random_state=42, batch_size=16, shuffle_kfold=True, shuffle_dataloader=True)\n","# get_train_test(X, y, categorical_indicator, attribute_names, train_split, seed)"],"metadata":{"id":"2i6wKLMbSm5M","executionInfo":{"status":"ok","timestamp":1692961587848,"user_tz":-60,"elapsed":579,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import sklearn\n","import torch"],"metadata":{"id":"HBbb4I8IzD2Z","executionInfo":{"status":"ok","timestamp":1692961587848,"user_tz":-60,"elapsed":1,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from TabularExperimentTrackerClient.ExperimentClient import ExperimentClient\n","\n","# ==== Setup ====\n","#creating experiment client utilities\n","ex = ExperimentClient(verbose = True)\n","if True:\n","\n","    # BART\n","\n","    #getting openml credentials from drive\n","    ex.define_opml_cred_drive('/My Drive/research/non-homogenous-data/creds/creds-openml.txt')\n","    #getting orchestration credentials from drive\n","    ex.define_orch_cred_drive('bart', '/My Drive//research/non-homogenous-data/creds/creds-colab.txt')\n","\n","else:\n","\n","    # DANIEL\n","\n","    #getting openml credentials from drive\n","    ex.define_opml_cred_drive('/My Drive/Colab Notebooks/Non-Homogeneous Data/openMLAPIKey.txt')\n","    #getting orchestration credentials from drive\n","    ex.define_orch_cred_drive('test1', '/My Drive/Colab Notebooks/Non-Homogeneous Data/tabExpTrackAPIKey.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hloNgGeKSypk","executionInfo":{"status":"ok","timestamp":1692961601237,"user_tz":-60,"elapsed":12859,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"8c5abccd-0562-4ef3-b99e-0f63d62d21d2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Parameters varying\n","depth = {'distribution': 'int_uniform', 'min':1, 'max':11}\n","seed = {'distribution': 'constant', 'value': 42}\n","drop_type = {'distribution': 'categorical', 'values':['node_dropconnect', 'none']}\n","p = {'distribution': 'int_uniform', 'min':0, 'max':1}\n","back_n = {'distribution': 'categorical', 'values':[0, 0, 0, 1]}\n","hidden_dim = {'distribution': 'categorical', 'values':[1, 1, 1, 2]}\n","anneal = {'distribution': 'categorical', 'values':['interpolation', 'none', 'approx']}\n","batch_size = {'distribution': 'categorical', 'values':[16,32,64,64,64,128,256]}\n","epochs = {'distribution': 'categorical', 'values':[30, 60, 90]}\n","lr = {'distribution': 'log_uniform', 'min':0.05, 'max':0.2} # yields mean = 0.1082, median 0.1\n","momentum = {'distribution': 'constant', 'value': 0.9}\n","no_cuda = {'distribution': 'constant', 'value': False}\n","lr_step_size = {'distribution': 'categorical', 'values':[10, 10, 15, 20]}\n","gamma = {'distribution': 'constant', 'value': 0.1}\n","\n","\n","\n","\n","# Regression Spaces\n","LCN_reg_SGD_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'regression'}\n","    }\n","\n","LCN_reg_AMSGrad_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'regression'}\n","    }\n","\n","LLN_reg_SGD_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'regression'}\n","    }\n","\n","\n","LLN_reg_AMSGrad_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'regression'}\n","    }\n","\n","# Classification spaces\n","\n","LCN_cls_SGD_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'classification'}\n","    }\n","\n","LCN_cls_AMSGrad_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'classification'}\n","    }\n","\n","LLN_cls_SGD_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'classification'}\n","    }\n","\n","\n","LLN_cls_AMSGrad_space = {\n","    'depth': depth,\n","    'seed': seed,\n","    'drop_type': drop_type,\n","    'p': p,\n","    'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    'shrinkage': {'distribution': 'constant', 'value': 1},\n","    'back_n': back_n,\n","    'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    'hidden_dim': hidden_dim,\n","    'anneal': anneal,\n","    'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    'batch_size': batch_size,\n","    'epochs': epochs,\n","    'lr': lr,\n","    'momentum': momentum,\n","    'no_cuda': no_cuda,\n","    'lr_step_size': lr_step_size,\n","    'gamma': gamma,\n","    'task': {'distribution': 'constant', 'value': 'classification'}\n","    }\n"],"metadata":{"id":"Nwt_9-JMTTft","executionInfo":{"status":"ok","timestamp":1692961601237,"user_tz":-60,"elapsed":5,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Testing Hyperparameter Spaces"],"metadata":{"id":"MP1WbmGEjSKT"}},{"cell_type":"code","source":["ex.monte_carlo_sample_space({\n","    # 'depth': depth,\n","    # 'seed': seed,\n","    'drop_type': drop_type,\n","    # 'p': p,\n","    # 'ensemble_n': {'distribution': 'constant', 'value': 1},\n","    # 'shrinkage': {'distribution': 'constant', 'value': 1},\n","    # 'back_n': back_n,\n","    # 'net_type': {'distribution': 'constant', 'value': 'locally_linear'},\n","    # 'hidden_dim': hidden_dim,\n","    # 'anneal': anneal,\n","    # 'optimizer': {'distribution': 'constant', 'value': 'AMSGrad'},\n","    # 'batch_size': batch_size,\n","    # 'epochs': epochs,\n","    # 'lr': lr,\n","    # 'momentum': momentum,\n","    # 'no_cuda': no_cuda,\n","    # 'lr_step_size': lr_step_size,\n","    # 'gamma': gamma,\n","    # 'task': {'distribution': 'constant', 'value': 'classification'}\n","    }, 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3BMwdFWjQi8","executionInfo":{"status":"ok","timestamp":1692961601853,"user_tz":-60,"elapsed":619,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"d5ae0fcc-c73c-4cc6-c4a9-68a37c7dcdb3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["sampled 1 points in the space:\n","{'drop_type': {'distribution': 'categorical', 'values': ['node_dropconnect', 'none']}}\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'drop_type': 'node_dropconnect'}]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["model_groups = {\n","    'LCN_reg_SGD':{'model':'LCN_reg_SGD', 'hype':LCN_reg_SGD_space},\n","    'LCN_reg_AMSGrad':{'model':'LCN_reg_AMSGrad', 'hype':LCN_reg_AMSGrad_space},\n","    'LLN_reg_SGD':{'model':'LLN_reg_SGD', 'hype':LLN_reg_SGD_space},\n","    'LLN_reg_AMSGrad':{'model':'LLN_reg_AMSGrad', 'hype':LLN_reg_AMSGrad_space},\n","    'LCN_cls_SGD':{'model':'LCN_cls_SGD', 'hype':LCN_cls_SGD_space},\n","    'LCN_cls_AMSGrad':{'model':'LCN_cls_AMSGrad', 'hype':LCN_cls_AMSGrad_space},\n","    'LLN_cls_SGD':{'model':'LLN_cls_SGD', 'hype':LLN_cls_SGD_space},\n","    'LLN_cls_AMSGrad':{'model':'LLN_cls_AMSGrad', 'hype':LLN_cls_AMSGrad_space},\n","}\n","\n","ex.def_model_groups(model_groups)"],"metadata":{"id":"cyzQSiOtzLBa","executionInfo":{"status":"ok","timestamp":1692961601853,"user_tz":-60,"elapsed":16,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["ex.def_data_groups_opml()\n","print('automatically defined data groups:')\n","print(ex.data_groups.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_hWhsfs9Lem","executionInfo":{"status":"ok","timestamp":1692961601853,"user_tz":-60,"elapsed":14,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"dd9c2c8e-1a70-40f1-e19f-1ef276825ba8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["automatically defined data groups:\n","dict_keys(['opml_reg_purnum_group', 'opml_class_purnum_group', 'opml_reg_numcat_group', 'opml_class_numcat_group'])\n"]}]},{"cell_type":"code","source":["classification_models = [k for k in model_groups.keys() if '_cls' in k]\n","regression_models = [k for k in model_groups.keys() if '_reg' in k]\n","\n","\n","applications = {'opml_reg_purnum_group': regression_models,\n","                'opml_reg_numcat_group': regression_models,\n","                'opml_class_purnum_group': classification_models,\n","                'opml_class_numcat_group': classification_models}\n","\n","ex.def_applications(applications)\n","ex.reg_experiment('Testing_LCN_3')\n","# ex.reg_experiment('20230822')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"9_3lujFq9gxJ","executionInfo":{"status":"ok","timestamp":1692961602491,"user_tz":-60,"elapsed":640,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"e88283b9-7385-48b6-e05b-157566e9b985"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["existing experiment found\n"]},{"output_type":"execute_result","data":{"text/plain":["'existing experiment found'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["run_info = ex.begin_run_sticky()\n","run_info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-j28taEW_XxU","executionInfo":{"status":"ok","timestamp":1692961603503,"user_tz":-60,"elapsed":1016,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"8381e214-6a6f-4083-ab76-90016011fd6c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["{'_id': '64e88b4229625b726f52d0fa', 'metrics_per_epoch': [], 'experiment_id': '64e5e073c1b33602b5f95fa9', 'experiment_name': 'Testing_LCN_3', 'mtpair_index': 103, 'mtpair_model': 'LLN_reg_AMSGrad', 'mtpair_task': '335-361101', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 10, 'seed': 42, 'drop_type': 'node_dropconnect', 'p': 0, 'ensemble_n': 1, 'shrinkage': 1, 'back_n': 0, 'net_type': 'locally_linear', 'hidden_dim': 1, 'anneal': 'approx', 'optimizer': 'AMSGrad', 'batch_size': 256, 'epochs': 60, 'lr': 0.0794751648916623, 'momentum': 0.9, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1, 'task': 'regression'}, 'model': 'LLN_reg_AMSGrad', 'task': '335-361101'}\n","4e88b4229625b726f52d0f\n"]},{"output_type":"execute_result","data":{"text/plain":["{'_id': '64e88b4229625b726f52d0fa',\n"," 'metrics_per_epoch': [],\n"," 'experiment_id': '64e5e073c1b33602b5f95fa9',\n"," 'experiment_name': 'Testing_LCN_3',\n"," 'mtpair_index': 103,\n"," 'mtpair_model': 'LLN_reg_AMSGrad',\n"," 'mtpair_task': '335-361101',\n"," 'is_completed': False,\n"," 'user_id': '64d3a7457658d6ec6db139d0',\n"," 'user_name': 'bart',\n"," 'hyp': {'depth': 10,\n","  'seed': 42,\n","  'drop_type': 'node_dropconnect',\n","  'p': 0,\n","  'ensemble_n': 1,\n","  'shrinkage': 1,\n","  'back_n': 0,\n","  'net_type': 'locally_linear',\n","  'hidden_dim': 1,\n","  'anneal': 'approx',\n","  'optimizer': 'AMSGrad',\n","  'batch_size': 256,\n","  'epochs': 60,\n","  'lr': 0.0794751648916623,\n","  'momentum': 0.9,\n","  'no_cuda': False,\n","  'lr_step_size': 10,\n","  'gamma': 0.1,\n","  'task': 'regression'},\n"," 'model': 'LLN_reg_AMSGrad',\n"," 'task': '335-361101'}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["run_info.get('hyp')"],"metadata":{"id":"eHPE_iIMAr2_","executionInfo":{"status":"ok","timestamp":1692961603503,"user_tz":-60,"elapsed":35,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"26d4ffa5-2e0e-4bf4-8d5e-d433531e50b8"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'depth': 10,\n"," 'seed': 42,\n"," 'drop_type': 'node_dropconnect',\n"," 'p': 0,\n"," 'ensemble_n': 1,\n"," 'shrinkage': 1,\n"," 'back_n': 0,\n"," 'net_type': 'locally_linear',\n"," 'hidden_dim': 1,\n"," 'anneal': 'approx',\n"," 'optimizer': 'AMSGrad',\n"," 'batch_size': 256,\n"," 'epochs': 60,\n"," 'lr': 0.0794751648916623,\n"," 'momentum': 0.9,\n"," 'no_cuda': False,\n"," 'lr_step_size': 10,\n"," 'gamma': 0.1,\n"," 'task': 'regression'}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["ex.opml_load_task(run_info['mtpair_task'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jEa1jmiXaS6","executionInfo":{"status":"ok","timestamp":1692961710414,"user_tz":-60,"elapsed":106936,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"efe41a4b-f3b5-4c9e-fa1c-4ae0583bc95d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["downloading task 335-361101\n","task different than previous task, downloading...\n"]},{"output_type":"execute_result","data":{"text/plain":["(       VendorID store_and_fwd_flag RatecodeID  passenger_count extra mta_tax  \\\n"," 0             1                  0          0                1     2       2   \n"," 1             1                  0          0                2     2       2   \n"," 2             1                  0          0                1     2       2   \n"," 3             1                  0          0                1     2       2   \n"," 4             1                  0          0                1     2       2   \n"," ...         ...                ...        ...              ...   ...     ...   \n"," 581830        0                  0          0                2     2       2   \n"," 581831        0                  0          0                1     2       2   \n"," 581832        0                  0          0                1     2       2   \n"," 581833        0                  0          0                1     2       2   \n"," 581834        0                  0          0                1     2       2   \n"," \n","         tolls_amount improvement_surcharge  total_amount trip_type  \\\n"," 0                0.0                     2          6.36         0   \n"," 1                0.0                     2          8.50         0   \n"," 2                0.0                     2          7.54         0   \n"," 3                0.0                     2         12.96         0   \n"," 4                0.0                     2         12.30         0   \n"," ...              ...                   ...           ...       ...   \n"," 581830           0.0                     2          7.55         0   \n"," 581831           0.0                     2          8.38         0   \n"," 581832           0.0                     2          7.30         0   \n"," 581833           0.0                     2          7.55         0   \n"," 581834           0.0                     2         23.75         0   \n"," \n","         lpep_pickup_datetime_day  lpep_pickup_datetime_hour  \\\n"," 0                              1                          0   \n"," 1                              1                          0   \n"," 2                              1                          0   \n"," 3                              1                          0   \n"," 4                              1                          0   \n"," ...                          ...                        ...   \n"," 581830                        31                         23   \n"," 581831                        31                         23   \n"," 581832                        31                         23   \n"," 581833                        31                         23   \n"," 581834                        31                         23   \n"," \n","         lpep_pickup_datetime_minute  lpep_dropoff_datetime_day  \\\n"," 0                                52                          1   \n"," 1                                10                          1   \n"," 2                                12                          1   \n"," 3                                29                          1   \n"," 4                                42                          1   \n"," ...                             ...                        ...   \n"," 581830                            1                         31   \n"," 581831                            1                         31   \n"," 581832                            0                         31   \n"," 581833                            0                         31   \n"," 581834                            0                         31   \n"," \n","         lpep_dropoff_datetime_hour  lpep_dropoff_datetime_minute  \n"," 0                                0                            54  \n"," 1                                0                            14  \n"," 2                                0                            15  \n"," 3                                0                            39  \n"," 4                                0                            52  \n"," ...                            ...                           ...  \n"," 581830                          23                             5  \n"," 581831                          23                             9  \n"," 581832                          23                             4  \n"," 581833                          23                             5  \n"," 581834                          23                            15  \n"," \n"," [581835 rows x 16 columns],\n"," 0         0.722706\n"," 1         0.993252\n"," 2         1.007958\n"," 3         1.150572\n"," 4         0.000000\n","             ...   \n"," 581830    0.810930\n"," 581831    0.076961\n"," 581832    0.000000\n"," 581833    0.810930\n"," 581834    1.599388\n"," Name: tip_amount, Length: 581835, dtype: float64,\n"," [True,\n","  True,\n","  True,\n","  False,\n","  True,\n","  True,\n","  False,\n","  True,\n","  False,\n","  True,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False,\n","  False],\n"," ['VendorID',\n","  'store_and_fwd_flag',\n","  'RatecodeID',\n","  'passenger_count',\n","  'extra',\n","  'mta_tax',\n","  'tolls_amount',\n","  'improvement_surcharge',\n","  'total_amount',\n","  'trip_type',\n","  'lpep_pickup_datetime_day',\n","  'lpep_pickup_datetime_hour',\n","  'lpep_pickup_datetime_minute',\n","  'lpep_dropoff_datetime_day',\n","  'lpep_dropoff_datetime_hour',\n","  'lpep_dropoff_datetime_minute'])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["from NNTraining import *\n","args = Hyperparams(**run_info.get('hyp'))"],"metadata":{"id":"t7QejAbP-tGO","executionInfo":{"status":"ok","timestamp":1692961710414,"user_tz":-60,"elapsed":9,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# 1. LCN Model"],"metadata":{"id":"c6fWISMKznlN"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","def my_softplus(x, tau=1., threshold=20.):\n","    truncate_mask = (x > threshold).type(torch.cuda.FloatTensor)\n","    return truncate_mask * x + (1. - truncate_mask) * (tau * torch.log(1 + torch.exp((1. - truncate_mask) * x / tau)))\n","\n","def my_softplus_derivative(x, tau=1., threshold=20.):\n","    truncate_mask = (x > threshold).type(torch.cuda.FloatTensor)\n","    return truncate_mask + (1. - truncate_mask) / (1 + torch.exp(- (1. - truncate_mask) * x / tau) )\n","\n","class Net(nn.Module):\n","    def __init__(self,\n","            input_dim,\n","            output_dim,\n","            hidden_dim,\n","            num_layer,\n","            num_back_layer,\n","            dense = False,\n","            drop_type = 'none',\n","            net_type = 'locally_constant',\n","            approx = 'none'):\n","        super(Net, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layer = num_layer\n","        self.num_back_layer = num_back_layer\n","        self.dense = dense\n","        self.drop_type = drop_type\n","        self.net_type = net_type\n","        self.n_neuron = self.hidden_dim * self.num_layer\n","        self.approx = approx\n","\n","        self.layer = nn.ModuleList()\n","\n","        self.weights = dict()\n","        self.biases = dict()\n","        self.output_constants = dict()\n","        self.output_weights = dict()\n","        self.output_biases = dict()\n","\n","        accu_dim = input_dim\n","        self.weight_masks = []\n","        for i in range(self.num_layer):\n","            self.layer.append(nn.Linear(accu_dim, hidden_dim))\n","            ite_mask = np.zeros((1, hidden_dim, accu_dim))\n","            pick = np.random.choice(accu_dim, int(np.sqrt(accu_dim)), replace=False)\n","            ite_mask[0, 0, pick] = 1\n","            assert(hidden_dim == 1)\n","            ite_mask = torch.tensor(ite_mask.astype(np.float32)).cuda()\n","            self.weight_masks.append(ite_mask)\n","\n","            if self.dense:\n","                accu_dim += hidden_dim\n","            else:\n","                accu_dim = hidden_dim\n","\n","        if self.net_type == 'locally_constant':\n","            self.backward_layer = nn.ModuleList()\n","            backward_dim = 256\n","            cur_dim = self.num_layer * self.hidden_dim * (1 + self.input_dim)\n","            for i in range(self.num_back_layer):\n","                self.backward_layer.append(nn.Linear(cur_dim, backward_dim))\n","                cur_dim = backward_dim\n","            self.backward_layer.append(nn.Linear(cur_dim, output_dim))\n","\n","        elif self.net_type == 'locally_linear':\n","            self.output_fc = nn.Linear(accu_dim, self.output_dim)\n","        else:\n","            print('net_type', self.net_type, 'is not supported')\n","            exit(0)\n","\n","    def normal_forward(self, init_layer, p=0, training=True):\n","        assert(self.net_type == 'locally_linear')\n","        relu_masks = []\n","        if len(init_layer.shape) == 4:\n","            bz = init_layer.shape[0]\n","            init_layer = init_layer.view(bz, -1)\n","        cur_embed = init_layer\n","        batch_size = init_layer.shape[0]\n","\n","        for i in range(self.num_layer):\n","            if training == False:\n","                next_embed = self.layer[i](cur_embed)\n","            else:\n","                w = self.layer[i].weight\n","                w = w.view(1, w.shape[0], w.shape[1]).expand(batch_size, -1, -1)\n","\n","                b = self.layer[i].bias\n","                b = b.view(1, b.shape[0]).expand(batch_size, -1)\n","\n","                next_embed = torch.bmm(F.dropout(w, p=p, training=training), cur_embed.unsqueeze(-1)) + b.unsqueeze(-1)\n","                next_embed = next_embed.squeeze(-1)\n","\n","            relu_masks.append( (next_embed > 0) )\n","            next_embed = F.relu(next_embed)\n","            if self.dense:\n","                cur_embed = torch.cat((cur_embed, next_embed), 1)\n","            else:\n","                cur_embed = next_embed\n","        return self.output_fc(cur_embed), relu_masks\n","\n","    def forward(self, init_layer, p=0, training=True, alpha=None, anneal='none'):\n","        assert(self.net_type == 'locally_constant')\n","        relu_masks = []\n","        if len(init_layer.shape) == 4:\n","            bz = init_layer.shape[0]\n","            init_layer = init_layer.view(bz, -1)\n","        # cur_embed is used for forward computation.\n","        cur_embed = init_layer\n","        # x is used to compute Jacobian using dynamic programming.\n","        x = init_layer.unsqueeze(-1)\n","\n","        batch_size = x.shape[0]\n","        patterns = []\n","        for i in range(self.num_layer):\n","            if self.drop_type != 'node_dropconnect':\n","                next_embed = self.layer[i](cur_embed)\n","            w = self.layer[i].weight\n","            w = w.view(1, w.shape[0], w.shape[1]).expand(batch_size, -1, -1)\n","\n","            if self.drop_type == 'node_dropconnect':\n","                b = self.layer[i].bias\n","                b = b.view(1, b.shape[0]).expand(batch_size, -1)\n","\n","                next_embed = torch.bmm(F.dropout(w, p=p, training=training), cur_embed.unsqueeze(-1)) + b.unsqueeze(-1)\n","                next_embed = next_embed.squeeze(-1)\n","            else:\n","                pass\n","\n","            relu_masks.append( (next_embed > 0) )\n","            if self.approx == 'approx':\n","                neur_deriv = my_softplus_derivative(next_embed)\n","                next_embed = my_softplus(next_embed)\n","            elif anneal == 'interpolation':\n","                neur_deriv = alpha * (next_embed > 0).type(torch.cuda.FloatTensor) + (1 - alpha) * my_softplus_derivative(next_embed)\n","                next_embed = alpha * F.relu(next_embed) + (1 - alpha) * my_softplus(next_embed)\n","            elif anneal == 'none':\n","                neur_deriv = (next_embed > 0).type(torch.cuda.FloatTensor)\n","                next_embed = F.relu(next_embed)\n","\n","            patterns.append(neur_deriv)\n","            neur_deriv = neur_deriv.unsqueeze(-1)\n","\n","            if i == 0:\n","                jacobians = neur_deriv * w\n","                offsets = next_embed - torch.bmm(jacobians, x).squeeze(-1)\n","            else:\n","                if self.dense:\n","                    ite_jacobians = w[:, :, :self.input_dim] + torch.bmm(w[:, :, self.input_dim:], jacobians)\n","                else:\n","                    ite_jacobians = torch.bmm(w, jacobians[:, -self.hidden_dim:, :])\n","\n","                ite_jacobians = neur_deriv * ite_jacobians\n","                ite_offsets = next_embed - torch.bmm(ite_jacobians, x).squeeze(-1)\n","\n","                jacobians = torch.cat([jacobians, ite_jacobians], dim=1)\n","                offsets = torch.cat([offsets, ite_offsets], dim=1)\n","\n","            if self.dense:\n","                cur_embed = torch.cat((cur_embed, next_embed), 1)\n","            else:\n","                cur_embed = next_embed\n","\n","        leaf_input = torch.cat([jacobians, offsets.unsqueeze(-1)], dim=2).view(batch_size, -1)\n","        for i in range(self.num_back_layer):\n","            leaf_input = F.relu(self.backward_layer[i](leaf_input))\n","        leaf_output = self.backward_layer[-1](leaf_input)\n","\n","        return leaf_output, relu_masks"],"metadata":{"id":"-5AmSj4lzmpp","executionInfo":{"status":"ok","timestamp":1692961710415,"user_tz":-60,"elapsed":7,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# 2. LCN utils"],"metadata":{"id":"eLYG6KXkYs1m"}},{"cell_type":"code","source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def print_args(args):\n","    print(\"\\nParameters:\")\n","    for attr, value in sorted(args.__dict__.items()):\n","        print(\"\\t{}={}\".format(attr.upper(), value))"],"metadata":{"id":"S4MIlQ2tX9Xg","executionInfo":{"status":"ok","timestamp":1692961710415,"user_tz":-60,"elapsed":6,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# 3. LCN Test"],"metadata":{"id":"VlSqjhUHYzn6"}},{"cell_type":"code","source":["def train(args, model, device, train_loader, optimizer, epoch, anneal, alpha=1):\n","    model.train()\n","    dataset_len = 0\n","    avg_loss = AverageMeter()\n","\n","    for (data, target) in train_loader:\n","        dataset_len += len(target)\n","        data, target = data.to(device), target.to(device)\n","        if args.task == 'classification':\n","            target = target.type(torch.cuda.LongTensor)\n","\n","        optimizer.zero_grad()\n","        ###############\n","        data.requires_grad = True\n","        if model.net_type == 'locally_constant':\n","            if args.p != -1:\n","                assert(args.p >= 0. and args.p < 1)\n","                output, regularization = model(data, alpha=alpha, anneal=anneal, p=args.p, training=True)\n","            else:\n","                output, regularization = model(data, alpha=alpha, anneal=anneal, p=1-alpha, training=True)\n","\n","        elif model.net_type == 'locally_linear':\n","            output, regularization = model.normal_forward(data)\n","        ###############\n","\n","        optimizer.zero_grad()\n","        if args.task == 'classification':\n","            loss = F.cross_entropy(output, target)\n","        elif args.task == 'regression':\n","            output = output.squeeze(-1)\n","            loss = ((output - target) ** 2).mean()\n","\n","        loss.backward()\n","        optimizer.step()\n","        avg_loss.update(loss.item())\n","\n","    return avg_loss.avg\n","\n","def test(args, model, device, test_loader, test_set_name):\n","    with torch.no_grad():\n","        model.eval()\n","        test_loss = 0\n","        correct = 0\n","\n","        score = []\n","        label = []\n","        dataset_len = 0\n","\n","        pattern_to_pred = dict()\n","        tree_x = []\n","        tree_pattern = []\n","\n","        for data, target in test_loader:\n","            dataset_len += len(target)\n","            label += list(target)\n","            data, target = data.to(device), target.to(device)\n","            if args.task == 'classification':\n","                target = target.type(torch.cuda.LongTensor)\n","\n","            ###############\n","            data.requires_grad = True\n","            if model.net_type == 'locally_constant':\n","                output, relu_masks = model(data, p=0, training=False)\n","            elif model.net_type == 'locally_linear':\n","                output, relu_masks = model.normal_forward(data, p=0, training=False)\n","            ###############\n","\n","            if args.task == 'classification':\n","                test_loss += F.cross_entropy(output, target, reduction='sum').item()\n","                output = torch.softmax(output, dim=-1)\n","                score += list(output[:, 1].cpu().data.numpy())\n","                pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n","                correct += pred.eq(target.view_as(pred)).sum().item()\n","                output = output[:, 1]\n","            elif args.task == 'regression':\n","                output = output.squeeze(-1)\n","                test_loss += ((output - target) ** 2).mean().item() * len(target)\n","\n","        test_loss /= dataset_len\n","        if args.task == 'classification':\n","            if args.output_dim == 2:\n","                AUC = roc_auc_score(label, score)\n","                test_score = AUC\n","            else:\n","                AUC = -1\n","                test_score = correct / dataset_len\n","\n","        elif args.task == 'regression':\n","            RMSE = np.sqrt(test_loss)\n","            test_score = -RMSE\n","\n","        return test_loss, test_score\n"],"metadata":{"id":"mL-qoJWeZF_f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. LCN Main"],"metadata":{"id":"aBgpmCRiZsGz"}},{"cell_type":"code","source":["def get_alpha(epoch, total_epoch):\n","    return float(epoch) / float(total_epoch)\n","\n","def main():\n","    # from some github repo...\n","    torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","    args = get_args()\n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","    torch.manual_seed(args.seed)\n","    np.random.seed(args.seed)\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    train_loader, valid_loader, test_loader = get_data_loaders(args.dataset, args.batch_size, sub_task=args.sub_task, dim=args.input_dim)\n","\n","    if args.dataset in ['sider_split/', 'tox21_split/']:\n","        args.dataset = args.dataset[:-1] + '-' + str(args.sub_task)\n","\n","    print('batch number: train={}, valid={}, test={}'.format(len(train_loader), len(valid_loader), len(test_loader)))\n","\n","    model = Net(input_dim=args.input_dim, output_dim=args.output_dim, hidden_dim=args.hidden_dim, num_layer=args.depth, num_back_layer=args.back_n, dense=True, drop_type=args.drop_type, net_type=args.net_type, approx=args.anneal).to(device)\n","\n","    if args.optimizer == 'SGD':\n","        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n","    elif args.optimizer == 'AMSGrad':\n","        optimizer = optim.Adam(model.parameters(), lr=args.lr, amsgrad=True)\n","    scheduler = StepLR(optimizer, step_size=args.lr_step_size, gamma=args.gamma)\n","\n","    best_score = -1e30\n","    start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n","    if args.anneal == 'approx':\n","        args.net_type = 'approx_' + args.net_type\n","\n","\n","    best_model_name = './checkpoint/{}/{}/best_seed{}_depth{}_ckpt.t7'.format(args.dataset.strip('/'), args.net_type, args.seed, args.depth)\n","    last_model_name = './checkpoint/{}/{}/last_seed{}_depth{}_ckpt.t7'.format(args.dataset.strip('/'), args.net_type, args.seed, args.depth)\n","\n","    best_log_file = 'log/' + args.dataset.strip('/') + '/{}/depth{}_backn{}_drop{}_p{}_best.log'.format(args.net_type, args.depth, args.back_n, args.drop_type, args.p)\n","    last_log_file = 'log/' + args.dataset.strip('/') + '/{}/depth{}_backn{}_drop{}_p{}_last.log'.format(args.net_type, args.depth, args.back_n, args.drop_type, args.p)\n","\n","    model_dir = './checkpoint/{}/{}/'.format(args.dataset.strip('/'), args.net_type)\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    log_dir = 'log/' + args.dataset.strip('/') + '/{}/'.format(args.net_type)\n","    if not os.path.exists(log_dir):\n","        os.makedirs(log_dir)\n","\n","    if not os.path.isdir('checkpoint'):\n","        os.mkdir('checkpoint')\n","\n","    for epoch in range(start_epoch, args.epochs + start_epoch):\n","        scheduler.step(epoch)\n","\n","        alpha = get_alpha(epoch, args.epochs)\n","        train_approximate_loss = train(args, model, device, train_loader, optimizer, epoch, args.anneal, alpha)\n","\n","        # used for plotting learning curves\n","        train_loss, train_score = test(args, model, device, train_loader, 'train')\n","        valid_loss, valid_score = test(args, model, device, valid_loader, 'valid')\n","        test_loss, test_score = test(args, model, device, test_loader, 'test')\n","\n","        # early stopping version\n","        if valid_score > best_score:\n","            state = {'model': model.state_dict()}\n","            torch.save(state, best_model_name)\n","            best_score = valid_score\n","\n","        # \"convergent\" version\n","        state = {'model': model.state_dict()}\n","        torch.save(state, last_model_name)\n","\n","    print('Training finished. Loading models from validation...')\n","    for model_name, log_file, setting in zip([best_model_name, last_model_name], [best_log_file, last_log_file], ['best', 'last']):\n","        print('\\nLoading the {} model...'.format(setting))\n","\n","        checkpoint = torch.load(model_name)\n","        model.load_state_dict(checkpoint['model'])\n","        train_loss, train_score = test(args, model, device, train_loader, 'train')\n","        valid_loss, valid_score = test(args, model, device, valid_loader, 'valid')\n","        test_loss, test_score = test(args, model, device, test_loader, 'test ')\n","\n","        with open(log_file, 'a') as fp:\n","            if args.task == 'classification':\n","                log_str = '{}\\t{:.4f}\\t{:.4f}\\t{:.4f}'.format(args.seed, train_score, valid_score, test_score)\n","            elif args.task == 'regression':\n","                log_str = '{}\\t{:.4f}\\t{:.4f}\\t{:.4f}'.format(args.seed, -train_score, -valid_score, -test_score)\n","            fp.write(log_str+'\\n')\n"],"metadata":{"id":"Ck1e9hNGZtj7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. LCN custom training loop"],"metadata":{"id":"XezqrpZxaKdn"}},{"cell_type":"code","source":["from NNTraining import *\n","\n","print('==== Begin run:====')\n","print('---- Initialising parameters for the run ----')\n","run_info = ex.begin_run_sticky()\n","\n","use_cuda = not args.no_cuda and torch.cuda.is_available()\n","torch.manual_seed(args.seed)\n","np.random.seed(args.seed)\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","args = Hyperparams(**run_info.get('hyp')) # hyperparameters for LCN need to be in form of an object\n","\n","print('---- Loading datasets ----')\n","X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n","train_data, test_data, input_dim, output_dim = get_train_test(X, y, categorical_indicator, attribute_names, 0.75, args.seed)\n","\n","\n","DataLoadersIter = kfold_dataloader_iterator(train_data, n_splits= 5,\n","                                            random_state= args.seed,\n","                                            batch_size= args.batch_size,\n","                                            shuffle_kfold= True,\n","                                            shuffle_dataloader= True)\n","\n","test_dataloader = torch.utils.data.DataLoader(test_data,\n","                                              batch_size=len(test_data),\n","                                              shuffle= True)\n","\n","for kfold, train_dataloader, val_dataloader in enumerate(DataLoadersIter):\n","  print(f\"---- {kfold}'th kfold starting ----\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"altcS6fzaJ7T","executionInfo":{"status":"ok","timestamp":1692963161116,"user_tz":-60,"elapsed":1198,"user":{"displayName":"bartosz azaniux","userId":"12656151146140381527"}},"outputId":"241ff2a8-6fc2-41cf-8962-4688ca188f2b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["==== Begin run:====\n","{'_id': '64e89158a11e5ad78a3acca1', 'metrics_per_epoch': [], 'experiment_id': '64e5e073c1b33602b5f95fa9', 'experiment_name': 'Testing_LCN_3', 'mtpair_index': 102, 'mtpair_model': 'LLN_reg_SGD', 'mtpair_task': '335-361101', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 9, 'seed': 42, 'drop_type': 'none', 'p': 0, 'ensemble_n': 1, 'shrinkage': 1, 'back_n': 0, 'net_type': 'locally_linear', 'hidden_dim': 1, 'anneal': 'interpolation', 'optimizer': 'SGD', 'batch_size': 16, 'epochs': 30, 'lr': 0.06111053599578259, 'momentum': 0.9, 'no_cuda': False, 'lr_step_size': 20, 'gamma': 0.1, 'task': 'regression'}, 'model': 'LLN_reg_SGD', 'task': '335-361101'}\n","4e89158a11e5ad78a3acca\n","downloading task 335-361101\n","using values from previous task load, skipped download\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VF4N3t4Gd9sH"},"execution_count":null,"outputs":[]}]}