{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Template Experiment notebook\n",
    "This notebook walks you through how to read, add and modify experiments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table of contents\n",
    "\n",
    "TODO: Expand"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Set-up and necessary requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: Explain the set-up and requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip uninstall NeuralNetworksTrainingPackage --y\n",
    "#%pip uninstall TabularExperimentTrackerClient --y\n",
    "!pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
    "!pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ex = ExperimentOrchstratorClient()\n",
    "ex.get_credentials"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Defining the experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Defining hyperparameter space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example we'll create 2 models:\n",
    "- One sklearn model\n",
    "- One Pytorch model\n",
    "\n",
    "We begin by creating the possible space of hyperparameters and parameters which will later be used to build our models. You can subdivide your models by tasks or run one model for multiple tasks, but be sure to correctly assign model-task pairs in section 1.3."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "our_sklearn_model_space = {'param_1': {\"distribution\": \"int_uniform\", \"min\":1, \"max\":11},\n",
    "                           'param_2': {\"distribution\": \"float_uniform\", \"min\":0.5, \"max\":1}}\n",
    "\n",
    "our_pytorch_regression_model_space = {'param_1': {'distribution': 'constant', 'value': 0.1},\n",
    "                                      'param_2': {'distribution': 'categorical', 'values':[10, 10, 15, 20]}}\n",
    "\n",
    "our_pytorch_classification_model_space = {'param_1': {'distribution': 'constant', 'value': 0.1},\n",
    "                                         'param_2': {'distribution': 'categorical', 'values':[10, 10, 15, 20]}}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Possible distributions:\n",
    "- TODO: list all distributions\n",
    "\n",
    "Remember that your further code should be able to re-create the models and training based on only the hyperparemeter space, so we also recommend including parameters like 'seed', 'cuda' and 'task'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "XGBoost_space = {\n",
    "    \"max_depth\": {\"distribution\": \"int_uniform\", \"min\":1, \"max\":11},\n",
    "    \"n_estimators\": {\"distribution\": \"int_uniform\", \"min\":100, \"max\":200},\n",
    "    \"min_child_weight\": {\"distribution\": \"log_uniform\", \"min\":1, \"max\":1e2},\n",
    "    \"subsample\": {\"distribution\": \"float_uniform\", \"min\":0.5, \"max\":1},\n",
    "    \"colsample_bylevel\": {\"distribution\": \"float_uniform\", \"min\":0.5, \"max\":1},\n",
    "    \"colsample_bytree\": {\"distribution\": \"float_uniform\", \"min\":0.5, \"max\":1},\n",
    "    \"gamma\": {\"distribution\": \"log_uniform\", \"min\":1e-8, \"max\":7},\n",
    "    \"reg_lambda\": {\"distribution\": \"log_uniform\", \"min\":1, \"max\":4},\n",
    "    \"reg_alpha\": {\"distribution\": \"log_uniform\", \"min\":1e-8, \"max\":1e2},\n",
    "}\n",
    "\n",
    "LCN_reg_SGD_space = {\n",
    "    'depth': {'distribution': 'int_uniform', 'min':1, 'max':11},\n",
    "    'seed': {'distribution': 'constant', 'value': 42},\n",
    "    'drop_type': {'distribution': 'categorical', 'values':['node_dropconnect', 'none']},\n",
    "    'p': {'distribution': 'float_uniform', 'min':0.25, 'max':0.75},\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': {'distribution': 'categorical', 'values':[0, 0, 0, 1]},\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n",
    "    'hidden_dim': {'distribution': 'constant', 'value': 1},\n",
    "    'anneal': {'distribution': 'categorical', 'values':['interpolation', 'none', 'approx']},\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n",
    "    'batch_size': {'distribution': 'categorical', 'values':[16,32,64,64,64,128,256]},\n",
    "    'epochs': {'distribution': 'constant', 'value': 30},\n",
    "    'lr': {'distribution': 'log_uniform', 'min':0.05, 'max':0.2},\n",
    "    'momentum': {'distribution': 'constant', 'value': 0.9},\n",
    "    'no_cuda': {'distribution': 'constant', 'value': False},\n",
    "    'lr_step_size': {'distribution': 'categorical', 'values':[10, 10, 15, 20]},\n",
    "    'gamma': {'distribution': 'constant', 'value': 0.1},\n",
    "    'task': {'distribution': 'constant', 'value': 'regression'}\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Assigning distributions to their models:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_groups = {\n",
    "    'our_sklearn_model':{'model':'our_sklearn_model', 'hype':our_sklearn_model_space},\n",
    "    'our_pytorch_regression_model':{'model':'our_pytorch_regression_model', 'hype':our_pytorch_regression_model_space},\n",
    "    'our_pytorch_classification_model':{'model':'our_pytorch_classification_model', 'hype':our_pytorch_regression_model_space},\n",
    "}\n",
    "\n",
    "ex.def_model_groups(model_groups)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.3 Assign experiments to appropriate tasks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO Explain:\n",
    "- opml_reg_purnum_group:\n",
    "- opml_reg_numcat_group:\n",
    "- opml_class_purnum_group:\n",
    "- opml_class_numcat_group:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regression_models = {'our_sklearn_model':{'model':'our_sklearn_model', 'hype':our_sklearn_model_space},\n",
    "                     'our_pytorch_regression_model':{'model':'our_pytorch_regression_model', 'hype':our_pytorch_regression_model_space}}\n",
    "\n",
    "classification_models = {\n",
    "    'our_sklearn_model':{'model':'our_sklearn_model', 'hype':our_sklearn_model_space},\n",
    "    'our_pytorch_classification_model':{'model':'our_pytorch_classification_model', 'hype':our_pytorch_regression_model_space}}\n",
    "\n",
    "\n",
    "applications = {'opml_reg_purnum_group': regression_models,\n",
    "                'opml_reg_numcat_group': regression_models,\n",
    "                'opml_class_purnum_group': classification_models,\n",
    "                'opml_class_numcat_group': classification_models}\n",
    "\n",
    "ex.def_applications(applications)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Registering an Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = 'template_experiment'\n",
    "ex.reg_experiment(experiment_name)\n",
    "exp_info = ex.experiment_info()\n",
    "successful_runs = exp_info['successful_runs']\n",
    "required_runs = exp_info['required_runs']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Task pre-processing steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our experiments data pre-processing steps are applied right after downloading datasets in the main loop of the experiment. Object of class `dataPreProcessingEventEmitter()` stores all of the pre-processing steps in queues and is responsible for executing them in the right order and on the right partitions of data (such as on `train`). In our code we separate queues into task-specific queues (under namespace such as `regression`) and model-specific queues (under namespace such as `'our_pytorch_classification_model'`), although you're not limited to only those and can introduce your own logic.\n",
    "\n",
    "You can also view this process as event-driven, where the `dataPreProcessingEventEmitter()` object listens for which model and task is coming in at every loop and responds by applying the correct transformations to the data.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Defining task specific pre-processing steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we begin by initialising task pre-processing steps stored in `NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing` module. Task pre-processing steps come in form of classes. When initialised those objects store parameters of how data pre-processing steps should be applied."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import filterCardinality, quantileTransform, trunctuateData, oneHotEncodePredictors, oneHotEncodeTargets, toDataFrame, splitTrainValTest\n",
    "\n",
    "n_sample = 20000\n",
    "split = [0.5, 0.25, 0.25]\n",
    "quantile_transform_distribution='uniform'\n",
    "\n",
    "filter_cardinality = filterCardinality(transform = 'all')\n",
    "trunctuate_data = trunctuateData(n = n_sample, transform = 'all')\n",
    "one_hot_encode_predictors = oneHotEncodePredictors(transform = 'all')\n",
    "one_hot_encode_targets = oneHotEncodeTargets(transform = 'all')\n",
    "to_data_frame = toDataFrame(transform = 'all')\n",
    "split_train_val_test = splitTrainValTest(split = split) # Special transformation\n",
    "quantile_transform = quantileTransform(output_distribution = quantile_transform_distribution, transform = 'train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-processing steps can be divided into two, ordinary and special.\n",
    "\n",
    "Ordinary steps:\n",
    "- `trunctuateData(n, seed = None, transform = 'all')`\n",
    "- `filterCardinality(transform = 'all')`\n",
    "- `quantileTransform(n_quantiles=1000, output_distribution='uniform',\n",
    "                 ignore_implicit_zeros=False,\n",
    "                 subsample=10000,\n",
    "                 random_state=None,\n",
    "                 copy=True,\n",
    "                 transform = 'all')`\n",
    "- `toDataFrame(transform = 'all')`\n",
    "- `oneHotEncodePredictors( transform = 'all')`\n",
    "- `oneHotEncodeTargets(transform = 'all'):`\n",
    "\n",
    "Ordinary classes all come with `transform = 'all'` parameter. This parameter dictates whether the transformation should be applied to the entire dataset `'all'`, only training `'train'`, only validation`'val'` or only test `'test'`. Bear in mind `transform = 'train'/'val'/'test'` is only available after the `splitTrainValTest` special step, before that step transform should be set to it's default `'all'`\n",
    "\n",
    "Special steps:\n",
    "- `splitTrainValTest(split = [0.5, 0.25, 0.25])`\n",
    "- `toPyTorchDatasets(wrapper = CustomDataset):`\n",
    "\n",
    "`splitTrainValTest(split = [train, val, test])`, is responsible for splitting the dataset into the train, validation and test sets. `toPyTorchDatasets(wrapper = CustomDataset)` is responsible for wrapping datasets into a pytorch friendly dataset format. Bear in mind after calling `toPyTorchDatasets`, no other pre-processing steps can be called. More on that in section 3.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Adding pre-processing steps to a queue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we need to add objects we've initiated into their corresponding tasks. `dataPreProcessingEventEmitter` works like a standard queue which sequentially applies all operations stored in that namespace, in our case `regression` or `classification`. Feel free to apply different pre-processing steps to different tasks, or add your own ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.event_handler import dataPreProcessingEventEmitter\n",
    "\n",
    "data_pre_processing = dataPreProcessingEventEmitter()\n",
    "\n",
    "# Transformations will be applied in the order they're added to data_pre_processing\n",
    "data_pre_processing.add_pre_processing_step('regression', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing_step('regression', trunctuate_data)\n",
    "data_pre_processing.add_pre_processing_step('regression', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing_step('regression', to_data_frame)\n",
    "data_pre_processing.add_pre_processing_step('regression', split_train_val_test)\n",
    "data_pre_processing.add_pre_processing_step('regression', quantile_transform)\n",
    "\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('classification', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing_step('classification', trunctuate_data)\n",
    "data_pre_processing.add_pre_processing_step('classification', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing_step('classification', one_hot_encode_targets) # different steps can be applied to different tasks\n",
    "data_pre_processing.add_pre_processing_step('classification', to_data_frame)\n",
    "data_pre_processing.add_pre_processing_step('classification', split_train_val_test)\n",
    "data_pre_processing.add_pre_processing_step('classification', quantile_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Adding your own pre-processing steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you'd like to add your own pre-processing steps using the following code template. The pre-processing step requires to have `self.seed = None`, `self.parent = None`, and `self.transform = transform`. The last one needs to be passed as an argument when initialising the object. The transformation steps are meant to receive X as `pd.DataFrame` and y as either `pd.DataFrame` or `pd.Series`, categorical_indicator as `List[bool]` and attribute_names as `List[str]`, and they're required to return them in the same format. You need to ensure proper type handling yourself! Transformation steps should be stateless. Although every step has a `transform` attribute you don't need to manually code separate transformations for `train`, `val`, `test`, the `dataPreProcessingEventEmitter` handles calling transformation steps on the appropriate partitions of the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class exampleTransformationTemplate():\n",
    "    def __init__(self, transform = 'all'): # You can add more arguments\n",
    "        self.seed = None\n",
    "        self.parent = None\n",
    "        self.transform = transform\n",
    "        # you can add more\n",
    "\n",
    "    def apply(self, X, y, categorical_indicator, attribute_names):\n",
    "        if not self.seed:\n",
    "            self.seed = self.parent.seed\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # Remember to adjust categorical_indicator as well as attribute_names if your transformation adjusts them\n",
    "        # ---------\n",
    "\n",
    "        return X, y, categorical_indicator, attribute_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Model specific transformations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After task specific data transformations we can add model specific transformations. We're not required to add model specific transformations, if an event emitter doesn't find any model names, it does nothing. Remember to add them to the same event emitter as the task specific ones'!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import CustomDataset, toPyTorchDatasets\n",
    "\n",
    "to_pytorch_datasets = toPyTorchDatasets(wrapper = CustomDataset)\n",
    "\n",
    "# Transformations will be called after general pre-processing steps, and in order they're added\n",
    "data_pre_processing.add_pre_processing_step('our_pytorch_regression_model', to_pytorch_datasets)\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('our_pytorch_classification_model', to_pytorch_datasets)\n",
    "\n",
    "# If our models don't require any additional pre-processing steps, you shouldn't add anything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`toPyTorchDatasets(wrapper = CustomDataset)` is responsible for wrapping datasets into a pytorch friendly dataset format. Bear in mind after calling `toPyTorchDatasets`, no other pre-processing steps can be called. By default, when indexed, the `CustomDataset` returns a tuple of tensors `torch.Size([1, number_of_predictor_columns])` and `torch.Size([1, number_of_outcome_columns])`, of `torch.float` data type. `CustomDataset` can be directly passed to `torch.utils.data.DataLoader()`. If your models requires different dimensions you can create your own pytorch dataset object and pass it as a wrapper to `toPyTorchDatasets`, skeleton code below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Creating your own format of output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Dataset object has the same requirements as a standard pytorch dataset object, but there're 2 additional constraints:\n",
    "- has to take in specifically X, y, categorical_indicator, attribute_names\n",
    "- has to have a `get_dims` methods that returns a number of predictor columns and output columns in form of a dict\n",
    "\n",
    "You can find more on creating your own dataset object in pytorch's official documentation:\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y, categorical_indicator, attribute_names, tensor_type=torch.float):\n",
    "        assert isinstance(X, pd.DataFrame), \"X must be a Pandas DataFrame\"\n",
    "        assert isinstance(Y, pd.DataFrame), \"Y must be a Pandas DataFrame\"\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "    def get_dims(self):\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "        return {'input_dim': num_columns_X, 'output_dim':num_columns_Y}\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 How data transformations will be applied"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `data_pre_processing` object from `dataPreProcessingEventEmitter` class, has several methods:\n",
    "- `.add_pre_processing_step('event_name', transformation_object)` to add a new transformation step object under that event name\n",
    "- `.set_seef_for_all(seed)` allows for setting the same seed for all transformations which require one\n",
    "- `.set_dataset(X, y, categorical_indicator, attribute_names)` which is how you pass it the datasets\n",
    "- `.apply('event_name')` applies all of the transformations defined under the event name\n",
    "- `.get_train_val_test()` retrieves train, val, and test datasets from the data_pre_processing event\n",
    "- `.reset(hard = False)` reset the dataset (called automatically when calling `.set_dataset`)\n",
    " - `.reset(hard = True)` to reset everything including the seed and all the added transformation steps\n",
    "- `.get('name')` retrieves a specific split of the data, name can be `train`, `val`, `test`\n",
    "\n",
    "When `.apply('event_name')` doesn't find any `'event_name'` it does nothing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example\n",
    "data_pre_processing.set_seed_for_all(seed)\n",
    "data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "data_pre_processing.apply('regression')\n",
    "data_pre_processing.apply('our_pytorch_regression_model')\n",
    "train_data, val_data, test_data = data_pre_processing.get_train_val_test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, if `toPyTorchDatasets(wrapper = CustomDataset)` is not applied, train_data, val_data, test_data, come in as a tuple of the corresponding `(X, y, categorical_indicator, attribute_names)`, if `toDataFrame` is included in the steps, both X, y are assured to be `pandas.DataFrame`. The `one_hot_encode_targets` encodes y with one-hot-encoding, so for binary classification it will have 2 columns or n columns for n-multi-label-classification."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Model Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we need to add how our metrics will be calculated. Due to different models outputting different formats, you need to code in your own class  that will be responsible for calculating metrics. You're also free to add more metrics. Even if you're using different model for different tasks you still need to add them under `model_name` and `task`, this is to ensure proper assignment of"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "our_sklearn_metrics_for_both_regression_and_classification = ourSklearnMetricsForBothRegressionAndClassification()\n",
    "our_pytorch_regression_metrics = ourPytorchRegressionMetrics()\n",
    "our_pytorch_classification_metrics = ourPytorchClassificationMetrics()\n",
    "\n",
    "\n",
    "sklearn_metrics = {'regression': our_sklearn_metrics_for_both_regression_and_classification,\n",
    "                   'classification': our_sklearn_metrics_for_both_regression_and_classification}\n",
    "\n",
    "pytorch_regression_metrics = {'regression': our_pytorch_regression_metrics}\n",
    "\n",
    "pytorch_classification_metrics = {'classification': our_pytorch_classification_metrics}\n",
    "\n",
    "metric_model_pairs = {\n",
    "    'our_sklearn_model': sklearn_metrics,\n",
    "    'our_pytorch_regression_model': pytorch_classification_metrics,\n",
    "    'our_pytorch_classification_model': pytorch_regression_metrics\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Adding your own sklearn metrics class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: create a unified sklearn metrics calculator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Adding your own pytorch metrics class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Metrics class is required to have `get_metrics()`, and `get_all()` methods. Only difference is `get_all()` outputs both metrics and a loss while `get_metrics()` outputs only metrics. This was done to avoid calculating train_loss twice, both during training and after the training. If computational power is not of concern, you can implement only one of them and adjust the main training loop to only use one of them. By default we gather"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyPytorchModelMetrics():\n",
    "    def __init__(self):\n",
    "        # You can add initialisation parameters if you wish to control how metrics are gathered at run-time\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_metrics(self, model, test_loader, hyperparameters, test_set_name=None):\n",
    "        if test_set_name:\n",
    "            assert isinstance(test_set_name, str), \"test_set_name must be a string, such as train, val, test\"\n",
    "\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            model: your model\n",
    "            test_loader: pytorch DataLoader, returns tensors of batch_size x num_columns from dataset wrapper initialised in section 3.\n",
    "            hyperparameters: your hyperparameters initialised in section 1.\n",
    "            test_set_name: str appended to the key's of the dictionary returned at the end\n",
    "        \"\"\"\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy_score': accuracy,\n",
    "            'roc_auc_score': roc_auc,\n",
    "            'confusion_matrix': confusion_matrix # Add more metrics here if you wish to gather more\n",
    "        }\n",
    "\n",
    "        if test_set_name:\n",
    "            metrics_name = f'{test_set_name}_metrics'\n",
    "            final_metrics = {metrics_name: metrics}\n",
    "        else:\n",
    "            final_metrics = {'metrics': metrics}\n",
    "\n",
    "        return final_metrics\n",
    "\n",
    "    \"\"\"\n",
    "    Example output for classification:\n",
    "        {'train_metrics': {\n",
    "            'accuracy_score': 0.59045,\n",
    "            'roc_auc_score': 0.023819148540496826,\n",
    "            'confusion_matrix': [[521, 14],[16, 312]]\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Example output for regression:\n",
    "    {'train_metrics': {'RMSE': 0.5904541168055158,\n",
    "                       'r2_score': 0.023819148540496826,\n",
    "                       'se_quant': {0.01: 7.687292236369103e-05, 0.025: 0.00033428650931455195, 0.05: 0.0007752738310955465\n",
    "                                    0.1: 0.003265600185841322, 0.2: 0.008951567113399506, (...), 0.99: 1.9776358604431152\n",
    "                       }\n",
    "     }\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def get_all(self, model, test_loader, hyperparameters, test_set_name=None):\n",
    "        if test_set_name:\n",
    "            assert isinstance(test_set_name, str), \"test_set_name must be a string, such as train, val, test\"\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy_score': accuracy,\n",
    "            'roc_auc_score': roc_auc,\n",
    "            'confusion_matrix': confusion_matrix}\n",
    "\n",
    "        if test_set_name:\n",
    "            metrics_name = f'{test_set_name}_metrics'\n",
    "            final_metrics = {metrics_name: metrics}\n",
    "        else:\n",
    "            final_metrics = {'metrics': metrics}\n",
    "\n",
    "\n",
    "        return final_metrics\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Example output for classification:\n",
    "        {'train_loss': 0.357,\n",
    "        'train_metrics': {\n",
    "            'accuracy_score': 0.59045,\n",
    "            'roc_auc_score': 0.023819148540496826,\n",
    "            'confusion_matrix': [[521, 14],[16, 312]]\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Example output for regression:\n",
    "    {'train_loss': 0.35755656490325927,\n",
    "     'train_metrics': {'RMSE': 0.5904541168055158,\n",
    "                       'r2_score': 0.023819148540496826,\n",
    "                       'se_quant': {0.01: 7.687292236369103e-05, 0.025: 0.00033428650931455195, 0.05: 0.0007752738310955465\n",
    "                                    0.1: 0.003265600185841322, 0.2: 0.008951567113399506, (...), 0.99: 1.9776358604431152\n",
    "                       }\n",
    "     }\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additionally, we've implemented `ConfusionMatrix()` in the `metrics/LcnMetrics.py` file that can simplify your confusion matrix calculations:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from metrics.LcnMetrics import ConfusionMatrix\n",
    "\n",
    "preds_1 = torch.tensor([1, 0, 0, 1, 1])\n",
    "target_1 = torch.tensor([1, 0, 1, 0, 1])\n",
    "\n",
    "preds_2 = torch.tensor([0, 0, 0, 1, 1])\n",
    "target_2 = torch.tensor([0, 0, 1, 0, 1])\n",
    "\n",
    "preds_3 = torch.tensor([0, 0, 1])\n",
    "target_3 = torch.tensor([0, 0, 1])\n",
    "\n",
    "confusion_accumulator = ConfusionMatrix()\n",
    "\n",
    "confusion_accumulator.update(preds_1, target_1)\n",
    "confusion_accumulator.update(preds_2, target_2)\n",
    "confusion_accumulator.update(preds_3, target_3)\n",
    "\n",
    "confusion_matrix = confusion_accumulator.compute()\n",
    "confusion_matrix = [[int(i) for i in row] for row in confusion_matrix.cpu().numpy().tolist()]\n",
    "# confusion_matrix: [[5, 3],[2, 4]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Model Training routine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sklearn_model_training = {'regression': our_sklearn_metrics_for_both_regression_and_classification,\n",
    "                   'classification': our_sklearn_metrics_for_both_regression_and_classification}\n",
    "\n",
    "pytorch_regression_model_training = {'regression': our_pytorch_regression_metrics}\n",
    "\n",
    "pytorch_classification_model_metrics = {'classification': our_pytorch_classification_metrics}\n",
    "\n",
    "metric_model_pairs = {\n",
    "    'our_sklearn_model': ,\n",
    "    'our_pytorch_regression_model': pytorch_classification_metrics,\n",
    "    'our_pytorch_classification_model': pytorch_regression_metrics\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}