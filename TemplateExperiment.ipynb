{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Template Experiment notebook\n",
    "This notebook walks you through how to read, add and modify experiments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table of contents\n",
    "\n",
    "TODO: Expand"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Set-up and necessary requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: Explain the set-up and requirements"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip uninstall NeuralNetworksTrainingPackage --y\n",
    "#%pip uninstall TabularExperimentTrackerClient --y\n",
    "!pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
    "!pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ex = ExperimentOrchstratorClient()\n",
    "ex.get_credentials"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Defining the experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Defining hyperparameter space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example we'll create 2 models:\n",
    "- One sklearn model\n",
    "- One Pytorch model\n",
    "\n",
    "We begin by creating the possible space of hyperparameters and parameters which will later be used to build our models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "our_sklearn_model_space = {'param_1': {\"distribution\": \"int_uniform\", \"min\":1, \"max\":11},\n",
    "                           'param_2': {\"distribution\": \"float_uniform\", \"min\":0.5, \"max\":1}}\n",
    "\n",
    "our_pytorch_regression_model_space = {'param_1': {'distribution': 'constant', 'value': 0.1},\n",
    "                                      'param_2': {'distribution': 'categorical', 'values':[10, 10, 15, 20]}}\n",
    "\n",
    "our_pytorch_classification_model_space = {'param_1': {'distribution': 'constant', 'value': 0.1},\n",
    "                                         'param_2': {'distribution': 'categorical', 'values':[10, 10, 15, 20]}}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Possible distributions:\n",
    "- TODO: list all distributions\n",
    "\n",
    "Remember that your further code should be able to re-create the models and training based on only the hyperparemeter space, so we also recommend including parameters like 'seed', 'cuda' and 'task'."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "XGBoost_space = {\n",
    "    \"max_depth\": {\"distribution\": \"int_uniform\", \"min\":1, \"max\":11},\n",
    "    \"n_estimators\": {\"distribution\": \"int_uniform\", \"min\":100, \"max\":200},\n",
    "    \"min_child_weight\": {\"distribution\": \"log_uniform\", \"min\":1, \"max\":1e2},\n",
    "    \"subsample\": {\"distribution\": \"float_uniform\", \"min\":0.5, \"max\":1},\n",
    "    \"colsample_bylevel\": {\"distribution\": \"float_uniform\", \"min\":0.5, \"max\":1},\n",
    "    \"colsample_bytree\": {\"distribution\": \"float_uniform\", \"min\":0.5, \"max\":1},\n",
    "    \"gamma\": {\"distribution\": \"log_uniform\", \"min\":1e-8, \"max\":7},\n",
    "    \"reg_lambda\": {\"distribution\": \"log_uniform\", \"min\":1, \"max\":4},\n",
    "    \"reg_alpha\": {\"distribution\": \"log_uniform\", \"min\":1e-8, \"max\":1e2},\n",
    "}\n",
    "\n",
    "LCN_reg_SGD_space = {\n",
    "    'depth': {'distribution': 'int_uniform', 'min':1, 'max':11},\n",
    "    'seed': {'distribution': 'constant', 'value': 42},\n",
    "    'drop_type': {'distribution': 'categorical', 'values':['node_dropconnect', 'none']},\n",
    "    'p': {'distribution': 'float_uniform', 'min':0.25, 'max':0.75},\n",
    "    'ensemble_n': {'distribution': 'constant', 'value': 1},\n",
    "    'shrinkage': {'distribution': 'constant', 'value': 1},\n",
    "    'back_n': {'distribution': 'categorical', 'values':[0, 0, 0, 1]},\n",
    "    'net_type': {'distribution': 'constant', 'value': 'locally_constant'},\n",
    "    'hidden_dim': {'distribution': 'constant', 'value': 1},\n",
    "    'anneal': {'distribution': 'categorical', 'values':['interpolation', 'none', 'approx']},\n",
    "    'optimizer': {'distribution': 'constant', 'value': 'SGD'},\n",
    "    'batch_size': {'distribution': 'categorical', 'values':[16,32,64,64,64,128,256]},\n",
    "    'epochs': {'distribution': 'constant', 'value': 30},\n",
    "    'lr': {'distribution': 'log_uniform', 'min':0.05, 'max':0.2},\n",
    "    'momentum': {'distribution': 'constant', 'value': 0.9},\n",
    "    'no_cuda': {'distribution': 'constant', 'value': False},\n",
    "    'lr_step_size': {'distribution': 'categorical', 'values':[10, 10, 15, 20]},\n",
    "    'gamma': {'distribution': 'constant', 'value': 0.1},\n",
    "    'task': {'distribution': 'constant', 'value': 'regression'}\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Assigning distributions to their models:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_groups = {\n",
    "    'our_sklearn_model':{'model':'our_sklearn_model', 'hype':our_sklearn_model_space},\n",
    "    'our_pytorch_regression_model':{'model':'our_pytorch_regression_model', 'hype':our_pytorch_regression_model_space},\n",
    "    'our_pytorch_classification_model':{'model':'our_pytorch_classification_model', 'hype':our_pytorch_regression_model_space},\n",
    "}\n",
    "\n",
    "ex.def_model_groups(model_groups)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.3 Assign experiments to appropriate tasks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO Explain:\n",
    "- opml_reg_purnum_group:\n",
    "- opml_reg_numcat_group:\n",
    "- opml_class_purnum_group:\n",
    "- opml_class_numcat_group:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regression_models = {'our_sklearn_model':{'model':'our_sklearn_model', 'hype':our_sklearn_model_space},\n",
    "                     'our_pytorch_regression_model':{'model':'our_pytorch_regression_model', 'hype':our_pytorch_regression_model_space}}\n",
    "\n",
    "classification_models = {\n",
    "    'our_sklearn_model':{'model':'our_sklearn_model', 'hype':our_sklearn_model_space},\n",
    "    'our_pytorch_classification_model':{'model':'our_pytorch_classification_model', 'hype':our_pytorch_regression_model_space}}\n",
    "\n",
    "\n",
    "applications = {'opml_reg_purnum_group': regression_models,\n",
    "                'opml_reg_numcat_group': regression_models,\n",
    "                'opml_class_purnum_group': classification_models,\n",
    "                'opml_class_numcat_group': classification_models}\n",
    "\n",
    "ex.def_applications(applications)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Registering an Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = 'template_experiment'\n",
    "ex.reg_experiment(experiment_name)\n",
    "exp_info = ex.experiment_info()\n",
    "successful_runs = exp_info['successful_runs']\n",
    "required_runs = exp_info['required_runs']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Task pre-processing steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our experiments data pre-processing steps are applied right after downloading datasets in the main loop of the experiment.\n",
    "Given that different models and tasks might require different pre-processing steps, we apply pre-processing steps in an event driven fashion. Those events are separated into task specific data pre-processing events that are applied based on the task, and model specific data pre-processing steps. The main object holding and executing all pre-processing steps is the `dataPreProcessingEventEmitter()` from `NeuralNetworksTrainingPackage.event_handler`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Defining task specific pre-processing steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly, we begin by initialising task pre-processing steps stored in `NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing` module. Task pre-processing steps come in form of classes. When initialised those objects store parameters of how data pre-processing steps should be applied."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import filterCardinality, quantileTransform, trunctuateData, oneHotEncodePredictors, oneHotEncodeTargets, toDataFrame, splitTrainValTest\n",
    "\n",
    "n_sample = 20000\n",
    "split = [0.5, 0.25, 0.25]\n",
    "quantile_transform_distribution='uniform'\n",
    "\n",
    "filter_cardinality = filterCardinality(transform = 'all')\n",
    "trunctuate_data = trunctuateData(n = n_sample, transform = 'all')\n",
    "one_hot_encode_predictors = oneHotEncodePredictors(transform = 'all')\n",
    "one_hot_encode_targets = oneHotEncodeTargets(transform = 'all')\n",
    "to_data_frame = toDataFrame(transform = 'all')\n",
    "split_train_val_test = splitTrainValTest(split = split) # Special transformation\n",
    "quantile_transform = quantileTransform(output_distribution = quantile_transform_distribution, transform = 'train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-processing steps can be divided into two, ordinary and special.\n",
    "\n",
    "Ordinary steps:\n",
    "- `trunctuateData(n, seed = None, transform = 'all')`\n",
    "- `filterCardinality(transform = 'all')`\n",
    "- `quantileTransform(n_quantiles=1000, output_distribution='uniform',\n",
    "                 ignore_implicit_zeros=False,\n",
    "                 subsample=10000,\n",
    "                 random_state=None,\n",
    "                 copy=True,\n",
    "                 transform = 'all')`\n",
    "- `toDataFrame(transform = 'all')`\n",
    "- `oneHotEncodePredictors( transform = 'all')`\n",
    "- `oneHotEncodeTargets(transform = 'all'):`\n",
    "\n",
    "Ordinary classes all come with `transform = 'all'` parameter. This parameter dictates whether the transformation should be applied to the entire dataset `'all'`, only training `'train'`, only validation`'val'` or only test `'test'`. Bear in mind `transform = 'train'/'val'/'test'` is only available after the `splitTrainValTest` special step, before that step transform should be set to it's default `'all'`\n",
    "\n",
    "Special steps:\n",
    "- `splitTrainValTest(split = [0.5, 0.25, 0.25])`\n",
    "- `toPyTorchDatasets(wrapper = CustomDataset):`\n",
    "\n",
    "`splitTrainValTest(split = [train, val, test])`, is responsible for splitting the dataset into the train, validation and test sets. `toPyTorchDatasets(wrapper = CustomDataset)` is responsible for wrapping datasets into a pytorch friendly dataset format. Bear in mind after calling `toPyTorchDatasets`, no other pre-processing steps can be called. More on that in section 3.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Adding pre-processing step objects to an event listener"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we need to add objects we've initiated into their corresponding tasks. `dataPreProcessingEventEmitter` works like a standard event emitter which fires events when it receives the name of the event, in our case `regression` or `classification` Feel free to apply different pre-processing steps to different tasks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.event_handler import dataPreProcessingEventEmitter\n",
    "\n",
    "data_pre_processing = dataPreProcessingEventEmitter()\n",
    "\n",
    "# Transformations will be applied in the order they're added to data_pre_processing\n",
    "data_pre_processing.add_pre_processing_step('regression', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing_step('regression', trunctuate_data)\n",
    "data_pre_processing.add_pre_processing_step('regression', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing_step('regression', to_data_frame)\n",
    "data_pre_processing.add_pre_processing_step('regression', split_train_val_test)\n",
    "data_pre_processing.add_pre_processing_step('regression', quantile_transform)\n",
    "\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('classification', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing_step('classification', trunctuate_data)\n",
    "data_pre_processing.add_pre_processing_step('classification', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing_step('classification', one_hot_encode_targets) # different steps can be applied to different tasks\n",
    "data_pre_processing.add_pre_processing_step('classification', to_data_frame)\n",
    "data_pre_processing.add_pre_processing_step('classification', split_train_val_test)\n",
    "data_pre_processing.add_pre_processing_step('classification', quantile_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Adding your own pre-processing steps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you'd like to add your own pre-processing steps using the following code template. The pre-processing step requires to have `self.seed = None`, `self.parent = None`, and `self.transform = transform`. The last one needs to be passed as an argument when initialising the object. The transformation steps are meant to receive X as `pd.DataFrame` and y as either `pd.DataFrame` or `pd.Series`, categorical_indicator as `List[bool]` and attribute_names as `List[str]`, and they're required to return them in the same format. You need to ensure proper type handling yourself! Transformation steps should be stateless. Although every step has a `transform` attribute you don't need to manually code separate transformations for `train`, `val`, `test`, the `dataPreProcessingEventEmitter` handles calling transformation steps on the appropriate partitions of the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class exampleTransformationTemplate():\n",
    "    def __init__(self, transform = 'all'): # You can add more arguments\n",
    "        self.seed = None\n",
    "        self.parent = None\n",
    "        self.transform = transform\n",
    "        # you can add more\n",
    "\n",
    "    def apply(self, X, y, categorical_indicator, attribute_names):\n",
    "        if not self.seed:\n",
    "            self.seed = self.parent.seed\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # Remember to adjust categorical_indicator as well as attribute_names if your transformation adjusts them\n",
    "        # ---------\n",
    "\n",
    "        return X, y, categorical_indicator, attribute_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Model specific transformations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After task specific data transformations we can add model specific transformations. We're not required to add model specific transformations, if an event emitter doesn't find any model names, it does nothing. Remember to add them to the same event emitter as the task specific ones'!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import CustomDataset, toPyTorchDatasets\n",
    "\n",
    "to_pytorch_datasets = toPyTorchDatasets(wrapper = CustomDataset)\n",
    "\n",
    "# Transformations will be called after general pre-processing steps, and in order they're added\n",
    "data_pre_processing.add_pre_processing_step('our_pytorch_regression_model', to_pytorch_datasets)\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('our_pytorch_classification_model', to_pytorch_datasets)\n",
    "\n",
    "# If our models don't require any additional pre-processing steps, you shouldn't add anything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`toPyTorchDatasets(wrapper = CustomDataset)` is responsible for wrapping datasets into a pytorch friendly dataset format. Bear in mind after calling `toPyTorchDatasets`, no other pre-processing steps can be called. By default, when indexed, the `CustomDataset` returns a tuple of tensors `torch.Size([1, number_of_predictor_columns])` and `torch.Size([1, number_of_outcome_columns])`, of `torch.float` data type. `CustomDataset` can be directly passed to `torch.utils.data.DataLoader()`. If your models requires different dimensions you can create your own pytorch dataset object and pass it as a wrapper to `toPyTorchDatasets`, skeleton code below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Creating your own format of output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Dataset object has the same requirements as a standard pytorch dataset object, but there're 2 additional constraints:\n",
    "- has to take in specifically X, y, categorical_indicator, attribute_names\n",
    "- has to have a `get_dims` methods that returns a number of predictor columns and output columns in form of a dict\n",
    "\n",
    "You can find more on creating your own dataset object in pytorch's official documentation:\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y, categorical_indicator, attribute_names, tensor_type=torch.float):\n",
    "        assert isinstance(X, pd.DataFrame), \"X must be a Pandas DataFrame\"\n",
    "        assert isinstance(Y, pd.DataFrame), \"Y must be a Pandas DataFrame\"\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "    def get_dims(self):\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "        return {'input_dim': num_columns_X, 'output_dim':num_columns_Y}\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "        return\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # ---------\n",
    "        # Add your own code here\n",
    "        # ---------\n",
    "\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 How data transformations will be applied"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `data_pre_processing` object from `dataPreProcessingEventEmitter` class, has several methods:\n",
    "- `.add_pre_processing_step('event_name', transformation_object)` to add a new transformation step object under that event name\n",
    "- `.set_seef_for_all(seed)` allows for setting the same seed for all transformations which require one\n",
    "- `.set_dataset(X, y, categorical_indicator, attribute_names)` which is how you pass it the datasets\n",
    "- `.apply('event_name')` applies all of the transformations defined under the event name\n",
    "- `.get_train_val_test()` retrieves train, val, and test datasets from the data_pre_processing event\n",
    "- `.reset(hard = False)` reset the dataset or `hard = True` to reset everything including all added transformations steps (called automatically when calling `.set_dataset`)\n",
    "- `.get('name')` retrieves a specific split of the data, name can be `train`, `val`, `test`\n",
    "\n",
    "When `.apply('event_name')` doesn't find any `'event_name'` it does nothing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example\n",
    "data_pre_processing.set_seed_for_all(seed)\n",
    "data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "data_pre_processing.apply('regression')\n",
    "data_pre_processing.apply('our_pytorch_regression_model')\n",
    "train_data, val_data, test_data = data_pre_processing.get_train_val_test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, if `toPyTorchDatasets(wrapper = CustomDataset)` is not applied, train_data, val_data, test_data, come in as a tuple of the corresponding `(X, y, categorical_indicator, attribute_names)`, if `toDataFrame` is included in the steps, both X, y are assured to be `pandas.DataFrame`. The `one_hot_encode_targets` encodes y with one-hot-encoding, so for binary classification it will have 2 columns or n columns for n-multi-label-classification."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Model Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we need to add how our metrics will be calculated. Due to different models outputting different formats, you need to code in your own class  that will be responsible for calculating metrics. You're also free to add more metrics. Even if you're using different model for different tasks you still need to add them under `model_name` and `task`, this is to ensure proper assignment of"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "our_sklearn_metrics_for_both_regression_and_classification = ourSklearnMetricsForBothRegressionAndClassification()\n",
    "our_pytorch_regression_metrics = ourPytorchRegressionMetrics()\n",
    "our_pytorch_classification_metrics = ourPytorchClassificationMetrics()\n",
    "\n",
    "\n",
    "sklearn_metrics = {'regression': our_sklearn_metrics_for_both_regression_and_classification,\n",
    "                   'classification': our_sklearn_metrics_for_both_regression_and_classification}\n",
    "\n",
    "pytorch_regression_metrics = {'regression': our_pytorch_regression_metrics}\n",
    "\n",
    "pytorch_classification_metrics = {'classification': our_pytorch_classification_metrics}\n",
    "\n",
    "metric_model_pairs = {\n",
    "    'our_sklearn_model': sklearn_metrics,\n",
    "    'our_pytorch_regression_model': pytorch_classification_metrics,\n",
    "    'our_pytorch_classification_model': pytorch_regression_metrics\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}