{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22311,
     "status": "ok",
     "timestamp": 1693527126975,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "hG3fkgsQBBCL",
    "outputId": "902e6808-21ae-4d46-fb4a-8ceb6bf74e5e",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drive version:\n",
    "# !pip uninstall TabularExperimentTrackerClient --y\n",
    "# !pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
    "# !pip uninstall NeuralNetworksTrainingPackage --y\n",
    "# !pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: NeuralNetworksTrainingPackage 1.0.0\n",
      "Uninstalling NeuralNetworksTrainingPackage-1.0.0:\n",
      "  Successfully uninstalled NeuralNetworksTrainingPackage-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: TabularExperimentTrackerClient 0.0.1\n",
      "Uninstalling TabularExperimentTrackerClient-0.0.1:\n",
      "  Successfully uninstalled TabularExperimentTrackerClient-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
      "  Cloning https://github.com/DanielWarfield1/TabularExperimentTrackerClient to /tmp/pip-req-build-exgqigmy\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/DanielWarfield1/TabularExperimentTrackerClient /tmp/pip-req-build-exgqigmy\n",
      "  Resolved https://github.com/DanielWarfield1/TabularExperimentTrackerClient to commit 5e738443d55b9637454776bd740a8083af70272d\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: openml in /opt/conda/lib/python3.10/site-packages (from TabularExperimentTrackerClient==0.0.1) (0.14.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from TabularExperimentTrackerClient==0.0.1) (2.28.2)\n",
      "Requirement already satisfied: liac-arff>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.5.0)\n",
      "Requirement already satisfied: xmltodict in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (0.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.0.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.23.5)\n",
      "Requirement already satisfied: minio in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (7.1.16)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (12.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (2023.5.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->openml->TabularExperimentTrackerClient==0.0.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->openml->TabularExperimentTrackerClient==0.0.1) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil->openml->TabularExperimentTrackerClient==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (3.1.0)\n",
      "Building wheels for collected packages: TabularExperimentTrackerClient\n",
      "  Building wheel for TabularExperimentTrackerClient (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for TabularExperimentTrackerClient: filename=TabularExperimentTrackerClient-0.0.1-py3-none-any.whl size=6408 sha256=14b4f8399a34b0ca4d1123f670d4340d3f72e717fdc365d648b9ca33d7e053ad\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v2hh6rfm/wheels/f9/2e/f3/69345202c956e07475c8f2f55074ad4d97b3e6a976b70fafab\n",
      "Successfully built TabularExperimentTrackerClient\n",
      "Installing collected packages: TabularExperimentTrackerClient\n",
      "Successfully installed TabularExperimentTrackerClient-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage\n",
      "  Cloning https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to /tmp/pip-req-build-1yv5s6rq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Bartosz-G/NeuralNetworksTrainingPackage /tmp/pip-req-build-1yv5s6rq\n",
      "  Resolved https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to commit e2b6589a9f980eb47d026bb480f73ad2e3610023\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.23.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.2.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (2.0.0)\n",
      "Requirement already satisfied: torcheval in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->NeuralNetworksTrainingPackage==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->NeuralNetworksTrainingPackage==1.0.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->NeuralNetworksTrainingPackage==1.0.0) (1.3.0)\n",
      "Building wheels for collected packages: NeuralNetworksTrainingPackage\n",
      "  Building wheel for NeuralNetworksTrainingPackage (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NeuralNetworksTrainingPackage: filename=NeuralNetworksTrainingPackage-1.0.0-py3-none-any.whl size=10422 sha256=9e6f81a184e218a552c30442d015ec1fc05f63af1b1877154145b1ea7891886d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zuzhtc5b/wheels/82/01/e8/da81de869be2f6b8c193a6fc5c42ee3ff0b6b97a7b70cd565a\n",
      "Successfully built NeuralNetworksTrainingPackage\n",
      "Installing collected packages: NeuralNetworksTrainingPackage\n",
      "Successfully installed NeuralNetworksTrainingPackage-1.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall NeuralNetworksTrainingPackage --y\n",
    "%pip uninstall TabularExperimentTrackerClient --y\n",
    "%pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
    "%pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5858,
     "status": "ok",
     "timestamp": 1693527132821,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "jguLhhgFBSkE",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43635,
     "status": "ok",
     "timestamp": 1693527176450,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "xsHc3zlSCO7J",
    "outputId": "121cd4b8-d138-4d2f-a0dc-d0de85c48f65",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from TabularExperimentTrackerClient.ExperimentClient import ExperimentClient\n",
    "\n",
    "path =  '../creds/'\n",
    "creds_orch_file = \"creds-orch.txt\"\n",
    "creds_openml_file = \"creds-openml.txt\"\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(path, creds_orch_file), 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    orchname = lines[0].strip()\n",
    "    orchsecret = lines[1].strip()\n",
    "\n",
    "with open (os.path.join(path, creds_openml_file), \"r\") as myfile:\n",
    "    openMLAPIKey = myfile.read()\n",
    "\n",
    "ex = ExperimentClient(verbose = True)\n",
    "\n",
    "\n",
    "ex.define_orch_cred(orchname, orchsecret)\n",
    "ex.define_opml_cred(openMLAPIKey)\n",
    "\n",
    "# Colab version\n",
    "# ex.define_opml_cred_drive('/My Drive/research/non-homogenous-data/creds/creds-openml.txt')\n",
    "# ex.define_orch_cred_drive('bart', '/My Drive//research/non-homogenous-data/creds/creds-colab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBYDfcuJE-bP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Defining the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1693527176452,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "lyMEiU5ODM2e",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = 'test_Mlp_4'\n",
    "\n",
    "\n",
    "# Mlp parameters\n",
    "depth = {'distribution': 'categorical', 'values':[2,3,4,5,6,7,8,9,10,11,12]}\n",
    "hidden_dim = {'distribution': 'categorical', 'values':[32, 64, 128, 256, 512]}\n",
    "seed = {'distribution': 'constant', 'value': 42}\n",
    "regularize = {'distribution': 'categorical', 'values':[None, None, None, 0.25, 0.5, 0.75, 'bn', 'bn','bn']} # float implies dropout\n",
    "embd_size = {'distribution': 'categorical', 'values':[None, None, None, 'sqrt', 64, 128, 256]} # 'sqrt implies embedding is sqrt smaller than the number of categories\n",
    "optimizer = {'distribution': 'categorical', 'values': ['SGD', 'SGD', 'SGD', 'SGD', 'Adam']}\n",
    "batch_size = {'distribution': 'categorical', 'values':[64, 128, 256, 512, 1024]}\n",
    "epochs = {'distribution': 'constant', 'value': 30}\n",
    "lr = {'distribution': 'log_uniform', 'min':1e-5, 'max':1e-2}\n",
    "momentum = {'distribution': 'categorical', 'values': [0.5, 0.9]}\n",
    "no_cuda = {'distribution': 'constant', 'value': False}\n",
    "lr_step_size = {'distribution': 'constant', 'value': 10}\n",
    "gamma = {'distribution': 'constant', 'value': 0.1}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1693527176453,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "3atPjhTUEn05",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# MLP on datasets containing categorical variables\n",
    "#============================================================\n",
    "Mlp_relu_cat_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'regularize':regularize,\n",
    "    'embd_size':embd_size,\n",
    "    'activation': {'distribution': 'constant', 'value': 'relu'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'optimizer': optimizer,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma\n",
    "    }\n",
    "\n",
    "Mlp_sigm_cat_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'regularize':regularize,\n",
    "    'embd_size':embd_size,\n",
    "    'activation': {'distribution': 'constant', 'value': 'sigmoid'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'optimizer': optimizer,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma\n",
    "}\n",
    "\n",
    "#============================================================\n",
    "# MLP on continous predictors\n",
    "#============================================================\n",
    "\n",
    "Mlp_relu_cont_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'regularize':regularize,\n",
    "    'embd_size': {'distribution': 'constant', 'value': None},\n",
    "    'activation': {'distribution': 'constant', 'value': 'relu'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'optimizer': optimizer,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma\n",
    "}\n",
    "\n",
    "\n",
    "Mlp_sigm_cont_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'regularize':regularize,\n",
    "    'embd_size': {'distribution': 'constant', 'value': None},\n",
    "    'activation': {'distribution': 'constant', 'value': 'sigmoid'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'optimizer': optimizer,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1693527176453,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "5UI-B-EbEJUh",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_groups = {\n",
    "    'Mlp_relu_cat':{'model':'Mlp_relu_cat', 'hype':Mlp_relu_cat_space},\n",
    "    'Mlp_sigm_cat':{'model':'Mlp_sigm_cat', 'hype':Mlp_sigm_cat_space},\n",
    "    'Mlp_relu_cont':{'model':'Mlp_relu_cont', 'hype':Mlp_relu_cont_space},\n",
    "    'Mlp_sigm_cont':{'model':'Mlp_sigm_cont', 'hype':Mlp_sigm_cont_space}\n",
    "}\n",
    "\n",
    "ex.def_model_groups(model_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automatically defined data groups: dict_keys(['opml_reg_purnum_group', 'opml_class_purnum_group', 'opml_reg_numcat_group', 'opml_class_numcat_group'])\n",
      "existing experiment found\n"
     ]
    }
   ],
   "source": [
    "ex.def_data_groups_opml()\n",
    "print(f'automatically defined data groups: {ex.data_groups.keys()}')\n",
    "\n",
    "categorical_models = [k for k in model_groups.keys() if '_cat' in k]\n",
    "continous_models = [k for k in model_groups.keys() if '_cont' in k]\n",
    "\n",
    "\n",
    "applications = {'opml_reg_purnum_group': continous_models,\n",
    "                'opml_reg_numcat_group': categorical_models,\n",
    "                'opml_class_purnum_group': continous_models,\n",
    "                'opml_class_numcat_group': categorical_models}\n",
    "\n",
    "ex.def_applications(applications)\n",
    "ex.reg_experiment(experiment_name)\n",
    "\n",
    "\n",
    "# Required to distinguish between classification or regression tasks\n",
    "regression_tasks = ex.__dict__['data_groups']['opml_reg_purnum_group'] + ex.__dict__['data_groups']['opml_reg_numcat_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful runs: 0\n",
      "total required runs: 7080\n"
     ]
    }
   ],
   "source": [
    "exp_info = ex.experiment_info()\n",
    "successful_runs = exp_info['successful_runs']\n",
    "required_runs = exp_info['required_runs']\n",
    "print('successful runs: {}'.format(successful_runs))\n",
    "print('total required runs: {}'.format(required_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. General Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.event_handler import dataPreProcessingEventEmitter\n",
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import filterCardinality, quantileTransform, truncateData ,balancedTruncateData, oneHotEncodePredictors, oneHotEncodeTargets, toDataFrame, splitTrainValTest, balancedSplitTrainValTest\n",
    "\n",
    "n_sample = 20000\n",
    "split = [0.5, 0.25, 0.25]\n",
    "quantile_transform_distribution='normal'\n",
    "\n",
    "\n",
    "data_pre_processing = dataPreProcessingEventEmitter()\n",
    "\n",
    "filter_cardinality = filterCardinality(transform = 'all')\n",
    "truncate_data = truncateData(n = n_sample, transform = 'all')\n",
    "balanced_truncate_data = balancedTruncateData(n = n_sample, transform = 'all') # Ensures balance of classes\n",
    "one_hot_encode_predictors = oneHotEncodePredictors(transform = 'all')\n",
    "one_hot_encode_targets = oneHotEncodeTargets(transform = 'all')\n",
    "to_data_frame = toDataFrame(transform = 'all')\n",
    "split_train_val_test = splitTrainValTest(split = split)\n",
    "balanced_split_train_val_test = balancedSplitTrainValTest(split = split)\n",
    "quantile_transform = quantileTransform(output_distribution = quantile_transform_distribution, transform = 'all')\n",
    "\n",
    "\n",
    "# Transformations will be called in the order they're added to data_pre_processing\n",
    "data_pre_processing.add_pre_processing_step('regression', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing_step('regression', truncate_data)\n",
    "data_pre_processing.add_pre_processing_step('regression', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing_step('regression', to_data_frame)\n",
    "data_pre_processing.add_pre_processing_step('regression', split_train_val_test)\n",
    "data_pre_processing.add_pre_processing_step('regression', quantile_transform)\n",
    "\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('classification', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing_step('classification', balanced_truncate_data)\n",
    "data_pre_processing.add_pre_processing_step('classification', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing_step('classification', one_hot_encode_targets)\n",
    "data_pre_processing.add_pre_processing_step('classification', to_data_frame)\n",
    "data_pre_processing.add_pre_processing_step('classification', balanced_split_train_val_test)\n",
    "data_pre_processing.add_pre_processing_step('classification', quantile_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Model Specific Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import CustomCategoricalSplitDataset, toPyTorchDatasets\n",
    "\n",
    "pytorch_data_class = CustomCategoricalSplitDataset\n",
    "to_pytorch_datasets = toPyTorchDatasets(wrapper = pytorch_data_class)\n",
    "\n",
    "\n",
    "# Transformations will be called after general pre-processing steps, and in order they're added\n",
    "data_pre_processing.add_pre_processing_step('Mlp_relu_cat', to_pytorch_datasets)\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('Mlp_sigm_cat', to_pytorch_datasets)\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('Mlp_relu_cont', to_pytorch_datasets)\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('Mlp_sigm_cont', to_pytorch_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from metrics.MlpMetrics import MlpMetricsClassification, MlpMetricsRegression\n",
    "\n",
    "mlp_metrics_regression = MlpMetricsRegression()\n",
    "mlp_metrics_classification = MlpMetricsClassification()\n",
    "\n",
    "mlp_metrics = {'regression': mlp_metrics_regression,\n",
    "               'classification': mlp_metrics_classification}\n",
    "\n",
    "metric_model_pairs = {\n",
    "    'Mlp_relu_cat': mlp_metrics,\n",
    "    'Mlp_sigm_cat': mlp_metrics,\n",
    "    'Mlp_relu_cont': mlp_metrics,\n",
    "    'Mlp_sigm_cont': mlp_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Model Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.MlpNetwork import init_MlpNetwork\n",
    "from training.MlpTrain import MlpTrainingRoutine\n",
    "\n",
    "mlp_model_and_training = {'model_init':init_MlpNetwork, 'training_routine': MlpTrainingRoutine}\n",
    "\n",
    "model_training_pairs = {\n",
    "    'Mlp_relu_cat': mlp_model_and_training,\n",
    "    'Mlp_sigm_cat': mlp_model_and_training,\n",
    "    'Mlp_relu_cont': mlp_model_and_training,\n",
    "    'Mlp_sigm_cont': mlp_model_and_training,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Main Experiment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Begin run:0 ====\n",
      "{'_id': '65103ffba838e5e2247b4565', 'metrics_per_epoch': [], 'experiment_id': '65103e58cb8a63188c88a2a0', 'experiment_name': 'test_Mlp_4', 'mtpair_index': 103, 'mtpair_model': 'Mlp_sigm_cont', 'mtpair_task': '337-361278', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 9, 'seed': 42, 'regularize': 0.75, 'embd_size': None, 'activation': 'sigmoid', 'hidden_dim': 64, 'optimizer': 'SGD', 'batch_size': 256, 'epochs': 30, 'lr': 6.697274667054666e-05, 'momentum': 0.9, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1}, 'model': 'Mlp_sigm_cont', 'task': '337-361278'}\n",
      "5103ffba838e5e2247b456\n",
      "---- Loading datasets ----\n",
      "downloading task 337-361278\n",
      "task different than previous task, downloading...\n",
      "----1th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0030093513488769533, 'epoch': 1}\n",
      "----2th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0029636173367500306, 'epoch': 2}\n",
      "----3th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002921079885959625, 'epoch': 3}\n",
      "----4th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002897361361980438, 'epoch': 4}\n",
      "----5th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.00286522319316864, 'epoch': 5}\n",
      "----6th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028500404477119446, 'epoch': 6}\n",
      "----7th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028260387897491453, 'epoch': 7}\n",
      "----8th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028243080258369448, 'epoch': 8}\n",
      "----9th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002806750237941742, 'epoch': 9}\n",
      "----10th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002801382613182068, 'epoch': 10}\n",
      "----11th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028048949003219603, 'epoch': 11}\n",
      "----12th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002804939615726471, 'epoch': 12}\n",
      "----13th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.00280457843542099, 'epoch': 13}\n",
      "----14th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.00279769229888916, 'epoch': 14}\n",
      "----15th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002809142744541168, 'epoch': 15}\n",
      "----16th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028045780897140503, 'epoch': 16}\n",
      "----17th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002803860652446747, 'epoch': 17}\n",
      "----18th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028011539697647093, 'epoch': 18}\n",
      "----19th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028022398591041565, 'epoch': 19}\n",
      "----20th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028015374422073365, 'epoch': 20}\n",
      "----21th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028028523206710815, 'epoch': 21}\n",
      "----22th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002807159328460693, 'epoch': 22}\n",
      "----23th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002801106369495392, 'epoch': 23}\n",
      "----24th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0027977328896522523, 'epoch': 24}\n",
      "----25th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002801034927368164, 'epoch': 25}\n",
      "----26th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002802528393268585, 'epoch': 26}\n",
      "----27th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0028021711468696592, 'epoch': 27}\n",
      "----28th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002800280690193176, 'epoch': 28}\n",
      "----29th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.002802967882156372, 'epoch': 29}\n",
      "----30th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0027992717146873474, 'train_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.47295724, 'confusion_matrix': [[2500, 0], [2500, 0]]}, 'val_loss': 1746.2132568359375, 'val_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.49291103999999997, 'confusion_matrix': [[1250, 0], [1250, 0]]}, 'test_loss': 1746.21337890625, 'test_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.49296096, 'confusion_matrix': [[1250, 0], [1250, 0]]}, 'epoch': 30, 'epoch_time': 68.85962915420532}\n",
      "run ended\n",
      "==== Begin run:1 ====\n",
      "{'_id': '6510404bb9a0fc2631d650ea', 'metrics_per_epoch': [], 'experiment_id': '65103e58cb8a63188c88a2a0', 'experiment_name': 'test_Mlp_4', 'mtpair_index': 85, 'mtpair_model': 'Mlp_sigm_cont', 'mtpair_task': '337-361066', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 12, 'seed': 42, 'regularize': None, 'embd_size': None, 'activation': 'sigmoid', 'hidden_dim': 64, 'optimizer': 'SGD', 'batch_size': 512, 'epochs': 30, 'lr': 4.58619715991094e-05, 'momentum': 0.5, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1}, 'model': 'Mlp_sigm_cont', 'task': '337-361066'}\n",
      "510404bb9a0fc2631d650e\n",
      "---- Loading datasets ----\n",
      "downloading task 337-361066\n",
      "task different than previous task, downloading...\n",
      "----1th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014426301764648368, 'epoch': 1}\n",
      "----2th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014428389505250733, 'epoch': 2}\n",
      "----3th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014425638989853968, 'epoch': 3}\n",
      "----4th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.00144240544170547, 'epoch': 4}\n",
      "----5th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014422162916404215, 'epoch': 5}\n",
      "----6th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014424255052961849, 'epoch': 6}\n",
      "----7th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.001442628023573923, 'epoch': 7}\n",
      "----8th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014424380394045416, 'epoch': 8}\n",
      "----9th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014420119924372155, 'epoch': 9}\n",
      "----10th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014427026759117345, 'epoch': 10}\n",
      "----11th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014428401678665322, 'epoch': 11}\n",
      "----12th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014427340337259866, 'epoch': 12}\n",
      "----13th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014427672851825084, 'epoch': 13}\n",
      "----14th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014427657635056846, 'epoch': 14}\n",
      "----15th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014421405008629123, 'epoch': 15}\n",
      "----16th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014423476968879237, 'epoch': 16}\n",
      "----17th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014423122023670468, 'epoch': 17}\n",
      "----18th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014421732676371864, 'epoch': 18}\n",
      "----19th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.001442655932254041, 'epoch': 19}\n",
      "----20th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.001442240920261608, 'epoch': 20}\n",
      "----21th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014424133882399947, 'epoch': 21}\n",
      "----22th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.00144217157688516, 'epoch': 22}\n",
      "----23th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.001442757952231322, 'epoch': 23}\n",
      "----24th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014421370742721384, 'epoch': 24}\n",
      "----25th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014424474174424488, 'epoch': 25}\n",
      "----26th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014425854166228544, 'epoch': 26}\n",
      "----27th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014422749607357424, 'epoch': 27}\n",
      "----28th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014426540386117714, 'epoch': 28}\n",
      "----29th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014425505420443871, 'epoch': 29}\n",
      "----30th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0014424815030033037, 'train_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.5, 'confusion_matrix': [[2644, 0], [2644, 0]]}, 'val_loss': 1833.3682861328125, 'val_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.49546142208774585, 'confusion_matrix': [[1322, 0], [1322, 0]]}, 'test_loss': 1834.7550048828125, 'test_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.49263038548752835, 'confusion_matrix': [[1323, 0], [1323, 0]]}, 'epoch': 30, 'epoch_time': 69.59226775169373}\n",
      "run ended\n",
      "==== Begin run:2 ====\n",
      "{'_id': '651040979c0c6fdd84dac0c6', 'metrics_per_epoch': [], 'experiment_id': '65103e58cb8a63188c88a2a0', 'experiment_name': 'test_Mlp_4', 'mtpair_index': 68, 'mtpair_model': 'Mlp_relu_cat', 'mtpair_task': '335-361293', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 2, 'seed': 42, 'regularize': 0.75, 'embd_size': None, 'activation': 'relu', 'hidden_dim': 32, 'optimizer': 'SGD', 'batch_size': 256, 'epochs': 30, 'lr': 0.008917615029669644, 'momentum': 0.5, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1}, 'model': 'Mlp_relu_cat', 'task': '335-361293'}\n",
      "51040979c0c6fdd84dac0c\n",
      "---- Loading datasets ----\n",
      "downloading task 335-361293\n",
      "task different than previous task, downloading...\n",
      "----1th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 4.096967685317993, 'epoch': 1}\n",
      "----2th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.936827090835571, 'epoch': 2}\n",
      "----3th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.911793941116333, 'epoch': 3}\n",
      "----4th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.907235933685303, 'epoch': 4}\n",
      "----5th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9040350971221924, 'epoch': 5}\n",
      "----6th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9025978843688964, 'epoch': 6}\n",
      "----7th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9022252075195314, 'epoch': 7}\n",
      "----8th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9023831535339357, 'epoch': 8}\n",
      "----9th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.902159972000122, 'epoch': 9}\n",
      "----10th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.907879418182373, 'epoch': 10}\n",
      "----11th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.903647055053711, 'epoch': 11}\n",
      "----12th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9027478607177732, 'epoch': 12}\n",
      "----13th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.902106935882568, 'epoch': 13}\n",
      "----14th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.901818169403076, 'epoch': 14}\n",
      "----15th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.901621802520752, 'epoch': 15}\n",
      "----16th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.901659804534912, 'epoch': 16}\n",
      "----17th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9014059730529786, 'epoch': 17}\n",
      "----18th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9013198043823243, 'epoch': 18}\n",
      "----19th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9013647636413573, 'epoch': 19}\n",
      "----20th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9012265182495116, 'epoch': 20}\n",
      "----21th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9012564655303956, 'epoch': 21}\n",
      "----22th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9012086917877196, 'epoch': 22}\n",
      "----23th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9011725299835205, 'epoch': 23}\n",
      "----24th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.901195185089111, 'epoch': 24}\n",
      "----25th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9012168991088867, 'epoch': 25}\n",
      "----26th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9012214168548582, 'epoch': 26}\n",
      "----27th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9012138496398925, 'epoch': 27}\n",
      "----28th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.901112313079834, 'epoch': 28}\n",
      "----29th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9011433433532714, 'epoch': 29}\n",
      "----30th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3.9011009689331053, 'train_metrics': {'RMSE': 1.975197762257609, 'r2_score': -0.00011396408081054688, 'se_quant': {0.01: 0.06956261396408081, 0.025: 0.07048330456018448, 0.05: 0.18200835585594177, 0.1: 0.1830321103334427, 0.2: 0.18623511493206024, 0.3: 1.255797266960144, 0.4: 2.2888262271881104, 0.5: 2.7224130630493164, 0.6: 3.880427360534668, 0.7: 4.925954818725586, 0.8: 6.289445877075195, 0.9: 8.985312461853027, 0.95: 12.805682182312012, 0.975: 16.093353271484375, 0.99: 19.49856948852539}}, 'val_loss': 4.013767242431641, 'val_metrics': {'RMSE': 2.003438854178395, 'r2_score': -0.00016260147094726562, 'se_quant': {0.01: 0.06961680203676224, 0.025: 0.0704018622636795, 0.05: 0.18204480409622192, 0.1: 0.18302206695079803, 0.2: 0.1862298846244812, 0.3: 1.2558505535125732, 0.4: 1.8639086484909058, 0.5: 2.7259159088134766, 0.6: 3.876675605773926, 0.7: 4.928285121917725, 0.8: 6.312191486358643, 0.9: 9.411059379577637, 0.95: 13.32068920135498, 0.975: 17.015851974487305, 0.99: 20.247455596923828}}, 'test_loss': 3.7553257942199707, 'test_metrics': {'RMSE': 1.9378662993663858, 'r2_score': -0.00018024444580078125, 'se_quant': {0.01: 0.06947867572307587, 0.025: 0.07027147710323334, 0.05: 0.1819009631872177, 0.1: 0.1828925758600235, 0.2: 0.18497595191001892, 0.3: 1.2534384727478027, 0.4: 1.8501218557357788, 0.5: 2.3368072509765625, 0.6: 3.395775318145752, 0.7: 4.9122724533081055, 0.8: 6.065178394317627, 0.9: 8.48508358001709, 0.95: 12.77824592590332, 0.975: 16.12706756591797, 0.99: 19.721792221069336}}, 'epoch': 30, 'epoch_time': 118.87352585792542}\n",
      "run ended\n",
      "==== Begin run:3 ====\n",
      "{'_id': '6510411843633b214514c832', 'metrics_per_epoch': [], 'experiment_id': '65103e58cb8a63188c88a2a0', 'experiment_name': 'test_Mlp_4', 'mtpair_index': 73, 'mtpair_model': 'Mlp_sigm_cont', 'mtpair_task': '337-361055', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 10, 'seed': 42, 'regularize': 0.75, 'embd_size': None, 'activation': 'sigmoid', 'hidden_dim': 512, 'optimizer': 'SGD', 'batch_size': 64, 'epochs': 30, 'lr': 1.2224278210490956e-05, 'momentum': 0.9, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1}, 'model': 'Mlp_sigm_cont', 'task': '337-361055'}\n",
      "510411843633b214514c83\n",
      "---- Loading datasets ----\n",
      "downloading task 337-361055\n",
      "task different than previous task, downloading...\n",
      "----1th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010887286405120529, 'epoch': 1}\n",
      "----2th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010876754429877917, 'epoch': 2}\n",
      "----3th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010874058354009333, 'epoch': 3}\n",
      "----4th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010878086867601807, 'epoch': 4}\n",
      "----5th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010872945360671737, 'epoch': 5}\n",
      "----6th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010884063445876922, 'epoch': 6}\n",
      "----7th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010875569512936532, 'epoch': 7}\n",
      "----8th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010883946276669755, 'epoch': 8}\n",
      "----9th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010882588452046541, 'epoch': 9}\n",
      "----10th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010877214054732645, 'epoch': 10}\n",
      "----11th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010882107812916621, 'epoch': 11}\n",
      "----12th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010880967934717111, 'epoch': 12}\n",
      "----13th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010873734230570612, 'epoch': 13}\n",
      "----14th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010883305726466766, 'epoch': 14}\n",
      "----15th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010875565175978175, 'epoch': 15}\n",
      "----16th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.01089602549033852, 'epoch': 16}\n",
      "----17th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010874203192725486, 'epoch': 17}\n",
      "----18th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010868175384129788, 'epoch': 18}\n",
      "----19th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010885284977303903, 'epoch': 19}\n",
      "----20th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010876732402694687, 'epoch': 20}\n",
      "----21th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010869053817925241, 'epoch': 21}\n",
      "----22th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010873242905974217, 'epoch': 22}\n",
      "----23th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010878716161685906, 'epoch': 23}\n",
      "----24th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010876102495159242, 'epoch': 24}\n",
      "----25th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0108856245297573, 'epoch': 25}\n",
      "----26th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010877078168126258, 'epoch': 26}\n",
      "----27th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010889591384356304, 'epoch': 27}\n",
      "----28th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010867991198929604, 'epoch': 28}\n",
      "----29th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010876439558141479, 'epoch': 29}\n",
      "----30th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.010872179060075886, 'train_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.48664258874072197, 'confusion_matrix': [[4178, 0], [4178, 0]]}, 'val_loss': 2895.965576171875, 'val_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.4794195976508282, 'confusion_matrix': [[2089, 0], [2089, 0]]}, 'test_loss': 2897.3515625, 'test_metrics': {'accuracy_score': 0.5, 'roc_auc_score': 0.46965414024404206, 'confusion_matrix': [[2090, 0], [2090, 0]]}, 'epoch': 30, 'epoch_time': 114.64386940002441}\n",
      "run ended\n",
      "==== Begin run:4 ====\n",
      "{'_id': '65104193b9a0fc2631d6d412', 'metrics_per_epoch': [], 'experiment_id': '65103e58cb8a63188c88a2a0', 'experiment_name': 'test_Mlp_4', 'mtpair_index': 0, 'mtpair_model': 'Mlp_relu_cont', 'mtpair_task': '336-361072', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 4, 'seed': 42, 'regularize': 'bn', 'embd_size': None, 'activation': 'relu', 'hidden_dim': 64, 'optimizer': 'SGD', 'batch_size': 256, 'epochs': 30, 'lr': 8.899798647233036e-05, 'momentum': 0.5, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1}, 'model': 'Mlp_relu_cont', 'task': '336-361072'}\n",
      "5104193b9a0fc2631d6d41\n",
      "---- Loading datasets ----\n",
      "downloading task 336-361072\n",
      "task different than previous task, downloading...\n",
      "----1th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 6918.710021972656, 'epoch': 1}\n",
      "----2th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 5908.001281738281, 'epoch': 2}\n",
      "----3th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 4601.283462524414, 'epoch': 3}\n",
      "----4th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 3071.993377685547, 'epoch': 4}\n",
      "----5th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 1714.0771408081055, 'epoch': 5}\n",
      "----6th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 866.3062171936035, 'epoch': 6}\n",
      "----7th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 495.4319591522217, 'epoch': 7}\n",
      "----8th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 374.47596645355225, 'epoch': 8}\n",
      "----9th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 340.54725551605225, 'epoch': 9}\n",
      "----10th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 330.3050079345703, 'epoch': 10}\n",
      "----11th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 330.3469409942627, 'epoch': 11}\n",
      "----12th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 328.66022872924805, 'epoch': 12}\n",
      "----13th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 328.1357889175415, 'epoch': 13}\n",
      "----14th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 327.7906503677368, 'epoch': 14}\n",
      "----15th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 328.76863193511963, 'epoch': 15}\n",
      "----16th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 326.3125410079956, 'epoch': 16}\n",
      "----17th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 326.01508808135986, 'epoch': 17}\n",
      "----18th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 327.1743278503418, 'epoch': 18}\n",
      "----19th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 326.257493019104, 'epoch': 19}\n",
      "----20th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 325.64513969421387, 'epoch': 20}\n",
      "----21th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 325.3115921020508, 'epoch': 21}\n",
      "----22th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 325.31142234802246, 'epoch': 22}\n",
      "----23th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 326.62042713165283, 'epoch': 23}\n",
      "----24th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 325.88889503479004, 'epoch': 24}\n",
      "----25th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 326.5660467147827, 'epoch': 25}\n",
      "----26th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 325.29394149780273, 'epoch': 26}\n",
      "----27th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 324.81433868408203, 'epoch': 27}\n",
      "----28th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 325.2863082885742, 'epoch': 28}\n",
      "----29th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 324.60159492492676, 'epoch': 29}\n",
      "----30th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 324.5807294845581, 'train_metrics': {'RMSE': 17.776648759091966, 'r2_score': 0.01068347692489624, 'se_quant': {0.01: 0.030890531837940216, 0.025: 0.1531657874584198, 0.05: 0.6998383402824402, 0.1: 2.8033881187438965, 0.2: 11.175052642822266, 0.3: 24.465360641479492, 0.4: 41.9056510925293, 0.5: 63.056007385253906, 0.6: 90.86683654785156, 0.7: 124.76725769042969, 0.8: 170.7997589111328, 0.9: 235.40280151367188, 0.95: 481.5467834472656, 0.975: 6679.66162109375, 0.99: 6916.443359375}}, 'val_loss': 362.381591796875, 'val_metrics': {'RMSE': 19.036322958934978, 'r2_score': 0.014473915100097656, 'se_quant': {0.01: 0.022188542410731316, 0.025: 0.1349496692419052, 0.05: 0.8027095198631287, 0.1: 3.2539751529693604, 0.2: 13.243245124816895, 0.3: 27.78436851501465, 0.4: 44.85152816772461, 0.5: 68.56327819824219, 0.6: 94.92101287841797, 0.7: 128.7753143310547, 0.8: 175.11431884765625, 0.9: 249.0467987060547, 0.95: 531.3179931640625, 0.975: 6750.744140625, 0.99: 6986.90234375}}, 'test_loss': 344.44940185546875, 'test_metrics': {'RMSE': 18.559348098881834, 'r2_score': 0.009188711643218994, 'se_quant': {0.01: 0.055685948580503464, 0.025: 0.19998252391815186, 0.05: 0.7846783995628357, 0.1: 3.5103118419647217, 0.2: 13.01395320892334, 0.3: 27.413454055786133, 0.4: 45.069644927978516, 0.5: 66.779296875, 0.6: 94.79733276367188, 0.7: 131.05027770996094, 0.8: 178.4461212158203, 0.9: 248.08203125, 0.95: 523.0147094726562, 0.975: 6719.32177734375, 0.99: 6976.86669921875}}, 'epoch': 30, 'epoch_time': 58.150198221206665}\n",
      "run ended\n",
      "==== Begin run:5 ====\n",
      "{'_id': '651041d39c0c6fdd84db38df', 'metrics_per_epoch': [], 'experiment_id': '65103e58cb8a63188c88a2a0', 'experiment_name': 'test_Mlp_4', 'mtpair_index': 93, 'mtpair_model': 'Mlp_sigm_cont', 'mtpair_task': '337-361273', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 3, 'seed': 42, 'regularize': 'bn', 'embd_size': None, 'activation': 'sigmoid', 'hidden_dim': 64, 'optimizer': 'SGD', 'batch_size': 1024, 'epochs': 30, 'lr': 2.681412443817194e-05, 'momentum': 0.5, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1}, 'model': 'Mlp_sigm_cont', 'task': '337-361273'}\n",
      "51041d39c0c6fdd84db38d\n",
      "---- Loading datasets ----\n",
      "downloading task 337-361273\n",
      "task different than previous task, downloading...\n",
      "----1th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783640325069428, 'epoch': 1}\n",
      "----2th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783401250839233, 'epoch': 2}\n",
      "----3th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006782583057880401, 'epoch': 3}\n",
      "----4th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006782528579235077, 'epoch': 4}\n",
      "----5th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783651232719421, 'epoch': 5}\n",
      "----6th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783153176307678, 'epoch': 6}\n",
      "----7th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006782567799091339, 'epoch': 7}\n",
      "----8th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783480882644653, 'epoch': 8}\n",
      "----9th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.000678180718421936, 'epoch': 9}\n",
      "----10th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.000678180742263794, 'epoch': 10}\n",
      "----11th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006782856166362763, 'epoch': 11}\n",
      "----12th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006782652616500854, 'epoch': 12}\n",
      "----13th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781599402427673, 'epoch': 13}\n",
      "----14th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781864583492279, 'epoch': 14}\n",
      "----15th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781742513179779, 'epoch': 15}\n",
      "----16th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006782037317752839, 'epoch': 16}\n",
      "----17th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783211708068848, 'epoch': 17}\n",
      "----18th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783330738544464, 'epoch': 18}\n",
      "----19th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781189680099487, 'epoch': 19}\n",
      "----20th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006782628774642945, 'epoch': 20}\n",
      "----21th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781350076198578, 'epoch': 21}\n",
      "----22th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781690061092376, 'epoch': 22}\n",
      "----23th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783488571643829, 'epoch': 23}\n",
      "----24th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781894981861114, 'epoch': 24}\n",
      "----25th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006783084094524384, 'epoch': 25}\n",
      "----26th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781357288360596, 'epoch': 26}\n",
      "----27th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781120896339417, 'epoch': 27}\n",
      "----28th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006781183302402497, 'epoch': 28}\n",
      "----29th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.0006782406806945801, 'epoch': 29}\n",
      "----30th training epoch ----\n",
      "run updated\n",
      "{'train_loss': 0.000678358280658722, 'train_metrics': {'accuracy_score': 0.5985, 'roc_auc_score': 0.59461627, 'confusion_matrix': [[3356, 1644], [2371, 2629]]}, 'val_loss': 3386.33056640625, 'val_metrics': {'accuracy_score': 0.6042, 'roc_auc_score': 0.5986774, 'confusion_matrix': [[1703, 797], [1182, 1318]]}, 'test_loss': 3385.650146484375, 'test_metrics': {'accuracy_score': 0.5976, 'roc_auc_score': 0.60274084, 'confusion_matrix': [[1702, 798], [1214, 1286]]}, 'epoch': 30, 'epoch_time': 119.4054811000824}\n",
      "run ended\n",
      "==== Begin run:6 ====\n",
      "{'_id': '651042549c0c6fdd84db6dd1', 'metrics_per_epoch': [], 'experiment_id': '65103e58cb8a63188c88a2a0', 'experiment_name': 'test_Mlp_4', 'mtpair_index': 108, 'mtpair_model': 'Mlp_relu_cat', 'mtpair_task': '334-361113', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 10, 'seed': 42, 'regularize': 0.5, 'embd_size': 64, 'activation': 'relu', 'hidden_dim': 32, 'optimizer': 'SGD', 'batch_size': 128, 'epochs': 30, 'lr': 6.454951015124873e-05, 'momentum': 0.5, 'no_cuda': False, 'lr_step_size': 10, 'gamma': 0.1}, 'model': 'Mlp_relu_cat', 'task': '334-361113'}\n",
      "51042549c0c6fdd84db6dd\n",
      "---- Loading datasets ----\n",
      "downloading task 334-361113\n",
      "task different than previous task, downloading...\n",
      "----1th training epoch ----\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m epoch_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     74\u001b[0m training_routine\u001b[38;5;241m.\u001b[39mscheduler_step(epoch)\n\u001b[0;32m---> 75\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_routine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---Stopping training due to loss being nan!---\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/non-homogenous-paper/training/MlpTrain.py:89\u001b[0m, in \u001b[0;36mMlpTrainingRoutine.train\u001b[0;34m(self, model, train_loader)\u001b[0m\n\u001b[1;32m     87\u001b[0m data_cont\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     88\u001b[0m data_cat\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_cont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_cat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m###############\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/non-homogenous-paper/models/MlpNetwork.py:208\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x_cont, x_cat)\u001b[0m\n\u001b[1;32m    206\u001b[0m         x_cat \u001b[38;5;241m=\u001b[39m x_cat\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    207\u001b[0m         x_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeds(x_cat)\n\u001b[0;32m--> 208\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_cont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_cat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x_cat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m x_cont\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m0\u001b[39m,):\n\u001b[1;32m    210\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_cont, x_cat], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "torch_models = ('Mlp_relu_cat', 'Mlp_sigm_cat', 'Mlp_relu_cont', 'Mlp_sigm_cont')\n",
    "\n",
    "sklearn_models = ('no_sklearn_models_in_this_training')\n",
    "\n",
    "\n",
    "for i in range(7080):\n",
    "    print(f'==== Begin run:{i} ====')\n",
    "    # run_info = ex.begin_run_sticky()\n",
    "    run_info = ex.begin_run()\n",
    "\n",
    "    hyperparameters = run_info['hyp']\n",
    "    model_name = run_info['model']\n",
    "\n",
    "    if hyperparameters.get('task') is None:\n",
    "        if run_info['task'] in regression_tasks:\n",
    "            hyperparameters['task'] = 'regression'\n",
    "        else:\n",
    "            hyperparameters['task'] = 'classification'\n",
    "    task = hyperparameters['task']\n",
    "    seed = hyperparameters['seed']\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "    print('---- Loading datasets ----')\n",
    "    X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "\n",
    "    # Pre-processing\n",
    "    data_pre_processing.set_seed_for_all(seed)\n",
    "    data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "    data_pre_processing.apply(task)\n",
    "    data_pre_processing.apply(model_name)\n",
    "    train_data, val_data, test_data = data_pre_processing.get_train_val_test()\n",
    "\n",
    "\n",
    "    # Getting appropriate metrics\n",
    "    metrics_calculator = metric_model_pairs[model_name][task]\n",
    "\n",
    "\n",
    "    match model_name:\n",
    "        case _ if model_name in sklearn_models:\n",
    "            pass\n",
    "\n",
    "        case _ if model_name in torch_models:\n",
    "            # hyperparameters will be updated with {'input_dim': num_columns_X, 'output_dim':num_columns_Y}\n",
    "            hyperparameters.update(train_data.get_dims())\n",
    "\n",
    "            train_batch_size = hyperparameters['batch_size']\n",
    "            train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size,shuffle= True)\n",
    "            val_dataloader = torch.utils.data.DataLoader(val_data,batch_size=len(val_data),shuffle= True)\n",
    "            test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=len(test_data),shuffle= True)\n",
    "\n",
    "\n",
    "            init_model = model_training_pairs[model_name]['model_init']\n",
    "            TrainingRoutine = model_training_pairs[model_name]['training_routine']\n",
    "\n",
    "\n",
    "            model = init_model(**hyperparameters)\n",
    "            training_routine = TrainingRoutine(**hyperparameters)\n",
    "            \n",
    "            training_routine.set_optimizer_scheduler(model)\n",
    "\n",
    "            start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n",
    "            total_epochs = hyperparameters['epochs']\n",
    "            start_time = time.time()\n",
    "\n",
    "            for epoch in range(start_epoch, total_epochs + start_epoch):\n",
    "                print(f\"----{epoch}th training epoch ----\")\n",
    "                epoch_metrics = {}\n",
    "\n",
    "                training_routine.scheduler_step(epoch)\n",
    "                train_loss = training_routine.train(model, train_dataloader)\n",
    "\n",
    "\n",
    "                if train_loss is None:\n",
    "                    print('---Stopping training due to loss being nan!---')\n",
    "                    epoch_metrics = {'train_loss': train_loss, 'epoch': epoch}\n",
    "                    ex.update_run(epoch_metrics)\n",
    "                    break\n",
    "\n",
    "                if epoch == total_epochs:\n",
    "                    continue\n",
    "\n",
    "                epoch_metrics.update(train_loss)\n",
    "                epoch_metrics.update({'epoch': epoch})\n",
    "                ex.update_run(epoch_metrics)\n",
    "                print(epoch_metrics)\n",
    "\n",
    "            else:\n",
    "                final_metrics = {}\n",
    "                training_time = time.time()-start_time\n",
    "\n",
    "                train_metrics = metrics_calculator.get_metrics(model, train_dataloader, hyperparameters, 'train')\n",
    "                val_all_metrics = metrics_calculator.get_all(model, val_dataloader, hyperparameters, 'val')\n",
    "                test_all_metrics = metrics_calculator.get_all(model, test_dataloader, hyperparameters, 'test')\n",
    "\n",
    "                final_metrics.update(train_loss)\n",
    "                final_metrics.update(train_metrics)\n",
    "                final_metrics.update(val_all_metrics)\n",
    "                final_metrics.update(test_all_metrics)\n",
    "                final_metrics.update({'epoch': epoch})\n",
    "                final_metrics.update({'epoch_time': training_time})\n",
    "\n",
    "                ex.update_run(final_metrics)\n",
    "                print(final_metrics)\n",
    "\n",
    "    ex.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Checking whether the code works as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch_models = ('LCN_reg', 'LCN_cls', 'LLN_reg', 'LLN_cls')\n",
    "\n",
    "# sklearn_models = ('no_sklearn_models_in_this_training')\n",
    "\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(f'==== Begin run:{i} ====')\n",
    "#     run_info = ex.begin_run()\n",
    "\n",
    "#     hyperparameters = run_info['hyp']\n",
    "#     model_name = run_info['model']\n",
    "\n",
    "#     task = hyperparameters['task']\n",
    "#     seed = hyperparameters['seed']\n",
    "\n",
    "\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "\n",
    "#     print('---- Loading datasets ----')\n",
    "#     X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "\n",
    "#     # Pre-processing\n",
    "#     data_pre_processing.set_seed_for_all(seed)\n",
    "#     data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "#     data_pre_processing.apply(task)\n",
    "#     data_pre_processing.apply(model_name)\n",
    "#     train_data, val_data, test_data = data_pre_processing.get_train_val_test()\n",
    "\n",
    "\n",
    "#     # Getting appropriate metrics\n",
    "#     metrics_calculator = metric_model_pairs[model_name][task]\n",
    "\n",
    "\n",
    "#     match model_name:\n",
    "#         case _ if model_name in sklearn_models:\n",
    "#             pass\n",
    "\n",
    "#         case _ if model_name in torch_models:\n",
    "#             # hyperparameters will be updated with {'input_dim': num_columns_X, 'output_dim':num_columns_Y}\n",
    "#             hyperparameters.update(train_data.get_dims())\n",
    "\n",
    "#             train_batch_size = hyperparameters['batch_size']\n",
    "#             train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size,shuffle= True)\n",
    "#             val_dataloader = torch.utils.data.DataLoader(val_data,batch_size=len(val_data),shuffle= True)\n",
    "#             test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=len(test_data),shuffle= True)\n",
    "\n",
    "\n",
    "#             init_model = model_training_pairs[model_name]['model_init']\n",
    "#             TrainingRoutine = model_training_pairs[model_name]['training_routine']\n",
    "\n",
    "\n",
    "#             model = init_model(**hyperparameters)\n",
    "#             training_routine = TrainingRoutine(**hyperparameters)\n",
    "            \n",
    "#             training_routine.set_optimizer_scheduler(model)\n",
    "\n",
    "#             start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n",
    "#             total_epochs = hyperparameters['epochs']\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             for epoch in range(start_epoch, total_epochs + start_epoch):\n",
    "#                 print(f\"----{epoch}th training epoch ----\")\n",
    "#                 epoch_metrics = {}\n",
    "\n",
    "#                 training_routine.scheduler_step(epoch)\n",
    "#                 train_loss = training_routine.train(model, train_dataloader)\n",
    "\n",
    "\n",
    "#                 if train_loss is None:\n",
    "#                     print('---Stopping training due to loss being nan!---')\n",
    "#                     epoch_metrics = {'train_loss': train_loss, 'epoch': epoch}\n",
    "#                     ex.update_run(epoch_metrics)\n",
    "#                     break\n",
    "\n",
    "#                 if epoch == total_epochs:\n",
    "#                     continue\n",
    "\n",
    "#                 epoch_metrics.update(train_loss)\n",
    "#                 epoch_metrics.update({'epoch': epoch})\n",
    "#                 ex.update_run(epoch_metrics)\n",
    "#                 print(epoch_metrics)\n",
    "\n",
    "#             else:\n",
    "#                 final_metrics = {}\n",
    "#                 training_time = time.time()-start_time\n",
    "\n",
    "#                 train_metrics = metrics_calculator.get_metrics(model, train_dataloader, hyperparameters, 'train')\n",
    "#                 val_all_metrics = metrics_calculator.get_all(model, val_dataloader, hyperparameters, 'val')\n",
    "#                 test_all_metrics = metrics_calculator.get_all(model, test_dataloader, hyperparameters, 'test')\n",
    "\n",
    "#                 final_metrics.update(train_loss)\n",
    "#                 final_metrics.update(train_metrics)\n",
    "#                 final_metrics.update(val_all_metrics)\n",
    "#                 final_metrics.update(test_all_metrics)\n",
    "#                 final_metrics.update({'epoch': epoch})\n",
    "#                 final_metrics.update({'epoch_time': training_time})\n",
    "\n",
    "#                 ex.update_run(final_metrics)\n",
    "#                 print(final_metrics)\n",
    "\n",
    "#     ex.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch_models = ('LCN_reg', 'LCN_cls', 'LLN_reg', 'LLN_cls')\n",
    "\n",
    "# sklearn_models = ('no_sklearn_models_in_this_training')\n",
    "\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(f'==== Begin run:{i} ====')\n",
    "# #     run_info = ex.begin_run()\n",
    "\n",
    "#     hyperparameters = run_info['hyp']\n",
    "#     model_name = run_info['model']\n",
    "\n",
    "#     task = hyperparameters['task']\n",
    "#     seed = hyperparameters['seed']\n",
    "\n",
    "\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "\n",
    "#     print('---- Loading datasets ----')\n",
    "#     X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "\n",
    "#     # Pre-processing\n",
    "#     data_pre_processing.set_seed_for_all(seed)\n",
    "#     data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "#     data_pre_processing.apply(task)\n",
    "#     data_pre_processing.apply(model_name)\n",
    "#     train_data, val_data, test_data = data_pre_processing.get_train_val_test()\n",
    "\n",
    "\n",
    "#     # Getting appropriate metrics\n",
    "#     metrics_calculator = metric_model_pairs[model_name][task]\n",
    "\n",
    "\n",
    "#     match model_name:\n",
    "#         case _ if model_name in sklearn_models:\n",
    "#             pass\n",
    "\n",
    "#         case _ if model_name in torch_models:\n",
    "#             # hyperparameters will be updated with {'input_dim': num_columns_X, 'output_dim':num_columns_Y}\n",
    "#             hyperparameters.update(train_data.get_dims())\n",
    "\n",
    "#             train_batch_size = hyperparameters['batch_size']\n",
    "#             train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size,shuffle= True)\n",
    "#             val_dataloader = torch.utils.data.DataLoader(val_data,batch_size=len(val_data),shuffle= True)\n",
    "#             test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=len(test_data),shuffle= True)\n",
    "\n",
    "\n",
    "#             init_model = model_training_pairs[model_name]['model_init']\n",
    "#             TrainingRoutine = model_training_pairs[model_name]['training_routine']\n",
    "\n",
    "\n",
    "#             model = init_model(**hyperparameters)\n",
    "#             training_routine = TrainingRoutine(**hyperparameters)\n",
    "            \n",
    "#             training_routine.set_optimizer_scheduler(model)\n",
    "\n",
    "#             start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n",
    "#             total_epochs = hyperparameters['epochs']\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             for epoch in range(start_epoch, total_epochs + start_epoch):\n",
    "#                 print(f\"----{epoch}th training epoch ----\")\n",
    "#                 epoch_metrics = {}\n",
    "\n",
    "#                 training_routine.scheduler_step(epoch)\n",
    "#                 train_loss = training_routine.train(model, train_dataloader)\n",
    "\n",
    "\n",
    "#                 if train_loss is None:\n",
    "#                     print('---Stopping training due to loss being nan!---')\n",
    "#                     epoch_metrics = {'train_loss': train_loss, 'epoch': epoch}\n",
    "#                     print(epoch_metrics)\n",
    "#                     # ex.update_run(epoch_metrics)\n",
    "#                     break\n",
    "\n",
    "#                 if epoch == total_epochs:\n",
    "#                     continue\n",
    "\n",
    "#                 epoch_metrics.update(train_loss)\n",
    "#                 epoch_metrics.update({'epoch': epoch})\n",
    "#                 # ex.update_run(epoch_metrics)\n",
    "#                 print(epoch_metrics)\n",
    "\n",
    "#             else:\n",
    "#                 final_metrics = {}\n",
    "#                 training_time = time.time()-start_time\n",
    "\n",
    "#                 train_metrics = metrics_calculator.get_metrics(model, train_dataloader, hyperparameters, 'train')\n",
    "#                 val_all_metrics = metrics_calculator.get_all(model, val_dataloader, hyperparameters, 'val')\n",
    "#                 test_all_metrics = metrics_calculator.get_all(model, test_dataloader, hyperparameters, 'test')\n",
    "\n",
    "#                 final_metrics.update(train_loss)\n",
    "#                 final_metrics.update(train_metrics)\n",
    "#                 final_metrics.update(val_all_metrics)\n",
    "#                 final_metrics.update(test_all_metrics)\n",
    "#                 final_metrics.update({'epoch': epoch})\n",
    "#                 final_metrics.update({'epoch_time': training_time})\n",
    "\n",
    "#                 # ex.update_run(final_metrics)\n",
    "#                 print(final_metrics)\n",
    "\n",
    "#     # ex.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class oneHotEncodeTargets():\n",
    "#     def __init__(self, transform = 'all'):\n",
    "#         self.parent = None\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def apply(self, X, y, categorical_indicator, attribute_names):\n",
    "        \n",
    "#         y = pd.get_dummies(y, dtype=int)\n",
    "\n",
    "#         # if isinstance(y, pd.DataFrame):\n",
    "#         #     is_categorical = any(y[col].dtype.name == 'category' for col in y.columns)\n",
    "#         #     if is_categorical:\n",
    "#         #         y = pd.get_dummies(y)\n",
    "#         # \n",
    "#         # if isinstance(y, pd.Series):\n",
    "#         #     is_categorical = y.dtype.name == 'category'\n",
    "#         # \n",
    "#         #     if is_categorical:\n",
    "#         #         y = pd.get_dummies(y)\n",
    "\n",
    "#         return X, y, categorical_indicator, attribute_names\n",
    "    \n",
    "# one_hot_encode_targets = oneHotEncodeTargets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     hyperparameters = run_info['hyp']\n",
    "#     model_name = run_info['model']\n",
    "\n",
    "#     task = hyperparameters['task']\n",
    "#     seed = hyperparameters['seed']\n",
    "\n",
    "\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "\n",
    "#     print('---- Loading datasets ----')\n",
    "#     X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "#     print(f'dtype: {y.dtype}')\n",
    "    \n",
    "#     X_o, y_o, categorical_indicator_o, attribute_names_0 = one_hot_encode_targets.apply(X, y, categorical_indicator, attribute_names)\n",
    "\n",
    "#     print(y_o)\n",
    "    \n",
    "#     # Pre-processing\n",
    "# #     data_pre_processing.set_seed_for_all(seed)\n",
    "# #     data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "# #     data_pre_processing.apply(task)\n",
    "# #     data_pre_processing.apply(model_name)\n",
    "# #     train_data, val_data, test_data = data_pre_processing.get_train_val_test()\n",
    "    \n",
    "# #     print(train_data.Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_pre_processing.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "#X_c, y_c, categorical_indicator_c, attribute_names_c = deepcopy(X), deepcopy(y), deepcopy(categorical_indicator), deepcopy(attribute_names)\n",
    "X, y, categorical_indicator, attribute_names = deepcopy(X_c), deepcopy(y_c), deepcopy(categorical_indicator_c), deepcopy(attribute_names_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from NeuralNetworksTrainingPackage.event_handler import dataPreProcessingEventEmitter\n",
    "# from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import filterCardinality, quantileTransform, truncateData ,balancedTruncateData, oneHotEncodePredictors, toDataFrame, splitTrainValTest, balancedSplitTrainValTest\n",
    "\n",
    "# n_sample = 20000\n",
    "# split = [0.5, 0.25, 0.25]\n",
    "# quantile_transform_distribution='normal'\n",
    "\n",
    "# class oneHotEncodeTargets():\n",
    "#     def __init__(self, transform = 'all'):\n",
    "#         self.parent = None\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def apply(self, X, y, categorical_indicator, attribute_names):\n",
    "        \n",
    "#         y = pd.get_dummies(y, dtype=int)\n",
    "\n",
    "#         # if isinstance(y, pd.DataFrame):\n",
    "#         #     is_categorical = any(y[col].dtype.name == 'category' for col in y.columns)\n",
    "#         #     if is_categorical:\n",
    "#         #         y = pd.get_dummies(y)\n",
    "#         # \n",
    "#         # if isinstance(y, pd.Series):\n",
    "#         #     is_categorical = y.dtype.name == 'category'\n",
    "#         # \n",
    "#         #     if is_categorical:\n",
    "#         #         y = pd.get_dummies(y)\n",
    "\n",
    "#         return X, y, categorical_indicator, attribute_names\n",
    "\n",
    "\n",
    "# data_pre_processing = dataPreProcessingEventEmitter()\n",
    "\n",
    "# filter_cardinality = filterCardinality(transform = 'all')\n",
    "# truncate_data = truncateData(n = n_sample, transform = 'all')\n",
    "# balanced_truncate_data = balancedTruncateData(n = n_sample, transform = 'all') # Ensures balance of classes\n",
    "# one_hot_encode_predictors = oneHotEncodePredictors(transform = 'all')\n",
    "# one_hot_encode_targets = oneHotEncodeTargets(transform = 'all')\n",
    "# to_data_frame = toDataFrame(transform = 'all')\n",
    "# split_train_val_test = splitTrainValTest(split = split)\n",
    "# balanced_split_train_val_test = balancedSplitTrainValTest(split = split)\n",
    "# quantile_transform = quantileTransform(output_distribution = quantile_transform_distribution, transform = 'all')\n",
    "\n",
    "\n",
    "# # Transformations will be called in the order they're added to data_pre_processing\n",
    "# data_pre_processing.add_pre_processing_step('regression', filter_cardinality)\n",
    "# data_pre_processing.add_pre_processing_step('regression', truncate_data)\n",
    "# data_pre_processing.add_pre_processing_step('regression', one_hot_encode_predictors)\n",
    "# data_pre_processing.add_pre_processing_step('regression', to_data_frame)\n",
    "# data_pre_processing.add_pre_processing_step('regression', split_train_val_test)\n",
    "# data_pre_processing.add_pre_processing_step('regression', quantile_transform)\n",
    "\n",
    "\n",
    "# data_pre_processing.add_pre_processing_step('classification', filter_cardinality)\n",
    "# data_pre_processing.add_pre_processing_step('classification', balanced_truncate_data)\n",
    "# data_pre_processing.add_pre_processing_step('classification', one_hot_encode_predictors)\n",
    "# data_pre_processing.add_pre_processing_step('classification', one_hot_encode_targets)\n",
    "# data_pre_processing.add_pre_processing_step('classification', to_data_frame)\n",
    "# data_pre_processing.add_pre_processing_step('classification', balanced_split_train_val_test)\n",
    "# data_pre_processing.add_pre_processing_step('classification', quantile_transform)\n",
    "\n",
    "# from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import CustomDataset, toPyTorchDatasets\n",
    "\n",
    "# to_pytorch_datasets = toPyTorchDatasets(wrapper = CustomDataset)\n",
    "\n",
    "# # Transformations will be called after general pre-processing steps, and in order they're added\n",
    "# data_pre_processing.add_pre_processing_step('LCN_reg', to_pytorch_datasets)\n",
    "\n",
    "# data_pre_processing.add_pre_processing_step('LCN_cls', to_pytorch_datasets)\n",
    "\n",
    "# data_pre_processing.add_pre_processing_step('LLN_reg', to_pytorch_datasets)\n",
    "\n",
    "# data_pre_processing.add_pre_processing_step('LLN_cls', to_pytorch_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_pre_processing.set_seed_for_all(seed)\n",
    "data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "data_pre_processing.apply('classification')\n",
    "data_pre_processing.apply(model_name)\n",
    "train_data, val_data, test_data = data_pre_processing.get_train_val_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size,shuffle= True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data,batch_size=len(val_data),shuffle= True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=len(test_data),shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, y, categorical_indicator, attribute_names = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_categorical_columns = [attr for attr, is_cat in zip(attribute_names, categorical_indicator) if not is_cat]\n",
    "print(X[non_categorical_columns].mean())\n",
    "print(X[non_categorical_columns].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_categorical_columns = [attr for attr, is_cat in zip(attribute_names_c, categorical_indicator_c) if not is_cat]\n",
    "print(X_c[non_categorical_columns].mean())\n",
    "print(X_c[non_categorical_columns].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(X.loc[:, categorical_indicator].nunique())\n",
    "print(X_c.loc[:, categorical_indicator_c].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_data = data_pre_processing.get('val')\n",
    "test_data = data_pre_processing.get('test')\n",
    "print(val_data[1].value_counts())\n",
    "print(test_data[1].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "type(train_data.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for data, target in train_dataloader:\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'336-361072' in regression_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ex.__dict__['data_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'336-361072' in regression_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex.__dict__['data_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "authorship_tag": "ABX9TyO6d7scFvmxtcd+gePzcxnN",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
