{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22311,
     "status": "ok",
     "timestamp": 1693527126975,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "hG3fkgsQBBCL",
    "outputId": "902e6808-21ae-4d46-fb4a-8ceb6bf74e5e",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drive version:\n",
    "# !pip uninstall TabularExperimentTrackerClient --y\n",
    "# !pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
    "# !pip uninstall NeuralNetworksTrainingPackage --y\n",
    "# !pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping NeuralNetworksTrainingPackage as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Skipping TabularExperimentTrackerClient as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
      "  Cloning https://github.com/DanielWarfield1/TabularExperimentTrackerClient to /tmp/pip-req-build-tsdna2jc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/DanielWarfield1/TabularExperimentTrackerClient /tmp/pip-req-build-tsdna2jc\n",
      "  Resolved https://github.com/DanielWarfield1/TabularExperimentTrackerClient to commit 5e738443d55b9637454776bd740a8083af70272d\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting openml (from TabularExperimentTrackerClient==0.0.1)\n",
      "  Using cached openml-0.14.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from TabularExperimentTrackerClient==0.0.1) (2.28.2)\n",
      "Collecting liac-arff>=2.4.0 (from openml->TabularExperimentTrackerClient==0.0.1)\n",
      "  Using cached liac_arff-2.5.0-py3-none-any.whl\n",
      "Collecting xmltodict (from openml->TabularExperimentTrackerClient==0.0.1)\n",
      "  Using cached xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (2.0.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (1.23.5)\n",
      "Collecting minio (from openml->TabularExperimentTrackerClient==0.0.1)\n",
      "  Using cached minio-7.1.16-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from openml->TabularExperimentTrackerClient==0.0.1) (12.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->TabularExperimentTrackerClient==0.0.1) (2023.5.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->openml->TabularExperimentTrackerClient==0.0.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->openml->TabularExperimentTrackerClient==0.0.1) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil->openml->TabularExperimentTrackerClient==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18->openml->TabularExperimentTrackerClient==0.0.1) (3.1.0)\n",
      "Building wheels for collected packages: TabularExperimentTrackerClient\n",
      "  Building wheel for TabularExperimentTrackerClient (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for TabularExperimentTrackerClient: filename=TabularExperimentTrackerClient-0.0.1-py3-none-any.whl size=6408 sha256=3e11d16c7dd28d61afd23bc762d9378f3a74021aa0f8f3f54e302b6d787fd2de\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7m9g40oh/wheels/f9/2e/f3/69345202c956e07475c8f2f55074ad4d97b3e6a976b70fafab\n",
      "Successfully built TabularExperimentTrackerClient\n",
      "Installing collected packages: xmltodict, minio, liac-arff, openml, TabularExperimentTrackerClient\n",
      "Successfully installed TabularExperimentTrackerClient-0.0.1 liac-arff-2.5.0 minio-7.1.16 openml-0.14.1 xmltodict-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage\n",
      "  Cloning https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to /tmp/pip-req-build-zrtzfw6e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Bartosz-G/NeuralNetworksTrainingPackage /tmp/pip-req-build-zrtzfw6e\n",
      "  Resolved https://github.com/Bartosz-G/NeuralNetworksTrainingPackage to commit e26f4250f90f8d579bd37a2544ca284030467822\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.23.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (1.2.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from NeuralNetworksTrainingPackage==1.0.0) (2.0.0)\n",
      "Collecting torcheval (from NeuralNetworksTrainingPackage==1.0.0)\n",
      "  Using cached torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->NeuralNetworksTrainingPackage==1.0.0) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->NeuralNetworksTrainingPackage==1.0.0) (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->NeuralNetworksTrainingPackage==1.0.0) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->NeuralNetworksTrainingPackage==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->NeuralNetworksTrainingPackage==1.0.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->NeuralNetworksTrainingPackage==1.0.0) (1.3.0)\n",
      "Building wheels for collected packages: NeuralNetworksTrainingPackage\n",
      "  Building wheel for NeuralNetworksTrainingPackage (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for NeuralNetworksTrainingPackage: filename=NeuralNetworksTrainingPackage-1.0.0-py3-none-any.whl size=10407 sha256=b0f02581877c05b36f9b52837f94a9ac3badd17c33e305e1683c26ca41d93fed\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ges8vkl1/wheels/82/01/e8/da81de869be2f6b8c193a6fc5c42ee3ff0b6b97a7b70cd565a\n",
      "Successfully built NeuralNetworksTrainingPackage\n",
      "Installing collected packages: torcheval, NeuralNetworksTrainingPackage\n",
      "Successfully installed NeuralNetworksTrainingPackage-1.0.0 torcheval-0.0.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall NeuralNetworksTrainingPackage --y\n",
    "%pip uninstall TabularExperimentTrackerClient --y\n",
    "%pip install git+https://github.com/DanielWarfield1/TabularExperimentTrackerClient\n",
    "%pip install git+https://github.com/Bartosz-G/NeuralNetworksTrainingPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5858,
     "status": "ok",
     "timestamp": 1693527132821,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "jguLhhgFBSkE",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43635,
     "status": "ok",
     "timestamp": 1693527176450,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "xsHc3zlSCO7J",
    "outputId": "121cd4b8-d138-4d2f-a0dc-d0de85c48f65",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from TabularExperimentTrackerClient.ExperimentClient import ExperimentClient\n",
    "\n",
    "path =  '../creds/'\n",
    "creds_orch_file = \"creds-orch.txt\"\n",
    "creds_openml_file = \"creds-openml.txt\"\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(path, creds_orch_file), 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    orchname = lines[0].strip()\n",
    "    orchsecret = lines[1].strip()\n",
    "\n",
    "with open (os.path.join(path, creds_openml_file), \"r\") as myfile:\n",
    "    openMLAPIKey = myfile.read()\n",
    "\n",
    "ex = ExperimentClient(verbose = True)\n",
    "\n",
    "\n",
    "ex.define_orch_cred(orchname, orchsecret)\n",
    "ex.define_opml_cred(openMLAPIKey)\n",
    "\n",
    "# Colab version\n",
    "# ex.define_opml_cred_drive('/My Drive/research/non-homogenous-data/creds/creds-openml.txt')\n",
    "# ex.define_orch_cred_drive('bart', '/My Drive//research/non-homogenous-data/creds/creds-colab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBYDfcuJE-bP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Defining the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1693527176452,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "lyMEiU5ODM2e",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = 'test_Mlp_1'\n",
    "\n",
    "\n",
    "# Mlp parameters\n",
    "depth = {'distribution': 'categorical', 'values':[2,3,4,5,6,7,8,9,10,11,12]}\n",
    "hidden_dim = {'distribution': 'categorical', 'values':[64, 128, 256, 512]}\n",
    "seed = {'distribution': 'constant', 'value': 42}\n",
    "regularize = {'distribution': 'categorical', 'values':[None, None, None, 0.25, 0.5, 0.75, 'bn', 'bn','bn']} # float implies dropout\n",
    "embd_size = {'distribution': 'categorical', 'values':[None, None, None, 'sqrt', 64, 128, 256]} # 'sqrt implies embedding is sqrt smaller than the number of categories\n",
    "optimizer = {'distribution': 'categorical', 'values': ['SGD', 'SGD', 'SGD', 'SGD', 'Adam']}\n",
    "batch_size = {'distribution': 'categorical', 'values':[64, 128, 256, 512, 1024]}\n",
    "epochs = {'distribution': 'categorical', 'values':[60, 90, 120, 150]}\n",
    "lr = {'distribution': 'log_uniform', 'min':1e-5, 'max':1e-2}\n",
    "momentum = {'distribution': 'categorical', 'values':[0, 0.5, 0.9]}\n",
    "no_cuda = {'distribution': 'constant', 'value': False}\n",
    "lr_step_size = {'distribution': 'categorical', 'values':[0, 20, 30]}\n",
    "gamma = {'distribution':'categorical', 'values':[0.2, 0.1, 0.05]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1693527176453,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "3atPjhTUEn05",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#============================================================\n",
    "# MLP on datasets containing categorical variables\n",
    "#============================================================\n",
    "Mlp_relu_cat_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'regularize':regularize,\n",
    "    'embd_size':embd_size,\n",
    "    'activation': {'distribution': 'constant', 'value': 'relu'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'optimizer': optimizer,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma\n",
    "    }\n",
    "\n",
    "Mlp_sigm_cat_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'regularize':regularize,\n",
    "    'embd_size':embd_size,\n",
    "    'activation': {'distribution': 'constant', 'value': 'sigmoid'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'optimizer': optimizer,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma\n",
    "}\n",
    "\n",
    "#============================================================\n",
    "# MLP on continous predictors\n",
    "#============================================================\n",
    "\n",
    "Mlp_relu_cont_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'regularize':regularize,\n",
    "    'embd_size': {'distribution': 'constant', 'value': None},\n",
    "    'activation': {'distribution': 'constant', 'value': 'relu'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'optimizer': optimizer,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma\n",
    "}\n",
    "\n",
    "\n",
    "Mlp_sigm_cont_space = {\n",
    "    'depth': depth,\n",
    "    'seed': seed,\n",
    "    'regularize':regularize,\n",
    "    'embd_size': {'distribution': 'constant', 'value': None},\n",
    "    'activation': {'distribution': 'constant', 'value': 'sigmoid'},\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'optimizer': optimizer,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    'lr': lr,\n",
    "    'momentum': momentum,\n",
    "    'no_cuda': no_cuda,\n",
    "    'lr_step_size': lr_step_size,\n",
    "    'gamma': gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1693527176453,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "5UI-B-EbEJUh",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_groups = {\n",
    "    'Mlp_relu_cat':{'model':'Mlp_relu_cat', 'hype':Mlp_relu_cat_space},\n",
    "    'Mlp_sigm_cat':{'model':'Mlp_sigm_cat', 'hype':Mlp_sigm_cat_space},\n",
    "    'Mlp_relu_cont':{'model':'Mlp_relu_cont', 'hype':Mlp_relu_cont_space},\n",
    "    'Mlp_sigm_cont':{'model':'Mlp_sigm_cont', 'hype':Mlp_sigm_cont_space}\n",
    "}\n",
    "\n",
    "ex.def_model_groups(model_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 1771,
     "status": "ok",
     "timestamp": 1693527178212,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "BFVftUr4EMSr",
    "outputId": "096cceb4-bd60-4c4c-f052-fc8967518e9f",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automatically defined data groups: dict_keys(['opml_reg_purnum_group', 'opml_class_purnum_group', 'opml_reg_numcat_group', 'opml_class_numcat_group'])\n",
      "existing experiment found\n"
     ]
    }
   ],
   "source": [
    "ex.def_data_groups_opml()\n",
    "print(f'automatically defined data groups: {ex.data_groups.keys()}')\n",
    "\n",
    "categorical_models = [k for k in model_groups.keys() if '_cat' in k]\n",
    "continous_models = [k for k in model_groups.keys() if '_cont' in k]\n",
    "\n",
    "\n",
    "applications = {'opml_reg_purnum_group': continous_models,\n",
    "                'opml_reg_numcat_group': categorical_models,\n",
    "                'opml_class_purnum_group': continous_models,\n",
    "                'opml_class_numcat_group': categorical_models}\n",
    "\n",
    "ex.def_applications(applications)\n",
    "ex.reg_experiment(experiment_name)\n",
    "\n",
    "\n",
    "# Required to distinguish between classification or regression tasks\n",
    "regression_tasks = ex.__dict__['data_groups']['opml_reg_purnum_group'] + ex.__dict__['data_groups']['opml_reg_numcat_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1693527178723,
     "user": {
      "displayName": "bartosz azaniux",
      "userId": "12656151146140381527"
     },
     "user_tz": -60
    },
    "id": "-I95Kh4qMo1u",
    "outputId": "db2495fa-b945-4480-e310-32f6e368c0c3",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total required runs: 7080\n"
     ]
    }
   ],
   "source": [
    "exp_info = ex.experiment_info()\n",
    "successful_runs = exp_info['successful_runs']\n",
    "required_runs = exp_info['required_runs']\n",
    "print('total required runs: {}'.format(required_runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLkZmpB9yVys",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. General Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.event_handler import dataPreProcessingEventEmitter\n",
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import filterCardinality, quantileTransform, truncateData ,balancedTruncateData, oneHotEncodePredictors, oneHotEncodeTargets, toDataFrame, splitTrainValTest, balancedSplitTrainValTest\n",
    "\n",
    "n_sample = 20000\n",
    "split = [0.5, 0.25, 0.25]\n",
    "quantile_transform_distribution='normal'\n",
    "\n",
    "\n",
    "data_pre_processing = dataPreProcessingEventEmitter()\n",
    "\n",
    "filter_cardinality = filterCardinality(transform = 'all')\n",
    "truncate_data = truncateData(n = n_sample, transform = 'all')\n",
    "balanced_truncate_data = balancedTruncateData(n = n_sample, transform = 'all') # Ensures balance of classes\n",
    "one_hot_encode_predictors = oneHotEncodePredictors(transform = 'all')\n",
    "one_hot_encode_targets = oneHotEncodeTargets(transform = 'all')\n",
    "to_data_frame = toDataFrame(transform = 'all')\n",
    "split_train_val_test = splitTrainValTest(split = split)\n",
    "balanced_split_train_val_test = balancedSplitTrainValTest(split = split)\n",
    "quantile_transform = quantileTransform(output_distribution = quantile_transform_distribution, transform = 'all')\n",
    "\n",
    "\n",
    "# Transformations will be called in the order they're added to data_pre_processing\n",
    "data_pre_processing.add_pre_processing_step('regression', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing_step('regression', truncate_data)\n",
    "data_pre_processing.add_pre_processing_step('regression', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing_step('regression', to_data_frame)\n",
    "data_pre_processing.add_pre_processing_step('regression', split_train_val_test)\n",
    "data_pre_processing.add_pre_processing_step('regression', quantile_transform)\n",
    "\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('classification', filter_cardinality)\n",
    "data_pre_processing.add_pre_processing_step('classification', balanced_truncate_data)\n",
    "data_pre_processing.add_pre_processing_step('classification', one_hot_encode_predictors)\n",
    "data_pre_processing.add_pre_processing_step('classification', one_hot_encode_targets)\n",
    "data_pre_processing.add_pre_processing_step('classification', to_data_frame)\n",
    "data_pre_processing.add_pre_processing_step('classification', balanced_split_train_val_test)\n",
    "data_pre_processing.add_pre_processing_step('classification', quantile_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Model Specific Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import CustomCategoricalSplitDataset, toPyTorchDatasets\n",
    "\n",
    "pytorch_data_class = CustomCategoricalSplitDataset\n",
    "to_pytorch_datasets = toPyTorchDatasets(wrapper = pytorch_data_class)\n",
    "\n",
    "\n",
    "# Transformations will be called after general pre-processing steps, and in order they're added\n",
    "data_pre_processing.add_pre_processing_step('Mlp_relu_cat', to_pytorch_datasets)\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('Mlp_sigm_cat', to_pytorch_datasets)\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('Mlp_relu_cont', to_pytorch_datasets)\n",
    "\n",
    "data_pre_processing.add_pre_processing_step('Mlp_sigm_cont', to_pytorch_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from metrics.MlpMetrics import MlpMetricsClassification, MlpMetricsRegression\n",
    "\n",
    "mlp_metrics_regression = MlpMetricsRegression()\n",
    "mlp_metrics_classification = MlpMetricsClassification()\n",
    "\n",
    "mlp_metrics = {'regression': mlp_metrics_regression,\n",
    "               'classification': mlp_metrics_classification}\n",
    "\n",
    "metric_model_pairs = {\n",
    "    'Mlp_relu_cat': mlp_metrics,\n",
    "    'Mlp_sigm_cat': mlp_metrics,\n",
    "    'Mlp_relu_cont': mlp_metrics,\n",
    "    'Mlp_sigm_cont': mlp_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Model Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.MlpNetwork import init_MlpNetwork\n",
    "from training.MlpTrain import MlpTrainingRoutine\n",
    "\n",
    "mlp_model_and_training = {'model_init':init_MlpNetwork, 'training_routine': MlpTrainingRoutine}\n",
    "\n",
    "model_training_pairs = {\n",
    "    'Mlp_relu_cat': mlp_model_and_training,\n",
    "    'Mlp_sigm_cat': mlp_model_and_training,\n",
    "    'Mlp_relu_cont': mlp_model_and_training,\n",
    "    'Mlp_sigm_cont': mlp_model_and_training,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Main Experiment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJKPv2m6nFzd",
    "outputId": "36bedcf2-a3b9-4b13-81f1-43c5b157c89f",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Begin run:0 ====\n",
      "{'_id': '650f249a1e88c2d3788d960f', 'metrics_per_epoch': [], 'experiment_id': '650f22271e88c2d3788cb61b', 'experiment_name': 'test_Mlp_1', 'mtpair_index': 20, 'mtpair_model': 'Mlp_relu_cont', 'mtpair_task': '336-361083', 'is_completed': False, 'user_id': '64d3a7457658d6ec6db139d0', 'user_name': 'bart', 'hyp': {'depth': 2, 'seed': 42, 'regularize': None, 'embd_size': None, 'activation': 'relu', 'hidden_dim': 256, 'optimizer': 'Adam', 'batch_size': 64, 'epochs': 90, 'lr': 5.379641495585897e-05, 'momentum': 0.5, 'no_cuda': False, 'lr_step_size': 20, 'gamma': 0.1}, 'model': 'Mlp_relu_cont', 'task': '336-361083'}\n",
      "50f249a1e88c2d3788d960\n",
      "---- Loading datasets ----\n",
      "downloading task 336-361083\n",
      "using values from previous task load, skipped download\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NeuralNetworksTrainingPackage/event_handler.py:57\u001b[0m, in \u001b[0;36mdataPreProcessingEventEmitter.apply\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 57\u001b[0m     \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NeuralNetworksTrainingPackage/dataprocessing/basic_pre_processing.py:442\u001b[0m, in \u001b[0;36mtoPyTorchDatasets.apply\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m X, y, categorical_indicator, attribute_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_indicator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NeuralNetworksTrainingPackage/dataprocessing/basic_pre_processing.py:409\u001b[0m, in \u001b[0;36mCustomCategoricalSplitDataset.__init__\u001b[0;34m(self, X, Y, categorical_indicator, attribute_names, tensor_type)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXcat \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mloc[:, categorical_indicator]\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXcont \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;241;43m~\u001b[39;49m\u001b[43mcategorical_indicator\u001b[49m]\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY \u001b[38;5;241m=\u001b[39m Y\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary ~: 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m data_pre_processing\u001b[38;5;241m.\u001b[39mset_dataset(X, y, categorical_indicator, attribute_names)\n\u001b[1;32m     33\u001b[0m data_pre_processing\u001b[38;5;241m.\u001b[39mapply(task)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mdata_pre_processing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m train_data, val_data, test_data \u001b[38;5;241m=\u001b[39m data_pre_processing\u001b[38;5;241m.\u001b[39mget_train_val_test()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Getting appropriate metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/NeuralNetworksTrainingPackage/event_handler.py:91\u001b[0m, in \u001b[0;36mdataPreProcessingEventEmitter.apply\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 91\u001b[0m     X \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     92\u001b[0m     y \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     93\u001b[0m     categorical_indicator \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "torch_models = ('Mlp_relu_cat', 'Mlp_sigm_cat', 'Mlp_relu_cont', 'Mlp_sigm_cont')\n",
    "\n",
    "sklearn_models = ('no_sklearn_models_in_this_training')\n",
    "\n",
    "\n",
    "for i in range(14160):\n",
    "    print(f'==== Begin run:{i} ====')\n",
    "    run_info = ex.begin_run_sticky()\n",
    "\n",
    "    hyperparameters = run_info['hyp']\n",
    "    model_name = run_info['model']\n",
    "\n",
    "    if hyperparameters.get('task') is None:\n",
    "        if run_info['task'] in regression_tasks:\n",
    "            hyperparameters['task'] = 'regression'\n",
    "        else:\n",
    "            hyperparameters['task'] = 'classification'\n",
    "    task = hyperparameters['task']\n",
    "    seed = hyperparameters['seed']\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "    print('---- Loading datasets ----')\n",
    "    X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "\n",
    "    # Pre-processing\n",
    "    data_pre_processing.set_seed_for_all(seed)\n",
    "    data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "    data_pre_processing.apply(task)\n",
    "    data_pre_processing.apply(model_name)\n",
    "    train_data, val_data, test_data = data_pre_processing.get_train_val_test()\n",
    "\n",
    "\n",
    "    # Getting appropriate metrics\n",
    "    metrics_calculator = metric_model_pairs[model_name][task]\n",
    "\n",
    "\n",
    "    match model_name:\n",
    "        case _ if model_name in sklearn_models:\n",
    "            pass\n",
    "\n",
    "        case _ if model_name in torch_models:\n",
    "            # hyperparameters will be updated with {'input_dim': num_columns_X, 'output_dim':num_columns_Y}\n",
    "            hyperparameters.update(train_data.get_dims())\n",
    "\n",
    "            train_batch_size = hyperparameters['batch_size']\n",
    "            train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size,shuffle= True)\n",
    "            val_dataloader = torch.utils.data.DataLoader(val_data,batch_size=len(val_data),shuffle= True)\n",
    "            test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=len(test_data),shuffle= True)\n",
    "\n",
    "\n",
    "            init_model = model_training_pairs[model_name]['model_init']\n",
    "            TrainingRoutine = model_training_pairs[model_name]['training_routine']\n",
    "\n",
    "\n",
    "            model = init_model(**hyperparameters)\n",
    "            training_routine = TrainingRoutine(**hyperparameters)\n",
    "            \n",
    "            training_routine.set_optimizer_scheduler(model)\n",
    "\n",
    "            start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n",
    "            total_epochs = hyperparameters['epochs']\n",
    "            start_time = time.time()\n",
    "\n",
    "            for epoch in range(start_epoch, total_epochs + start_epoch):\n",
    "                print(f\"----{epoch}th training epoch ----\")\n",
    "                epoch_metrics = {}\n",
    "\n",
    "                training_routine.scheduler_step(epoch)\n",
    "                train_loss = training_routine.train(model, train_dataloader)\n",
    "\n",
    "\n",
    "                if train_loss is None:\n",
    "                    print('---Stopping training due to loss being nan!---')\n",
    "                    epoch_metrics = {'train_loss': train_loss, 'epoch': epoch}\n",
    "                    ex.update_run(epoch_metrics)\n",
    "                    break\n",
    "\n",
    "                if epoch == total_epochs:\n",
    "                    continue\n",
    "\n",
    "                epoch_metrics.update(train_loss)\n",
    "                epoch_metrics.update({'epoch': epoch})\n",
    "                ex.update_run(epoch_metrics)\n",
    "                print(epoch_metrics)\n",
    "\n",
    "            else:\n",
    "                final_metrics = {}\n",
    "                training_time = time.time()-start_time\n",
    "\n",
    "                train_metrics = metrics_calculator.get_metrics(model, train_dataloader, hyperparameters, 'train')\n",
    "                val_all_metrics = metrics_calculator.get_all(model, val_dataloader, hyperparameters, 'val')\n",
    "                test_all_metrics = metrics_calculator.get_all(model, test_dataloader, hyperparameters, 'test')\n",
    "\n",
    "                final_metrics.update(train_loss)\n",
    "                final_metrics.update(train_metrics)\n",
    "                final_metrics.update(val_all_metrics)\n",
    "                final_metrics.update(test_all_metrics)\n",
    "                final_metrics.update({'epoch': epoch})\n",
    "                final_metrics.update({'epoch_time': training_time})\n",
    "\n",
    "                ex.update_run(final_metrics)\n",
    "                print(final_metrics)\n",
    "\n",
    "    ex.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsz24ztP9CQ8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Checking whether the code works as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch_models = ('LCN_reg', 'LCN_cls', 'LLN_reg', 'LLN_cls')\n",
    "\n",
    "# sklearn_models = ('no_sklearn_models_in_this_training')\n",
    "\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(f'==== Begin run:{i} ====')\n",
    "#     run_info = ex.begin_run()\n",
    "\n",
    "#     hyperparameters = run_info['hyp']\n",
    "#     model_name = run_info['model']\n",
    "\n",
    "#     task = hyperparameters['task']\n",
    "#     seed = hyperparameters['seed']\n",
    "\n",
    "\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "\n",
    "#     print('---- Loading datasets ----')\n",
    "#     X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "\n",
    "#     # Pre-processing\n",
    "#     data_pre_processing.set_seed_for_all(seed)\n",
    "#     data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "#     data_pre_processing.apply(task)\n",
    "#     data_pre_processing.apply(model_name)\n",
    "#     train_data, val_data, test_data = data_pre_processing.get_train_val_test()\n",
    "\n",
    "\n",
    "#     # Getting appropriate metrics\n",
    "#     metrics_calculator = metric_model_pairs[model_name][task]\n",
    "\n",
    "\n",
    "#     match model_name:\n",
    "#         case _ if model_name in sklearn_models:\n",
    "#             pass\n",
    "\n",
    "#         case _ if model_name in torch_models:\n",
    "#             # hyperparameters will be updated with {'input_dim': num_columns_X, 'output_dim':num_columns_Y}\n",
    "#             hyperparameters.update(train_data.get_dims())\n",
    "\n",
    "#             train_batch_size = hyperparameters['batch_size']\n",
    "#             train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size,shuffle= True)\n",
    "#             val_dataloader = torch.utils.data.DataLoader(val_data,batch_size=len(val_data),shuffle= True)\n",
    "#             test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=len(test_data),shuffle= True)\n",
    "\n",
    "\n",
    "#             init_model = model_training_pairs[model_name]['model_init']\n",
    "#             TrainingRoutine = model_training_pairs[model_name]['training_routine']\n",
    "\n",
    "\n",
    "#             model = init_model(**hyperparameters)\n",
    "#             training_routine = TrainingRoutine(**hyperparameters)\n",
    "            \n",
    "#             training_routine.set_optimizer_scheduler(model)\n",
    "\n",
    "#             start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n",
    "#             total_epochs = hyperparameters['epochs']\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             for epoch in range(start_epoch, total_epochs + start_epoch):\n",
    "#                 print(f\"----{epoch}th training epoch ----\")\n",
    "#                 epoch_metrics = {}\n",
    "\n",
    "#                 training_routine.scheduler_step(epoch)\n",
    "#                 train_loss = training_routine.train(model, train_dataloader)\n",
    "\n",
    "\n",
    "#                 if train_loss is None:\n",
    "#                     print('---Stopping training due to loss being nan!---')\n",
    "#                     epoch_metrics = {'train_loss': train_loss, 'epoch': epoch}\n",
    "#                     ex.update_run(epoch_metrics)\n",
    "#                     break\n",
    "\n",
    "#                 if epoch == total_epochs:\n",
    "#                     continue\n",
    "\n",
    "#                 epoch_metrics.update(train_loss)\n",
    "#                 epoch_metrics.update({'epoch': epoch})\n",
    "#                 ex.update_run(epoch_metrics)\n",
    "#                 print(epoch_metrics)\n",
    "\n",
    "#             else:\n",
    "#                 final_metrics = {}\n",
    "#                 training_time = time.time()-start_time\n",
    "\n",
    "#                 train_metrics = metrics_calculator.get_metrics(model, train_dataloader, hyperparameters, 'train')\n",
    "#                 val_all_metrics = metrics_calculator.get_all(model, val_dataloader, hyperparameters, 'val')\n",
    "#                 test_all_metrics = metrics_calculator.get_all(model, test_dataloader, hyperparameters, 'test')\n",
    "\n",
    "#                 final_metrics.update(train_loss)\n",
    "#                 final_metrics.update(train_metrics)\n",
    "#                 final_metrics.update(val_all_metrics)\n",
    "#                 final_metrics.update(test_all_metrics)\n",
    "#                 final_metrics.update({'epoch': epoch})\n",
    "#                 final_metrics.update({'epoch_time': training_time})\n",
    "\n",
    "#                 ex.update_run(final_metrics)\n",
    "#                 print(final_metrics)\n",
    "\n",
    "#     ex.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch_models = ('LCN_reg', 'LCN_cls', 'LLN_reg', 'LLN_cls')\n",
    "\n",
    "# sklearn_models = ('no_sklearn_models_in_this_training')\n",
    "\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(f'==== Begin run:{i} ====')\n",
    "# #     run_info = ex.begin_run()\n",
    "\n",
    "#     hyperparameters = run_info['hyp']\n",
    "#     model_name = run_info['model']\n",
    "\n",
    "#     task = hyperparameters['task']\n",
    "#     seed = hyperparameters['seed']\n",
    "\n",
    "\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "\n",
    "#     print('---- Loading datasets ----')\n",
    "#     X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "\n",
    "#     # Pre-processing\n",
    "#     data_pre_processing.set_seed_for_all(seed)\n",
    "#     data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "#     data_pre_processing.apply(task)\n",
    "#     data_pre_processing.apply(model_name)\n",
    "#     train_data, val_data, test_data = data_pre_processing.get_train_val_test()\n",
    "\n",
    "\n",
    "#     # Getting appropriate metrics\n",
    "#     metrics_calculator = metric_model_pairs[model_name][task]\n",
    "\n",
    "\n",
    "#     match model_name:\n",
    "#         case _ if model_name in sklearn_models:\n",
    "#             pass\n",
    "\n",
    "#         case _ if model_name in torch_models:\n",
    "#             # hyperparameters will be updated with {'input_dim': num_columns_X, 'output_dim':num_columns_Y}\n",
    "#             hyperparameters.update(train_data.get_dims())\n",
    "\n",
    "#             train_batch_size = hyperparameters['batch_size']\n",
    "#             train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size,shuffle= True)\n",
    "#             val_dataloader = torch.utils.data.DataLoader(val_data,batch_size=len(val_data),shuffle= True)\n",
    "#             test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=len(test_data),shuffle= True)\n",
    "\n",
    "\n",
    "#             init_model = model_training_pairs[model_name]['model_init']\n",
    "#             TrainingRoutine = model_training_pairs[model_name]['training_routine']\n",
    "\n",
    "\n",
    "#             model = init_model(**hyperparameters)\n",
    "#             training_routine = TrainingRoutine(**hyperparameters)\n",
    "            \n",
    "#             training_routine.set_optimizer_scheduler(model)\n",
    "\n",
    "#             start_epoch = 1  # start from epoch 1 or last checkpoint epoch\n",
    "#             total_epochs = hyperparameters['epochs']\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             for epoch in range(start_epoch, total_epochs + start_epoch):\n",
    "#                 print(f\"----{epoch}th training epoch ----\")\n",
    "#                 epoch_metrics = {}\n",
    "\n",
    "#                 training_routine.scheduler_step(epoch)\n",
    "#                 train_loss = training_routine.train(model, train_dataloader)\n",
    "\n",
    "\n",
    "#                 if train_loss is None:\n",
    "#                     print('---Stopping training due to loss being nan!---')\n",
    "#                     epoch_metrics = {'train_loss': train_loss, 'epoch': epoch}\n",
    "#                     print(epoch_metrics)\n",
    "#                     # ex.update_run(epoch_metrics)\n",
    "#                     break\n",
    "\n",
    "#                 if epoch == total_epochs:\n",
    "#                     continue\n",
    "\n",
    "#                 epoch_metrics.update(train_loss)\n",
    "#                 epoch_metrics.update({'epoch': epoch})\n",
    "#                 # ex.update_run(epoch_metrics)\n",
    "#                 print(epoch_metrics)\n",
    "\n",
    "#             else:\n",
    "#                 final_metrics = {}\n",
    "#                 training_time = time.time()-start_time\n",
    "\n",
    "#                 train_metrics = metrics_calculator.get_metrics(model, train_dataloader, hyperparameters, 'train')\n",
    "#                 val_all_metrics = metrics_calculator.get_all(model, val_dataloader, hyperparameters, 'val')\n",
    "#                 test_all_metrics = metrics_calculator.get_all(model, test_dataloader, hyperparameters, 'test')\n",
    "\n",
    "#                 final_metrics.update(train_loss)\n",
    "#                 final_metrics.update(train_metrics)\n",
    "#                 final_metrics.update(val_all_metrics)\n",
    "#                 final_metrics.update(test_all_metrics)\n",
    "#                 final_metrics.update({'epoch': epoch})\n",
    "#                 final_metrics.update({'epoch_time': training_time})\n",
    "\n",
    "#                 # ex.update_run(final_metrics)\n",
    "#                 print(final_metrics)\n",
    "\n",
    "#     # ex.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class oneHotEncodeTargets():\n",
    "#     def __init__(self, transform = 'all'):\n",
    "#         self.parent = None\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def apply(self, X, y, categorical_indicator, attribute_names):\n",
    "        \n",
    "#         y = pd.get_dummies(y, dtype=int)\n",
    "\n",
    "#         # if isinstance(y, pd.DataFrame):\n",
    "#         #     is_categorical = any(y[col].dtype.name == 'category' for col in y.columns)\n",
    "#         #     if is_categorical:\n",
    "#         #         y = pd.get_dummies(y)\n",
    "#         # \n",
    "#         # if isinstance(y, pd.Series):\n",
    "#         #     is_categorical = y.dtype.name == 'category'\n",
    "#         # \n",
    "#         #     if is_categorical:\n",
    "#         #         y = pd.get_dummies(y)\n",
    "\n",
    "#         return X, y, categorical_indicator, attribute_names\n",
    "    \n",
    "# one_hot_encode_targets = oneHotEncodeTargets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     hyperparameters = run_info['hyp']\n",
    "#     model_name = run_info['model']\n",
    "\n",
    "#     task = hyperparameters['task']\n",
    "#     seed = hyperparameters['seed']\n",
    "\n",
    "\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "\n",
    "#     print('---- Loading datasets ----')\n",
    "#     X, y, categorical_indicator, attribute_names = ex.opml_load_task(run_info['mtpair_task'])\n",
    "#     print(f'dtype: {y.dtype}')\n",
    "    \n",
    "#     X_o, y_o, categorical_indicator_o, attribute_names_0 = one_hot_encode_targets.apply(X, y, categorical_indicator, attribute_names)\n",
    "\n",
    "#     print(y_o)\n",
    "    \n",
    "#     # Pre-processing\n",
    "# #     data_pre_processing.set_seed_for_all(seed)\n",
    "# #     data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "# #     data_pre_processing.apply(task)\n",
    "# #     data_pre_processing.apply(model_name)\n",
    "# #     train_data, val_data, test_data = data_pre_processing.get_train_val_test()\n",
    "    \n",
    "# #     print(train_data.Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_pre_processing.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "#X_c, y_c, categorical_indicator_c, attribute_names_c = deepcopy(X), deepcopy(y), deepcopy(categorical_indicator), deepcopy(attribute_names)\n",
    "X, y, categorical_indicator, attribute_names = deepcopy(X_c), deepcopy(y_c), deepcopy(categorical_indicator_c), deepcopy(attribute_names_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from NeuralNetworksTrainingPackage.event_handler import dataPreProcessingEventEmitter\n",
    "# from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import filterCardinality, quantileTransform, truncateData ,balancedTruncateData, oneHotEncodePredictors, toDataFrame, splitTrainValTest, balancedSplitTrainValTest\n",
    "\n",
    "# n_sample = 20000\n",
    "# split = [0.5, 0.25, 0.25]\n",
    "# quantile_transform_distribution='normal'\n",
    "\n",
    "# class oneHotEncodeTargets():\n",
    "#     def __init__(self, transform = 'all'):\n",
    "#         self.parent = None\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def apply(self, X, y, categorical_indicator, attribute_names):\n",
    "        \n",
    "#         y = pd.get_dummies(y, dtype=int)\n",
    "\n",
    "#         # if isinstance(y, pd.DataFrame):\n",
    "#         #     is_categorical = any(y[col].dtype.name == 'category' for col in y.columns)\n",
    "#         #     if is_categorical:\n",
    "#         #         y = pd.get_dummies(y)\n",
    "#         # \n",
    "#         # if isinstance(y, pd.Series):\n",
    "#         #     is_categorical = y.dtype.name == 'category'\n",
    "#         # \n",
    "#         #     if is_categorical:\n",
    "#         #         y = pd.get_dummies(y)\n",
    "\n",
    "#         return X, y, categorical_indicator, attribute_names\n",
    "\n",
    "\n",
    "# data_pre_processing = dataPreProcessingEventEmitter()\n",
    "\n",
    "# filter_cardinality = filterCardinality(transform = 'all')\n",
    "# truncate_data = truncateData(n = n_sample, transform = 'all')\n",
    "# balanced_truncate_data = balancedTruncateData(n = n_sample, transform = 'all') # Ensures balance of classes\n",
    "# one_hot_encode_predictors = oneHotEncodePredictors(transform = 'all')\n",
    "# one_hot_encode_targets = oneHotEncodeTargets(transform = 'all')\n",
    "# to_data_frame = toDataFrame(transform = 'all')\n",
    "# split_train_val_test = splitTrainValTest(split = split)\n",
    "# balanced_split_train_val_test = balancedSplitTrainValTest(split = split)\n",
    "# quantile_transform = quantileTransform(output_distribution = quantile_transform_distribution, transform = 'all')\n",
    "\n",
    "\n",
    "# # Transformations will be called in the order they're added to data_pre_processing\n",
    "# data_pre_processing.add_pre_processing_step('regression', filter_cardinality)\n",
    "# data_pre_processing.add_pre_processing_step('regression', truncate_data)\n",
    "# data_pre_processing.add_pre_processing_step('regression', one_hot_encode_predictors)\n",
    "# data_pre_processing.add_pre_processing_step('regression', to_data_frame)\n",
    "# data_pre_processing.add_pre_processing_step('regression', split_train_val_test)\n",
    "# data_pre_processing.add_pre_processing_step('regression', quantile_transform)\n",
    "\n",
    "\n",
    "# data_pre_processing.add_pre_processing_step('classification', filter_cardinality)\n",
    "# data_pre_processing.add_pre_processing_step('classification', balanced_truncate_data)\n",
    "# data_pre_processing.add_pre_processing_step('classification', one_hot_encode_predictors)\n",
    "# data_pre_processing.add_pre_processing_step('classification', one_hot_encode_targets)\n",
    "# data_pre_processing.add_pre_processing_step('classification', to_data_frame)\n",
    "# data_pre_processing.add_pre_processing_step('classification', balanced_split_train_val_test)\n",
    "# data_pre_processing.add_pre_processing_step('classification', quantile_transform)\n",
    "\n",
    "# from NeuralNetworksTrainingPackage.dataprocessing.basic_pre_processing import CustomDataset, toPyTorchDatasets\n",
    "\n",
    "# to_pytorch_datasets = toPyTorchDatasets(wrapper = CustomDataset)\n",
    "\n",
    "# # Transformations will be called after general pre-processing steps, and in order they're added\n",
    "# data_pre_processing.add_pre_processing_step('LCN_reg', to_pytorch_datasets)\n",
    "\n",
    "# data_pre_processing.add_pre_processing_step('LCN_cls', to_pytorch_datasets)\n",
    "\n",
    "# data_pre_processing.add_pre_processing_step('LLN_reg', to_pytorch_datasets)\n",
    "\n",
    "# data_pre_processing.add_pre_processing_step('LLN_cls', to_pytorch_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pre_processing.set_seed_for_all(seed)\n",
    "data_pre_processing.set_dataset(X, y, categorical_indicator, attribute_names)\n",
    "data_pre_processing.apply('classification')\n",
    "data_pre_processing.apply(model_name)\n",
    "train_data, val_data, test_data = data_pre_processing.get_train_val_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=train_batch_size,shuffle= True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data,batch_size=len(val_data),shuffle= True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,batch_size=len(test_data),shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y, categorical_indicator, attribute_names = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_categorical_columns = [attr for attr, is_cat in zip(attribute_names, categorical_indicator) if not is_cat]\n",
    "print(X[non_categorical_columns].mean())\n",
    "print(X[non_categorical_columns].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_categorical_columns = [attr for attr, is_cat in zip(attribute_names_c, categorical_indicator_c) if not is_cat]\n",
    "print(X_c[non_categorical_columns].mean())\n",
    "print(X_c[non_categorical_columns].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X.loc[:, categorical_indicator].nunique())\n",
    "print(X_c.loc[:, categorical_indicator_c].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_data = data_pre_processing.get('val')\n",
    "test_data = data_pre_processing.get('test')\n",
    "print(val_data[1].value_counts())\n",
    "print(test_data[1].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(train_data.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data, target in train_dataloader:\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'336-361072' in regression_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 6,\n",
       " 'seed': 42,\n",
       " 'regularize': 0.5,\n",
       " 'embd_size': None,\n",
       " 'activation': 'sigmoid',\n",
       " 'hidden_dim': 64,\n",
       " 'optimizer': 'SGD',\n",
       " 'batch_size': 256,\n",
       " 'epochs': 150,\n",
       " 'lr': 0.004464764641809309,\n",
       " 'momentum': 0,\n",
       " 'no_cuda': False,\n",
       " 'lr_step_size': 0,\n",
       " 'gamma': 0.1,\n",
       " 'task': '336-361083'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '650f222909a509ad7a989bcd',\n",
       " 'metrics_per_epoch': [],\n",
       " 'experiment_id': '650f22271e88c2d3788cb61b',\n",
       " 'experiment_name': 'test_Mlp_1',\n",
       " 'mtpair_index': 21,\n",
       " 'mtpair_model': 'Mlp_sigm_cont',\n",
       " 'mtpair_task': '336-361083',\n",
       " 'is_completed': False,\n",
       " 'user_id': '64d3a7457658d6ec6db139d0',\n",
       " 'user_name': 'bart',\n",
       " 'hyp': {'depth': 6,\n",
       "  'seed': 42,\n",
       "  'regularize': 0.5,\n",
       "  'embd_size': None,\n",
       "  'activation': 'sigmoid',\n",
       "  'hidden_dim': 64,\n",
       "  'optimizer': 'SGD',\n",
       "  'batch_size': 256,\n",
       "  'epochs': 150,\n",
       "  'lr': 0.004464764641809309,\n",
       "  'momentum': 0,\n",
       "  'no_cuda': False,\n",
       "  'lr_step_size': 0,\n",
       "  'gamma': 0.1,\n",
       "  'task': '336-361083'},\n",
       " 'model': 'Mlp_sigm_cont',\n",
       " 'task': '336-361083'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'opml_reg_purnum_group': ['336-361072',\n",
       "  '336-361073',\n",
       "  '336-361074',\n",
       "  '336-361076',\n",
       "  '336-361077',\n",
       "  '336-361078',\n",
       "  '336-361079',\n",
       "  '336-361080',\n",
       "  '336-361081',\n",
       "  '336-361082',\n",
       "  '336-361083',\n",
       "  '336-361084',\n",
       "  '336-361085',\n",
       "  '336-361086',\n",
       "  '336-361087',\n",
       "  '336-361088',\n",
       "  '336-361279',\n",
       "  '336-361280',\n",
       "  '336-361281'],\n",
       " 'opml_class_purnum_group': ['337-361055',\n",
       "  '337-361060',\n",
       "  '337-361061',\n",
       "  '337-361062',\n",
       "  '337-361063',\n",
       "  '337-361065',\n",
       "  '337-361066',\n",
       "  '337-361068',\n",
       "  '337-361069',\n",
       "  '337-361070',\n",
       "  '337-361273',\n",
       "  '337-361274',\n",
       "  '337-361275',\n",
       "  '337-361276',\n",
       "  '337-361277',\n",
       "  '337-361278'],\n",
       " 'opml_reg_numcat_group': ['335-361093',\n",
       "  '335-361094',\n",
       "  '335-361096',\n",
       "  '335-361097',\n",
       "  '335-361098',\n",
       "  '335-361099',\n",
       "  '335-361101',\n",
       "  '335-361102',\n",
       "  '335-361103',\n",
       "  '335-361104',\n",
       "  '335-361287',\n",
       "  '335-361288',\n",
       "  '335-361289',\n",
       "  '335-361291',\n",
       "  '335-361292',\n",
       "  '335-361293',\n",
       "  '335-361294'],\n",
       " 'opml_class_numcat_group': ['334-361110',\n",
       "  '334-361111',\n",
       "  '334-361113',\n",
       "  '334-361282',\n",
       "  '334-361283',\n",
       "  '334-361285',\n",
       "  '334-361286']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.__dict__['data_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "authorship_tag": "ABX9TyO6d7scFvmxtcd+gePzcxnN",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
